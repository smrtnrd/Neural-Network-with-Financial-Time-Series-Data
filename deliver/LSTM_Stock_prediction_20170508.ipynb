{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock value prediction from Open, High, Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import pandas_datareader.data as web\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "d = 0.2\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Download data and normalize it\n",
    "Data since 1950 to today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name, normalize=True):\n",
    "    start = datetime.datetime(1950, 1, 1)\n",
    "    end = datetime.date.today()\n",
    "    df = web.DataReader(stock_name, \"yahoo\", start, end)\n",
    "    df.drop(['Volume', 'Close'], 1, inplace=True)\n",
    "    \n",
    "    if normalize:        \n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        df['Open'] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))\n",
    "        df['High'] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))\n",
    "        df['Low'] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))\n",
    "        df['Adj Close'] = min_max_scaler.fit_transform(df['Adj Close'].values.reshape(-1,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low  Adj Close\n",
      "Date                                               \n",
      "1950-01-03  0.000004  0.000000  0.000004   0.000000\n",
      "1950-01-04  0.000078  0.000074  0.000078   0.000074\n",
      "1950-01-05  0.000110  0.000106  0.000110   0.000106\n",
      "1950-01-06  0.000129  0.000125  0.000129   0.000125\n",
      "1950-01-09  0.000172  0.000168  0.000169   0.000164\n"
     ]
    }
   ],
   "source": [
    "df = get_stock_data(stock_name, normalize=True)\n",
    "# summarize first 5 rows\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot out the Normalized Adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stock(stock_name):\n",
    "    df = get_stock_data(stock_name, normalize=True)\n",
    "    print(df.head())\n",
    "    plt.plot(df['Adj Close'], color='red', label='Adj Close')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low  Adj Close\n",
      "Date                                               \n",
      "1950-01-03  0.000004  0.000000  0.000004   0.000000\n",
      "1950-01-04  0.000078  0.000074  0.000078   0.000074\n",
      "1950-01-05  0.000110  0.000106  0.000110   0.000106\n",
      "1950-01-06  0.000129  0.000125  0.000129   0.000125\n",
      "1950-01-09  0.000172  0.000168  0.000169   0.000164\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvYtt7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e612748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stock(stock_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set last day Adjusted Close as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix() \n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    result = []\n",
    "    \n",
    "    for index in range(len(data) - sequence_length): # maxmimum date = lastest date - sequence length\n",
    "        result.append(data[index: index + sequence_length]) # index : index + 22days\n",
    "    \n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0]) # 90% split\n",
    "    \n",
    "    train = result[:int(row), :] # 90% date\n",
    "    X_train = train[:, :-1] # all data until day m\n",
    "    y_train = train[:, -1][:,-1] # day m + 1 adjusted close price\n",
    "    \n",
    "    X_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1] \n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(df, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15335, 22, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15335"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Buidling neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model2(layers, neurons, d):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    # adam = keras.optimizers.Adam(decay=0.2)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,841\n",
      "Trainable params: 203,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model2(shape, neurons, d)\n",
    "# layers = [4, 22, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13801 samples, validate on 1534 samples\n",
      "Epoch 1/300\n",
      "13801/13801 [==============================] - 19s - loss: 0.0091 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.1204e-04 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.8921e-04 - acc: 0.0000e+00 - val_loss: 4.5630e-04 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "13801/13801 [==============================] - 20s - loss: 1.4859e-04 - acc: 0.0000e+00 - val_loss: 3.1502e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "13801/13801 [==============================] - 20s - loss: 1.3470e-04 - acc: 0.0000e+00 - val_loss: 2.7502e-04 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.2538e-04 - acc: 0.0000e+00 - val_loss: 2.4122e-04 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "13801/13801 [==============================] - 18s - loss: 1.1421e-04 - acc: 0.0000e+00 - val_loss: 4.7353e-04 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.1727e-04 - acc: 0.0000e+00 - val_loss: 2.4959e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "13801/13801 [==============================] - 20s - loss: 1.1644e-04 - acc: 0.0000e+00 - val_loss: 3.2337e-04 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.0953e-04 - acc: 0.0000e+00 - val_loss: 2.6601e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "13801/13801 [==============================] - 18s - loss: 9.8681e-05 - acc: 0.0000e+00 - val_loss: 2.0938e-04 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.0259e-04 - acc: 0.0000e+00 - val_loss: 2.1435e-04 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.0652e-04 - acc: 0.0000e+00 - val_loss: 2.3771e-04 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.0294e-04 - acc: 0.0000e+00 - val_loss: 2.1228e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "13801/13801 [==============================] - 20s - loss: 9.7486e-05 - acc: 0.0000e+00 - val_loss: 2.0034e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "13801/13801 [==============================] - 19s - loss: 9.5807e-05 - acc: 0.0000e+00 - val_loss: 1.9288e-04 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "13801/13801 [==============================] - 18s - loss: 1.0554e-04 - acc: 0.0000e+00 - val_loss: 1.8231e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "13801/13801 [==============================] - 18s - loss: 9.1816e-05 - acc: 0.0000e+00 - val_loss: 1.8671e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "13801/13801 [==============================] - 18s - loss: 9.8040e-05 - acc: 0.0000e+00 - val_loss: 3.0588e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "13801/13801 [==============================] - 18s - loss: 1.0610e-04 - acc: 0.0000e+00 - val_loss: 2.5848e-04 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.0111e-04 - acc: 0.0000e+00 - val_loss: 1.9608e-04 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "13801/13801 [==============================] - 19s - loss: 8.8135e-05 - acc: 0.0000e+00 - val_loss: 1.7066e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "13801/13801 [==============================] - 19s - loss: 8.9385e-05 - acc: 0.0000e+00 - val_loss: 2.0909e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "13801/13801 [==============================] - 18s - loss: 9.4763e-05 - acc: 0.0000e+00 - val_loss: 3.6722e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "13801/13801 [==============================] - 19s - loss: 9.4087e-05 - acc: 0.0000e+00 - val_loss: 2.6175e-04 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "13801/13801 [==============================] - 20s - loss: 8.6669e-05 - acc: 0.0000e+00 - val_loss: 1.6968e-04 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "13801/13801 [==============================] - 19s - loss: 9.0887e-05 - acc: 0.0000e+00 - val_loss: 2.0333e-04 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "13801/13801 [==============================] - 19s - loss: 8.1240e-05 - acc: 0.0000e+00 - val_loss: 1.7244e-04 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.9687e-05 - acc: 0.0000e+00 - val_loss: 1.5591e-04 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "13801/13801 [==============================] - 18s - loss: 8.5557e-05 - acc: 0.0000e+00 - val_loss: 4.5228e-04 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "13801/13801 [==============================] - 19s - loss: 8.6444e-05 - acc: 0.0000e+00 - val_loss: 1.7189e-04 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "13801/13801 [==============================] - 18s - loss: 9.1727e-05 - acc: 0.0000e+00 - val_loss: 2.5178e-04 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "13801/13801 [==============================] - 19s - loss: 8.5425e-05 - acc: 0.0000e+00 - val_loss: 2.3323e-04 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "13801/13801 [==============================] - 20s - loss: 8.4691e-05 - acc: 0.0000e+00 - val_loss: 2.3152e-04 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "13801/13801 [==============================] - 20s - loss: 8.3887e-05 - acc: 0.0000e+00 - val_loss: 1.4486e-04 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.9911e-05 - acc: 0.0000e+00 - val_loss: 2.3559e-04 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "13801/13801 [==============================] - 20s - loss: 9.8349e-05 - acc: 0.0000e+00 - val_loss: 1.5213e-04 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "13801/13801 [==============================] - 20s - loss: 8.4888e-05 - acc: 0.0000e+00 - val_loss: 3.5946e-04 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "13801/13801 [==============================] - 20s - loss: 9.2377e-05 - acc: 0.0000e+00 - val_loss: 3.1259e-04 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.8954e-05 - acc: 0.0000e+00 - val_loss: 1.6936e-04 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.7294e-05 - acc: 0.0000e+00 - val_loss: 1.3640e-04 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.1740e-04 - acc: 0.0000e+00 - val_loss: 1.6861e-04 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "13801/13801 [==============================] - 20s - loss: 1.0232e-04 - acc: 0.0000e+00 - val_loss: 1.4254e-04 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.6989e-05 - acc: 0.0000e+00 - val_loss: 1.3449e-04 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.7536e-05 - acc: 0.0000e+00 - val_loss: 1.5911e-04 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "13801/13801 [==============================] - 20s - loss: 8.2183e-05 - acc: 0.0000e+00 - val_loss: 1.5853e-04 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.6993e-05 - acc: 0.0000e+00 - val_loss: 1.7072e-04 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "13801/13801 [==============================] - 21s - loss: 8.0726e-05 - acc: 0.0000e+00 - val_loss: 1.5226e-04 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.4072e-05 - acc: 0.0000e+00 - val_loss: 1.4714e-04 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.2866e-05 - acc: 0.0000e+00 - val_loss: 1.3080e-04 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "13801/13801 [==============================] - 19s - loss: 1.1015e-04 - acc: 0.0000e+00 - val_loss: 1.3909e-04 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "13801/13801 [==============================] - 21s - loss: 7.6685e-05 - acc: 0.0000e+00 - val_loss: 1.7370e-04 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.4323e-05 - acc: 0.0000e+00 - val_loss: 1.5798e-04 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.5575e-05 - acc: 0.0000e+00 - val_loss: 1.2453e-04 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.3650e-05 - acc: 0.0000e+00 - val_loss: 1.2882e-04 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13801/13801 [==============================] - 20s - loss: 6.9256e-05 - acc: 0.0000e+00 - val_loss: 1.6756e-04 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.5244e-05 - acc: 0.0000e+00 - val_loss: 1.2315e-04 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.5652e-05 - acc: 0.0000e+00 - val_loss: 1.1780e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.5993e-05 - acc: 0.0000e+00 - val_loss: 2.5360e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.0021e-05 - acc: 0.0000e+00 - val_loss: 1.8405e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "13801/13801 [==============================] - 20s - loss: 6.8316e-05 - acc: 0.0000e+00 - val_loss: 1.5847e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.3811e-05 - acc: 0.0000e+00 - val_loss: 1.7920e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.4169e-05 - acc: 0.0000e+00 - val_loss: 1.5316e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.6046e-05 - acc: 0.0000e+00 - val_loss: 1.8284e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "13801/13801 [==============================] - 19s - loss: 8.1405e-05 - acc: 0.0000e+00 - val_loss: 2.2487e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "13801/13801 [==============================] - 18s - loss: 6.9531e-05 - acc: 0.0000e+00 - val_loss: 1.5244e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.7373e-05 - acc: 0.0000e+00 - val_loss: 1.0915e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.6516e-05 - acc: 0.0000e+00 - val_loss: 1.2027e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.7954e-05 - acc: 0.0000e+00 - val_loss: 1.8294e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.6958e-05 - acc: 0.0000e+00 - val_loss: 1.7126e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.6948e-05 - acc: 0.0000e+00 - val_loss: 1.2334e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "13801/13801 [==============================] - 23s - loss: 8.1297e-05 - acc: 0.0000e+00 - val_loss: 1.9726e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "13801/13801 [==============================] - 21s - loss: 7.6876e-05 - acc: 0.0000e+00 - val_loss: 1.3920e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "13801/13801 [==============================] - 20s - loss: 6.3275e-05 - acc: 0.0000e+00 - val_loss: 1.0208e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "13801/13801 [==============================] - 18s - loss: 6.4480e-05 - acc: 0.0000e+00 - val_loss: 2.0924e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "13801/13801 [==============================] - 19s - loss: 8.9165e-05 - acc: 0.0000e+00 - val_loss: 2.9156e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "13801/13801 [==============================] - 20s - loss: 7.1003e-05 - acc: 0.0000e+00 - val_loss: 2.2993e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.3119e-05 - acc: 0.0000e+00 - val_loss: 9.9961e-05 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "13801/13801 [==============================] - 18s - loss: 6.1893e-05 - acc: 0.0000e+00 - val_loss: 1.4276e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.6000e-05 - acc: 0.0000e+00 - val_loss: 2.6073e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "13801/13801 [==============================] - 20s - loss: 6.3313e-05 - acc: 0.0000e+00 - val_loss: 1.0700e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "13801/13801 [==============================] - 18s - loss: 6.0393e-05 - acc: 0.0000e+00 - val_loss: 1.8426e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.2475e-05 - acc: 0.0000e+00 - val_loss: 1.5125e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "13801/13801 [==============================] - 18s - loss: 7.1739e-05 - acc: 0.0000e+00 - val_loss: 1.3937e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.0765e-05 - acc: 0.0000e+00 - val_loss: 1.0203e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "13801/13801 [==============================] - 18s - loss: 5.7081e-05 - acc: 0.0000e+00 - val_loss: 1.2238e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "13801/13801 [==============================] - 18s - loss: 6.2250e-05 - acc: 0.0000e+00 - val_loss: 1.8352e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "13801/13801 [==============================] - 18s - loss: 6.7909e-05 - acc: 0.0000e+00 - val_loss: 1.8522e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "13801/13801 [==============================] - 18s - loss: 6.7722e-05 - acc: 0.0000e+00 - val_loss: 1.7789e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "13801/13801 [==============================] - 18s - loss: 6.3459e-05 - acc: 0.0000e+00 - val_loss: 9.8871e-05 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "13801/13801 [==============================] - 18s - loss: 6.2544e-05 - acc: 0.0000e+00 - val_loss: 1.6143e-04 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.3435e-05 - acc: 0.0000e+00 - val_loss: 9.6427e-05 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.1949e-05 - acc: 0.0000e+00 - val_loss: 1.2084e-04 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "13801/13801 [==============================] - 21s - loss: 6.3611e-05 - acc: 0.0000e+00 - val_loss: 9.2830e-05 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "13801/13801 [==============================] - 19s - loss: 7.2340e-05 - acc: 0.0000e+00 - val_loss: 1.7560e-04 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "13801/13801 [==============================] - 20s - loss: 5.6824e-05 - acc: 0.0000e+00 - val_loss: 1.3008e-04 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.5537e-05 - acc: 0.0000e+00 - val_loss: 9.3201e-05 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.0336e-05 - acc: 0.0000e+00 - val_loss: 9.5576e-05 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.4408e-05 - acc: 0.0000e+00 - val_loss: 1.1635e-04 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.9764e-05 - acc: 0.0000e+00 - val_loss: 1.1294e-04 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "13801/13801 [==============================] - 18s - loss: 5.7181e-05 - acc: 0.0000e+00 - val_loss: 8.8277e-05 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.5209e-05 - acc: 0.0000e+00 - val_loss: 1.3768e-04 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.3415e-05 - acc: 0.0000e+00 - val_loss: 1.8172e-04 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.0963e-05 - acc: 0.0000e+00 - val_loss: 1.3163e-04 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.2547e-05 - acc: 0.0000e+00 - val_loss: 1.2041e-04 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "13801/13801 [==============================] - 20s - loss: 6.1521e-05 - acc: 0.0000e+00 - val_loss: 1.0855e-04 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.4848e-05 - acc: 0.0000e+00 - val_loss: 9.8056e-05 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.2724e-05 - acc: 0.0000e+00 - val_loss: 1.3037e-04 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "13801/13801 [==============================] - 21s - loss: 5.7351e-05 - acc: 0.0000e+00 - val_loss: 8.5848e-05 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.3587e-05 - acc: 0.0000e+00 - val_loss: 8.9964e-05 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.9974e-05 - acc: 0.0000e+00 - val_loss: 1.1792e-04 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13801/13801 [==============================] - 19s - loss: 5.2964e-05 - acc: 0.0000e+00 - val_loss: 9.2523e-05 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "13801/13801 [==============================] - 19s - loss: 6.3993e-05 - acc: 0.0000e+00 - val_loss: 2.1575e-04 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.0215e-05 - acc: 0.0000e+00 - val_loss: 8.9052e-05 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.1551e-05 - acc: 0.0000e+00 - val_loss: 8.9702e-05 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.8610e-05 - acc: 0.0000e+00 - val_loss: 8.4827e-05 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.6071e-05 - acc: 0.0000e+00 - val_loss: 9.6913e-05 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.1617e-05 - acc: 0.0000e+00 - val_loss: 8.8872e-05 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.8594e-05 - acc: 0.0000e+00 - val_loss: 8.3117e-05 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.8244e-05 - acc: 0.0000e+00 - val_loss: 8.5765e-05 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "13801/13801 [==============================] - 18s - loss: 5.1736e-05 - acc: 0.0000e+00 - val_loss: 8.2395e-05 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.9037e-05 - acc: 0.0000e+00 - val_loss: 9.5676e-05 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.9357e-05 - acc: 0.0000e+00 - val_loss: 1.0126e-04 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.8104e-05 - acc: 0.0000e+00 - val_loss: 1.3462e-04 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.1920e-05 - acc: 0.0000e+00 - val_loss: 1.8830e-04 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.6096e-05 - acc: 0.0000e+00 - val_loss: 8.0476e-05 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "13801/13801 [==============================] - 20s - loss: 5.2493e-05 - acc: 0.0000e+00 - val_loss: 8.1854e-05 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "13801/13801 [==============================] - 21s - loss: 5.0191e-05 - acc: 0.0000e+00 - val_loss: 8.4101e-05 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.5274e-05 - acc: 0.0000e+00 - val_loss: 1.6723e-04 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.5620e-05 - acc: 0.0000e+00 - val_loss: 9.4767e-05 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "13801/13801 [==============================] - 21s - loss: 4.7617e-05 - acc: 0.0000e+00 - val_loss: 8.0999e-05 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.4870e-05 - acc: 0.0000e+00 - val_loss: 8.1743e-05 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.7622e-05 - acc: 0.0000e+00 - val_loss: 1.0808e-04 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.7021e-05 - acc: 0.0000e+00 - val_loss: 8.1504e-05 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.6274e-05 - acc: 0.0000e+00 - val_loss: 1.0647e-04 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.7142e-05 - acc: 0.0000e+00 - val_loss: 8.0819e-05 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.7519e-05 - acc: 0.0000e+00 - val_loss: 8.8904e-05 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.6815e-05 - acc: 0.0000e+00 - val_loss: 1.7585e-04 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "13801/13801 [==============================] - 21s - loss: 6.6971e-05 - acc: 0.0000e+00 - val_loss: 3.2381e-04 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "13801/13801 [==============================] - 19s - loss: 5.9259e-05 - acc: 0.0000e+00 - val_loss: 9.0408e-05 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.5127e-05 - acc: 0.0000e+00 - val_loss: 8.1609e-05 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.7533e-05 - acc: 0.0000e+00 - val_loss: 7.7114e-05 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.3161e-05 - acc: 0.0000e+00 - val_loss: 7.6314e-05 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.7725e-05 - acc: 0.0000e+00 - val_loss: 7.7610e-05 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.1672e-05 - acc: 0.0000e+00 - val_loss: 8.4683e-05 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.1720e-05 - acc: 0.0000e+00 - val_loss: 7.4527e-05 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.1994e-05 - acc: 0.0000e+00 - val_loss: 9.8735e-05 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.4242e-05 - acc: 0.0000e+00 - val_loss: 8.0068e-05 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.4430e-05 - acc: 0.0000e+00 - val_loss: 7.5373e-05 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.2004e-05 - acc: 0.0000e+00 - val_loss: 8.2286e-05 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.1823e-05 - acc: 0.0000e+00 - val_loss: 1.3206e-04 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.9341e-05 - acc: 0.0000e+00 - val_loss: 1.1101e-04 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.8059e-05 - acc: 0.0000e+00 - val_loss: 9.7226e-05 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.0912e-05 - acc: 0.0000e+00 - val_loss: 7.3188e-05 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.9951e-05 - acc: 0.0000e+00 - val_loss: 1.2474e-04 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.2542e-05 - acc: 0.0000e+00 - val_loss: 7.4191e-05 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.3083e-05 - acc: 0.0000e+00 - val_loss: 8.9798e-05 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.2587e-05 - acc: 0.0000e+00 - val_loss: 8.7157e-05 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "13801/13801 [==============================] - 21s - loss: 4.4888e-05 - acc: 0.0000e+00 - val_loss: 9.2526e-05 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.0686e-05 - acc: 0.0000e+00 - val_loss: 7.7102e-05 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.2789e-05 - acc: 0.0000e+00 - val_loss: 9.6980e-05 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.0842e-05 - acc: 0.0000e+00 - val_loss: 8.5392e-05 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.0295e-05 - acc: 0.0000e+00 - val_loss: 9.1978e-05 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.2933e-05 - acc: 0.0000e+00 - val_loss: 1.1650e-04 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.0717e-05 - acc: 0.0000e+00 - val_loss: 1.7289e-04 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.1033e-05 - acc: 0.0000e+00 - val_loss: 1.4019e-04 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13801/13801 [==============================] - 18s - loss: 4.8898e-05 - acc: 0.0000e+00 - val_loss: 8.0995e-05 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.0547e-05 - acc: 0.0000e+00 - val_loss: 6.8751e-05 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.5886e-05 - acc: 0.0000e+00 - val_loss: 8.0521e-05 - val_acc: 0.0000e+00\n",
      "Epoch 170/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.8390e-05 - acc: 0.0000e+00 - val_loss: 8.7637e-05 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.2593e-05 - acc: 0.0000e+00 - val_loss: 1.7974e-04 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.1809e-05 - acc: 0.0000e+00 - val_loss: 7.3465e-05 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.9760e-05 - acc: 0.0000e+00 - val_loss: 7.9742e-05 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.1272e-05 - acc: 0.0000e+00 - val_loss: 9.2224e-05 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.8347e-05 - acc: 0.0000e+00 - val_loss: 6.7677e-05 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.3086e-05 - acc: 0.0000e+00 - val_loss: 6.8535e-05 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.8578e-05 - acc: 0.0000e+00 - val_loss: 6.7844e-05 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "13801/13801 [==============================] - 20s - loss: 3.5926e-05 - acc: 0.0000e+00 - val_loss: 6.8494e-05 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "13801/13801 [==============================] - 20s - loss: 3.9212e-05 - acc: 0.0000e+00 - val_loss: 1.1818e-04 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.0345e-05 - acc: 0.0000e+00 - val_loss: 8.3167e-05 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.5909e-05 - acc: 0.0000e+00 - val_loss: 9.4012e-05 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.0033e-05 - acc: 0.0000e+00 - val_loss: 6.6309e-05 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.1621e-05 - acc: 0.0000e+00 - val_loss: 7.8046e-05 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.7026e-05 - acc: 0.0000e+00 - val_loss: 6.4157e-05 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "13801/13801 [==============================] - 20s - loss: 3.6947e-05 - acc: 0.0000e+00 - val_loss: 7.7060e-05 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.5996e-05 - acc: 0.0000e+00 - val_loss: 6.4248e-05 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.0684e-05 - acc: 0.0000e+00 - val_loss: 2.0463e-04 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.3614e-05 - acc: 0.0000e+00 - val_loss: 1.0443e-04 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.6127e-05 - acc: 0.0000e+00 - val_loss: 7.6186e-05 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.8846e-05 - acc: 0.0000e+00 - val_loss: 1.1970e-04 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.1557e-05 - acc: 0.0000e+00 - val_loss: 7.2444e-05 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.3486e-05 - acc: 0.0000e+00 - val_loss: 7.0264e-05 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.5929e-05 - acc: 0.0000e+00 - val_loss: 6.2401e-05 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.6037e-05 - acc: 0.0000e+00 - val_loss: 7.3056e-05 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.9354e-05 - acc: 0.0000e+00 - val_loss: 8.8243e-05 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.8775e-05 - acc: 0.0000e+00 - val_loss: 6.0726e-05 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.7914e-05 - acc: 0.0000e+00 - val_loss: 1.0361e-04 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "13801/13801 [==============================] - 20s - loss: 3.3615e-05 - acc: 0.0000e+00 - val_loss: 6.3489e-05 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.4523e-05 - acc: 0.0000e+00 - val_loss: 7.0345e-05 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.7061e-05 - acc: 0.0000e+00 - val_loss: 6.6732e-05 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.4129e-05 - acc: 0.0000e+00 - val_loss: 6.8495e-05 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.7351e-05 - acc: 0.0000e+00 - val_loss: 9.4533e-05 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.4931e-05 - acc: 0.0000e+00 - val_loss: 9.4184e-05 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.4670e-05 - acc: 0.0000e+00 - val_loss: 8.5216e-05 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.5586e-05 - acc: 0.0000e+00 - val_loss: 6.6788e-05 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.4265e-05 - acc: 0.0000e+00 - val_loss: 1.3984e-04 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.9234e-05 - acc: 0.0000e+00 - val_loss: 6.0081e-05 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "13801/13801 [==============================] - 18s - loss: 4.0191e-05 - acc: 0.0000e+00 - val_loss: 1.0389e-04 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.3771e-05 - acc: 0.0000e+00 - val_loss: 7.8053e-05 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.3395e-05 - acc: 0.0000e+00 - val_loss: 1.2576e-04 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.6555e-05 - acc: 0.0000e+00 - val_loss: 7.2477e-05 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.4945e-05 - acc: 0.0000e+00 - val_loss: 9.3056e-05 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.5473e-05 - acc: 0.0000e+00 - val_loss: 7.1424e-05 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "13801/13801 [==============================] - 21s - loss: 3.4398e-05 - acc: 0.0000e+00 - val_loss: 9.3716e-05 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.5743e-05 - acc: 0.0000e+00 - val_loss: 6.6341e-05 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.1894e-05 - acc: 0.0000e+00 - val_loss: 5.8150e-05 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.2678e-05 - acc: 0.0000e+00 - val_loss: 6.8537e-05 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "13801/13801 [==============================] - 19s - loss: 4.1008e-05 - acc: 0.0000e+00 - val_loss: 9.8633e-05 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.8410e-05 - acc: 0.0000e+00 - val_loss: 8.0398e-05 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.3095e-05 - acc: 0.0000e+00 - val_loss: 8.2878e-05 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.0569e-05 - acc: 0.0000e+00 - val_loss: 5.7120e-05 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13801/13801 [==============================] - 19s - loss: 3.1583e-05 - acc: 0.0000e+00 - val_loss: 5.8311e-05 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.1790e-05 - acc: 0.0000e+00 - val_loss: 5.9624e-05 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.0798e-05 - acc: 0.0000e+00 - val_loss: 5.8020e-05 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.0259e-05 - acc: 0.0000e+00 - val_loss: 5.6575e-05 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.3945e-05 - acc: 0.0000e+00 - val_loss: 7.1700e-05 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.4730e-05 - acc: 0.0000e+00 - val_loss: 5.6186e-05 - val_acc: 0.0000e+00\n",
      "Epoch 228/300\n",
      "13801/13801 [==============================] - 20s - loss: 3.3454e-05 - acc: 0.0000e+00 - val_loss: 8.3944e-05 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.1481e-05 - acc: 0.0000e+00 - val_loss: 9.0448e-05 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.4038e-05 - acc: 0.0000e+00 - val_loss: 7.3691e-05 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.1543e-05 - acc: 0.0000e+00 - val_loss: 6.4954e-05 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.0737e-05 - acc: 0.0000e+00 - val_loss: 6.0338e-05 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.1680e-05 - acc: 0.0000e+00 - val_loss: 7.3448e-05 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.1973e-05 - acc: 0.0000e+00 - val_loss: 6.9950e-05 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.3348e-05 - acc: 0.0000e+00 - val_loss: 6.9638e-05 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "13801/13801 [==============================] - 20s - loss: 3.0820e-05 - acc: 0.0000e+00 - val_loss: 5.7499e-05 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.2425e-05 - acc: 0.0000e+00 - val_loss: 8.0996e-05 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.2138e-05 - acc: 0.0000e+00 - val_loss: 5.8548e-05 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.2753e-05 - acc: 0.0000e+00 - val_loss: 7.1959e-05 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.0258e-05 - acc: 0.0000e+00 - val_loss: 6.3815e-05 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.1364e-05 - acc: 0.0000e+00 - val_loss: 6.9582e-05 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.4353e-05 - acc: 0.0000e+00 - val_loss: 9.9734e-05 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.0599e-05 - acc: 0.0000e+00 - val_loss: 5.5819e-05 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "13801/13801 [==============================] - 21s - loss: 3.5592e-05 - acc: 0.0000e+00 - val_loss: 7.4337e-05 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.6785e-05 - acc: 0.0000e+00 - val_loss: 9.2471e-05 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "13801/13801 [==============================] - 20s - loss: 4.1093e-05 - acc: 0.0000e+00 - val_loss: 1.1217e-04 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.9770e-05 - acc: 0.0000e+00 - val_loss: 5.8048e-05 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.9017e-05 - acc: 0.0000e+00 - val_loss: 8.6317e-05 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.1761e-05 - acc: 0.0000e+00 - val_loss: 5.7927e-05 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.2387e-05 - acc: 0.0000e+00 - val_loss: 8.3342e-05 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.9203e-05 - acc: 0.0000e+00 - val_loss: 6.2700e-05 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "13801/13801 [==============================] - 20s - loss: 2.8736e-05 - acc: 0.0000e+00 - val_loss: 8.4971e-05 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "13801/13801 [==============================] - 20s - loss: 2.7752e-05 - acc: 0.0000e+00 - val_loss: 5.8828e-05 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.9553e-05 - acc: 0.0000e+00 - val_loss: 7.4567e-05 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.0380e-05 - acc: 0.0000e+00 - val_loss: 9.6474e-05 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.9502e-05 - acc: 0.0000e+00 - val_loss: 6.5707e-05 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.9837e-05 - acc: 0.0000e+00 - val_loss: 5.9245e-05 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.9232e-05 - acc: 0.0000e+00 - val_loss: 7.5563e-05 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.8978e-05 - acc: 0.0000e+00 - val_loss: 6.7713e-05 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.2725e-05 - acc: 0.0000e+00 - val_loss: 5.6351e-05 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.0147e-05 - acc: 0.0000e+00 - val_loss: 8.0857e-05 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.5068e-05 - acc: 0.0000e+00 - val_loss: 1.0056e-04 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.1337e-05 - acc: 0.0000e+00 - val_loss: 5.4146e-05 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "13801/13801 [==============================] - 21s - loss: 2.8397e-05 - acc: 0.0000e+00 - val_loss: 6.1360e-05 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.9541e-05 - acc: 0.0000e+00 - val_loss: 5.9202e-05 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.1176e-05 - acc: 0.0000e+00 - val_loss: 5.4456e-05 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.8275e-05 - acc: 0.0000e+00 - val_loss: 5.7499e-05 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "13801/13801 [==============================] - 20s - loss: 2.7898e-05 - acc: 0.0000e+00 - val_loss: 5.5775e-05 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "13801/13801 [==============================] - 20s - loss: 2.9818e-05 - acc: 0.0000e+00 - val_loss: 5.4113e-05 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "13801/13801 [==============================] - 21s - loss: 3.2899e-05 - acc: 0.0000e+00 - val_loss: 5.6793e-05 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.8144e-05 - acc: 0.0000e+00 - val_loss: 6.6938e-05 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "13801/13801 [==============================] - 21s - loss: 2.9473e-05 - acc: 0.0000e+00 - val_loss: 5.7557e-05 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.8231e-05 - acc: 0.0000e+00 - val_loss: 9.1914e-05 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.9283e-05 - acc: 0.0000e+00 - val_loss: 5.3792e-05 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.8439e-05 - acc: 0.0000e+00 - val_loss: 5.4317e-05 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.0432e-05 - acc: 0.0000e+00 - val_loss: 6.4306e-05 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13801/13801 [==============================] - 19s - loss: 3.0122e-05 - acc: 0.0000e+00 - val_loss: 1.2027e-04 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.3844e-05 - acc: 0.0000e+00 - val_loss: 8.4834e-05 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.9367e-05 - acc: 0.0000e+00 - val_loss: 8.0732e-05 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "13801/13801 [==============================] - 21s - loss: 3.5422e-05 - acc: 0.0000e+00 - val_loss: 7.8360e-05 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.6831e-05 - acc: 0.0000e+00 - val_loss: 6.6958e-05 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.2445e-05 - acc: 0.0000e+00 - val_loss: 9.1909e-05 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.9282e-05 - acc: 0.0000e+00 - val_loss: 6.2358e-05 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.7561e-05 - acc: 0.0000e+00 - val_loss: 5.2943e-05 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.0918e-05 - acc: 0.0000e+00 - val_loss: 5.7081e-05 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.8984e-05 - acc: 0.0000e+00 - val_loss: 5.2033e-05 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "13801/13801 [==============================] - 18s - loss: 3.3141e-05 - acc: 0.0000e+00 - val_loss: 1.0456e-04 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.8906e-05 - acc: 0.0000e+00 - val_loss: 7.3712e-05 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.6924e-05 - acc: 0.0000e+00 - val_loss: 5.2485e-05 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.7581e-05 - acc: 0.0000e+00 - val_loss: 5.3466e-05 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.8483e-05 - acc: 0.0000e+00 - val_loss: 5.8920e-05 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.8479e-05 - acc: 0.0000e+00 - val_loss: 5.9745e-05 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.7321e-05 - acc: 0.0000e+00 - val_loss: 7.8334e-05 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.7237e-05 - acc: 0.0000e+00 - val_loss: 5.5821e-05 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.5928e-05 - acc: 0.0000e+00 - val_loss: 6.5500e-05 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.8410e-05 - acc: 0.0000e+00 - val_loss: 1.0531e-04 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "13801/13801 [==============================] - 19s - loss: 3.3920e-05 - acc: 0.0000e+00 - val_loss: 1.2461e-04 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.9636e-05 - acc: 0.0000e+00 - val_loss: 5.2315e-05 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "13801/13801 [==============================] - 18s - loss: 2.7130e-05 - acc: 0.0000e+00 - val_loss: 8.0625e-05 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "13801/13801 [==============================] - 19s - loss: 2.7955e-05 - acc: 0.0000e+00 - val_loss: 7.9064e-05 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12732ae80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Result on training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00002 MSE (0.00 RMSE)\n",
      "Test Score: 0.00017 MSE (0.01 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.8758331758155624e-05, 0.0001690568601012453)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prediction vs Real results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = percentage_difference(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Plot out prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def denormalize(stock_name, normalized_value):\n",
    "    start = datetime.datetime(2000, 1, 1)\n",
    "    end = datetime.date.today()\n",
    "    df = web.DataReader(stock_name, \"yahoo\", start, end)\n",
    "    \n",
    "    df = df['Adj Close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp = denormalize(stock_name, normalized_value_p)\n",
    "    newy_test = denormalize(stock_name, normalized_value_y_test)\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('Days')\n",
    "    plt2.ylabel('Adjusted Close')\n",
    "    plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1261b87b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result(stock_name, p, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Save for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('LSTM_Stock_prediction-20170429.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Fine tune model\n",
    "# 11. Function to load data, train model and see score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quick_measure(stock_name, seq_len, d, shape, neurons, epochs):\n",
    "    df = get_stock_data(stock_name)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, seq_len)\n",
    "    model = build_model2(shape, neurons, d)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    # model.save('LSTM_Stock_prediction-20170429.h5')\n",
    "    trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)\n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Fine tune hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.1 Optimial Dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dlist = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "neurons_LSTM = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "dropout_result = {}\n",
    "\n",
    "for d in dlist:    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs)\n",
    "    dropout_result[d] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_val = min(dropout_result.values())\n",
    "min_val_key = [k for k, v in dropout_result.items() if v == min_val]\n",
    "print (dropout_result)\n",
    "print (min_val_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lists = sorted(dropout_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Dropout')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.2 Optimial epochs value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochslist = [10,20,30,40,50,60,70,80,90,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_result = {}\n",
    "\n",
    "for epochs in epochslist:    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs)\n",
    "    epochs_result[epochs] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lists = sorted(epochs_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.3 Optimal number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "epochs = 90\n",
    "dropout = 0.3\n",
    "neuronlist1 = [32, 64, 128, 256, 512]\n",
    "neuronlist2 = [16, 32, 64]\n",
    "neurons_result = {}\n",
    "\n",
    "for neuron_lstm in neuronlist1:\n",
    "    neurons = [neuron_lstm, neuron_lstm]\n",
    "    for activation in neuronlist2:\n",
    "        neurons.append(activation)\n",
    "        neurons.append(1)\n",
    "        trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs)\n",
    "        neurons_result[str(neurons)] = testScore\n",
    "        neurons = neurons[:2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lists = sorted(neurons_result.items())\n",
    "x,y = zip(*lists)\n",
    "\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('neurons')\n",
    "plt.ylabel('Mean Square Error')\n",
    "\n",
    "plt.bar(range(len(lists)), y, align='center')\n",
    "plt.xticks(range(len(lists)), x)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "12.4 Optimial Dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [256, 256, 32, 1]\n",
    "epochs = 90\n",
    "decaylist = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model3(layers, neurons, d, decay):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quick_measure(stock_name, seq_len, d, shape, neurons, epochs, decay):\n",
    "    df = get_stock_data(stock_name)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, seq_len)\n",
    "    model = build_model3(shape, neurons, d, decay)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    # model.save('LSTM_Stock_prediction-20170429.h5')\n",
    "    trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)\n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decay_result = {}\n",
    "\n",
    "for decay in decaylist:    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs, decay)\n",
    "    decay_result[decay] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lists = sorted(decay_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Decay')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "neurons = [256, 256, 32, 1]\n",
    "epochs = 90\n",
    "d = 0.3 #dropout\n",
    "decay = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len_list = [5, 10, 22, 60, 120, 180]\n",
    "\n",
    "seq_len_result = {}\n",
    "\n",
    "for seq_len in seq_len_list:\n",
    "    shape = [4, seq_len, 1]\n",
    "    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs, decay)\n",
    "    seq_len_result[seq_len] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lists = sorted(seq_len_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
