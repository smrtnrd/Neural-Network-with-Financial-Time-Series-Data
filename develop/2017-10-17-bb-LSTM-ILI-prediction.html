<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>2017-10-17-bb-LSTM-ILI-prediction</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
[dir="rtl"] #ipython_notebook {
  float: right !important;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
[dir="rtl"] #tabs li {
  float: right;
}
ul#tabs {
  margin-bottom: 4px;
}
[dir="rtl"] ul#tabs {
  margin-right: 0px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons {
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-right {
  padding-top: 1px;
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-left {
  float: right !important;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
[dir="rtl"] #tree-selector a {
  float: right;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
[dir="rtl"] #new-menu {
  text-align: right;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
[dir="rtl"] #running .col-sm-8 {
  float: right !important;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}

@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ILI-activiy-prediction-from-Lat,-Long">ILI activiy prediction from Lat, Long<a class="anchor-link" href="#ILI-activiy-prediction-from-Lat,-Long">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Import-module">Import module<a class="anchor-link" href="#Import-module">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt2</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="k">import</span> <span class="n">read_csv</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="k">import</span> <span class="n">datetime</span>

<span class="kn">import</span> <span class="nn">math</span><span class="o">,</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="k">import</span> <span class="n">sqrt</span>

<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">sklearn</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">mean_squared_error</span>

<span class="kn">from</span> <span class="nn">sklearn.externals.joblib</span> <span class="k">import</span> <span class="n">Memory</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">Memory</span><span class="p">(</span><span class="n">cachedir</span><span class="o">=</span><span class="s1">&#39;/tmp&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>



<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="k">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="nn">keras.layers.recurrent</span> <span class="k">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">load_model</span>

<span class="kn">import</span> <span class="nn">h5py</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Input-parameters">Input parameters<a class="anchor-link" href="#Input-parameters">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="s1">&#39;../data/raw.csv&#39;</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">d</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1">#decay</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># feature, window, output</span>
<span class="n">neurons</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">300</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1.-Download-data-and-normalize-it">1. Download data and normalize it<a class="anchor-link" href="#1.-Download-data-and-normalize-it">&#182;</a></h1><p>Data since 2010 to 2015</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#TO DO : create a function to preprocess the data</span>
<span class="nd">@memory</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">get_ili_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    
    <span class="n">df</span> <span class="o">=</span> <span class="n">read_csv</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="c1"># manually specify column names</span>
    <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;statename&#39;</span><span class="p">,</span><span class="s1">&#39;activity_level&#39;</span><span class="p">,</span><span class="s1">&#39;activity_level_label&#39;</span><span class="p">,</span><span class="s1">&#39;season&#39;</span><span class="p">,</span><span class="s1">&#39;weeknumber&#39;</span><span class="p">,</span><span class="s1">&#39;Latitude&#39;</span><span class="p">,</span><span class="s1">&#39;Longitude&#39;</span><span class="p">]</span>
    <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;date&#39;</span>
    
    <span class="c1"># convert index to datetime</span>
    <span class="n">df</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;%b-</span><span class="si">%d</span><span class="s1">-%Y&#39;</span><span class="p">)</span>
    
    <span class="c1"># manually remove the feature we don;t want to evaluate </span>
    <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;statename&#39;</span><span class="p">,</span> <span class="s1">&#39;season&#39;</span><span class="p">,</span> <span class="s1">&#39;weeknumber&#39;</span><span class="p">,</span><span class="s1">&#39;activity_level_label&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>        
        <span class="n">min_max_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;activity_level&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_max_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">activity_level</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Latitude&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_max_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Latitude</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Longitude&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_max_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Longitude</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">get_ili_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># summarize first 5 rows</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>            activity_level  Latitude  Longitude
date                                           
2016-10-01             0.1  0.290799    0.80243
2016-09-24             0.1  0.290799    0.80243
2016-09-17             0.1  0.290799    0.80243
2016-09-10             0.1  0.290799    0.80243
2016-09-03             0.1  0.290799    0.80243
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stderr output_text">
<pre>/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="2.-Plot-out-the-ILI-activity-level">2. Plot out the ILI activity level<a class="anchor-link" href="#2.-Plot-out-the-ILI-activity-level">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@memory</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">plot_ili</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">get_ili_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;activity_level&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ILI activity&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="k">def</span> <span class="nf">plot_ili_group</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">get_ili_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">values</span>
    <span class="c1"># specify columns to plot</span>
    <span class="n">groups</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># plot each column</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">groups</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">values</span><span class="p">[:,</span> <span class="n">group</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">group</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_ili</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># plot_ili_group(data)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>            activity_level  Latitude  Longitude
date                                           
2016-10-01             0.1  0.290799    0.80243
2016-09-24             0.1  0.290799    0.80243
2016-09-17             0.1  0.290799    0.80243
2016-09-10             0.1  0.290799    0.80243
2016-09-03             0.1  0.290799    0.80243
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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==
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="3.-Set-last-day-Adjusted-Close-as-y">3. Set last day Adjusted Close as y<a class="anchor-link" href="#3.-Set-last-day-Adjusted-Close-as-y">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@memory</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">ili_data</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">):</span>
    <span class="n">amount_of_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ili_data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">ili_data</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> 
    <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># index starting from 0</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">sequence_length</span><span class="p">):</span> <span class="c1"># maxmimum date = lastest date - sequence length</span>
        <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">index</span><span class="p">:</span> <span class="n">index</span> <span class="o">+</span> <span class="n">sequence_length</span><span class="p">])</span> <span class="c1"># index : index + 22days</span>
    
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">row</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 90% split</span>
    
    <span class="n">train</span> <span class="o">=</span> <span class="n">result</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">row</span><span class="p">),</span> <span class="p">:]</span> <span class="c1"># 90% date</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># all data until day m</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">][:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># day m + 1 adjusted close price</span>
    
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">row</span><span class="p">):,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">row</span><span class="p">):,</span> <span class="o">-</span><span class="mi">1</span><span class="p">][:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> 

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">amount_of_features</span><span class="p">))</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">amount_of_features</span><span class="p">))</span>  

    <span class="k">return</span> <span class="p">[</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[32]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>(14319, 22, 3)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[33]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>14319</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="4.-Buidling-neural-network">4. Buidling neural network<a class="anchor-link" href="#4.-Buidling-neural-network">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@memory</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">build_model2</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
        
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">neurons</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
        
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">neurons</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>        
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">neurons</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
    <span class="c1"># model = load_model(&#39;my_LSTM_stock_model1000.h5&#39;)</span>
    <span class="c1"># adam = keras.optimizers.Adam(decay=0.2)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="6.-Model-Execution">6. Model Execution<a class="anchor-link" href="#6.-Model-Execution">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">build_model2</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="c1"># layers = [3, 22, 1]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_7 (LSTM)                (None, 22, 128)           67584     
_________________________________________________________________
dropout_7 (Dropout)          (None, 22, 128)           0         
_________________________________________________________________
lstm_8 (LSTM)                (None, 128)               131584    
_________________________________________________________________
dropout_8 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_7 (Dense)              (None, 32)                4128      
_________________________________________________________________
dense_8 (Dense)              (None, 1)                 33        
=================================================================
Total params: 203,329
Trainable params: 203,329
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-35-8d7918e67027&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>model <span class="ansi-blue-fg">=</span> build_model2<span class="ansi-blue-fg">(</span>shape<span class="ansi-blue-fg">,</span> neurons<span class="ansi-blue-fg">,</span> d<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span class="ansi-red-fg"># layers = [3, 22, 1]</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    560</span> 
<span class="ansi-green-intense-fg ansi-bold">    561</span>     <span class="ansi-green-fg">def</span> __call__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 562</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>_cached_call<span class="ansi-blue-fg">(</span>args<span class="ansi-blue-fg">,</span> kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">    563</span> 
<span class="ansi-green-intense-fg ansi-bold">    564</span>     <span class="ansi-green-fg">def</span> __reduce__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py</span> in <span class="ansi-cyan-fg">_cached_call</span><span class="ansi-blue-fg">(self, args, kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    508</span>                           <span class="ansi-blue-fg">&#39;directory %s&#39;</span>
<span class="ansi-green-intense-fg ansi-bold">    509</span>                         % (name, argument_hash, output_dir))
<span class="ansi-green-fg">--&gt; 510</span><span class="ansi-red-fg">             </span>out<span class="ansi-blue-fg">,</span> metadata <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    511</span>             <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>mmap_mode <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    512</span>                 <span class="ansi-red-fg"># Memmap the output at the first call to be consistent with</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py</span> in <span class="ansi-cyan-fg">call</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    743</span>             print<span class="ansi-blue-fg">(</span>format_call<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>func<span class="ansi-blue-fg">,</span> args<span class="ansi-blue-fg">,</span> kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    744</span>         output <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>func<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 745</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_persist_output<span class="ansi-blue-fg">(</span>output<span class="ansi-blue-fg">,</span> output_dir<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    746</span>         duration <span class="ansi-blue-fg">=</span> time<span class="ansi-blue-fg">.</span>time<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-</span> start_time
<span class="ansi-green-intense-fg ansi-bold">    747</span>         metadata <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_persist_input<span class="ansi-blue-fg">(</span>output_dir<span class="ansi-blue-fg">,</span> duration<span class="ansi-blue-fg">,</span> args<span class="ansi-blue-fg">,</span> kwargs<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py</span> in <span class="ansi-cyan-fg">_persist_output</span><span class="ansi-blue-fg">(self, output, dir)</span>
<span class="ansi-green-intense-fg ansi-bold">    762</span>             write_func = functools.partial(numpy_pickle.dump,
<span class="ansi-green-intense-fg ansi-bold">    763</span>                                            compress=self.compress)
<span class="ansi-green-fg">--&gt; 764</span><span class="ansi-red-fg">             </span>concurrency_safe_write<span class="ansi-blue-fg">(</span>output<span class="ansi-blue-fg">,</span> filename<span class="ansi-blue-fg">,</span> write_func<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    765</span>             <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>_verbose <span class="ansi-blue-fg">&gt;</span> <span class="ansi-cyan-fg">10</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    766</span>                 print<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;Persisting in %s&#39;</span> <span class="ansi-blue-fg">%</span> dir<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py</span> in <span class="ansi-cyan-fg">concurrency_safe_write</span><span class="ansi-blue-fg">(to_write, filename, write_func)</span>
<span class="ansi-green-intense-fg ansi-bold">    209</span>     temporary_filename = &#39;{}.thread-{}-pid-{}&#39;.format(
<span class="ansi-green-intense-fg ansi-bold">    210</span>         filename, thread_id, os.getpid())
<span class="ansi-green-fg">--&gt; 211</span><span class="ansi-red-fg">     </span>write_func<span class="ansi-blue-fg">(</span>to_write<span class="ansi-blue-fg">,</span> temporary_filename<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    212</span>     concurrency_safe_rename<span class="ansi-blue-fg">(</span>temporary_filename<span class="ansi-blue-fg">,</span> filename<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    213</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">dump</span><span class="ansi-blue-fg">(value, filename, compress, protocol, cache_size)</span>
<span class="ansi-green-intense-fg ansi-bold">    482</span>     <span class="ansi-green-fg">elif</span> is_filename<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    483</span>         <span class="ansi-green-fg">with</span> open<span class="ansi-blue-fg">(</span>filename<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#39;wb&#39;</span><span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">as</span> f<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 484</span><span class="ansi-red-fg">             </span>NumpyPickler<span class="ansi-blue-fg">(</span>f<span class="ansi-blue-fg">,</span> protocol<span class="ansi-blue-fg">=</span>protocol<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>dump<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    485</span>     <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    486</span>         NumpyPickler<span class="ansi-blue-fg">(</span>filename<span class="ansi-blue-fg">,</span> protocol<span class="ansi-blue-fg">=</span>protocol<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>dump<span class="ansi-blue-fg">(</span>value<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">dump</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    407</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>proto <span class="ansi-blue-fg">&gt;=</span> <span class="ansi-cyan-fg">4</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    408</span>             self<span class="ansi-blue-fg">.</span>framer<span class="ansi-blue-fg">.</span>start_framing<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 409</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    410</span>         self<span class="ansi-blue-fg">.</span>write<span class="ansi-blue-fg">(</span>STOP<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    411</span>         self<span class="ansi-blue-fg">.</span>framer<span class="ansi-blue-fg">.</span>end_framing<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    519</span> 
<span class="ansi-green-intense-fg ansi-bold">    520</span>         <span class="ansi-red-fg"># Save the reduce() output and finally memoize the object</span>
<span class="ansi-green-fg">--&gt; 521</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>save_reduce<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">=</span>obj<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>rv<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    522</span> 
<span class="ansi-green-intense-fg ansi-bold">    523</span>     <span class="ansi-green-fg">def</span> persistent_id<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_reduce</span><span class="ansi-blue-fg">(self, func, args, state, listitems, dictitems, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    632</span> 
<span class="ansi-green-intense-fg ansi-bold">    633</span>         <span class="ansi-green-fg">if</span> state <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 634</span><span class="ansi-red-fg">             </span>save<span class="ansi-blue-fg">(</span>state<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    635</span>             write<span class="ansi-blue-fg">(</span>BUILD<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    636</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         f <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dispatch<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span>t<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    475</span>         <span class="ansi-green-fg">if</span> f <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 476</span><span class="ansi-red-fg">             </span>f<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># Call unbound method with explicit self</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    478</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_dict</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    819</span> 
<span class="ansi-green-intense-fg ansi-bold">    820</span>         self<span class="ansi-blue-fg">.</span>memoize<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 821</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_batch_setitems<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">.</span>items<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    822</span> 
<span class="ansi-green-intense-fg ansi-bold">    823</span>     dispatch<span class="ansi-blue-fg">[</span>dict<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> save_dict

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">_batch_setitems</span><span class="ansi-blue-fg">(self, items)</span>
<span class="ansi-green-intense-fg ansi-bold">    845</span>                 <span class="ansi-green-fg">for</span> k<span class="ansi-blue-fg">,</span> v <span class="ansi-green-fg">in</span> tmp<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    846</span>                     save<span class="ansi-blue-fg">(</span>k<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 847</span><span class="ansi-red-fg">                     </span>save<span class="ansi-blue-fg">(</span>v<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    848</span>                 write<span class="ansi-blue-fg">(</span>SETITEMS<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    849</span>             <span class="ansi-green-fg">elif</span> n<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         f <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dispatch<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span>t<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    475</span>         <span class="ansi-green-fg">if</span> f <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 476</span><span class="ansi-red-fg">             </span>f<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># Call unbound method with explicit self</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    478</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_list</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    779</span> 
<span class="ansi-green-intense-fg ansi-bold">    780</span>         self<span class="ansi-blue-fg">.</span>memoize<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 781</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_batch_appends<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    782</span> 
<span class="ansi-green-intense-fg ansi-bold">    783</span>     dispatch<span class="ansi-blue-fg">[</span>list<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> save_list

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">_batch_appends</span><span class="ansi-blue-fg">(self, items)</span>
<span class="ansi-green-intense-fg ansi-bold">    803</span>                 write<span class="ansi-blue-fg">(</span>MARK<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    804</span>                 <span class="ansi-green-fg">for</span> x <span class="ansi-green-fg">in</span> tmp<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 805</span><span class="ansi-red-fg">                     </span>save<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    806</span>                 write<span class="ansi-blue-fg">(</span>APPENDS<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    807</span>             <span class="ansi-green-fg">elif</span> n<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    519</span> 
<span class="ansi-green-intense-fg ansi-bold">    520</span>         <span class="ansi-red-fg"># Save the reduce() output and finally memoize the object</span>
<span class="ansi-green-fg">--&gt; 521</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>save_reduce<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">=</span>obj<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>rv<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    522</span> 
<span class="ansi-green-intense-fg ansi-bold">    523</span>     <span class="ansi-green-fg">def</span> persistent_id<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_reduce</span><span class="ansi-blue-fg">(self, func, args, state, listitems, dictitems, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    632</span> 
<span class="ansi-green-intense-fg ansi-bold">    633</span>         <span class="ansi-green-fg">if</span> state <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 634</span><span class="ansi-red-fg">             </span>save<span class="ansi-blue-fg">(</span>state<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    635</span>             write<span class="ansi-blue-fg">(</span>BUILD<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    636</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         f <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dispatch<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span>t<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    475</span>         <span class="ansi-green-fg">if</span> f <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 476</span><span class="ansi-red-fg">             </span>f<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># Call unbound method with explicit self</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    478</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_dict</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    819</span> 
<span class="ansi-green-intense-fg ansi-bold">    820</span>         self<span class="ansi-blue-fg">.</span>memoize<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 821</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_batch_setitems<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">.</span>items<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    822</span> 
<span class="ansi-green-intense-fg ansi-bold">    823</span>     dispatch<span class="ansi-blue-fg">[</span>dict<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> save_dict

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">_batch_setitems</span><span class="ansi-blue-fg">(self, items)</span>
<span class="ansi-green-intense-fg ansi-bold">    845</span>                 <span class="ansi-green-fg">for</span> k<span class="ansi-blue-fg">,</span> v <span class="ansi-green-fg">in</span> tmp<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    846</span>                     save<span class="ansi-blue-fg">(</span>k<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 847</span><span class="ansi-red-fg">                     </span>save<span class="ansi-blue-fg">(</span>v<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    848</span>                 write<span class="ansi-blue-fg">(</span>SETITEMS<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    849</span>             <span class="ansi-green-fg">elif</span> n<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         f <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dispatch<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span>t<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    475</span>         <span class="ansi-green-fg">if</span> f <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 476</span><span class="ansi-red-fg">             </span>f<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># Call unbound method with explicit self</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    478</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_list</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    779</span> 
<span class="ansi-green-intense-fg ansi-bold">    780</span>         self<span class="ansi-blue-fg">.</span>memoize<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 781</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_batch_appends<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    782</span> 
<span class="ansi-green-intense-fg ansi-bold">    783</span>     dispatch<span class="ansi-blue-fg">[</span>list<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> save_list

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">_batch_appends</span><span class="ansi-blue-fg">(self, items)</span>
<span class="ansi-green-intense-fg ansi-bold">    803</span>                 write<span class="ansi-blue-fg">(</span>MARK<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    804</span>                 <span class="ansi-green-fg">for</span> x <span class="ansi-green-fg">in</span> tmp<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 805</span><span class="ansi-red-fg">                     </span>save<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    806</span>                 write<span class="ansi-blue-fg">(</span>APPENDS<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    807</span>             <span class="ansi-green-fg">elif</span> n<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    519</span> 
<span class="ansi-green-intense-fg ansi-bold">    520</span>         <span class="ansi-red-fg"># Save the reduce() output and finally memoize the object</span>
<span class="ansi-green-fg">--&gt; 521</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>save_reduce<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">=</span>obj<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>rv<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    522</span> 
<span class="ansi-green-intense-fg ansi-bold">    523</span>     <span class="ansi-green-fg">def</span> persistent_id<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_reduce</span><span class="ansi-blue-fg">(self, func, args, state, listitems, dictitems, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    632</span> 
<span class="ansi-green-intense-fg ansi-bold">    633</span>         <span class="ansi-green-fg">if</span> state <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 634</span><span class="ansi-red-fg">             </span>save<span class="ansi-blue-fg">(</span>state<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    635</span>             write<span class="ansi-blue-fg">(</span>BUILD<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    636</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         f <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dispatch<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span>t<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    475</span>         <span class="ansi-green-fg">if</span> f <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 476</span><span class="ansi-red-fg">             </span>f<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># Call unbound method with explicit self</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    478</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_dict</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    819</span> 
<span class="ansi-green-intense-fg ansi-bold">    820</span>         self<span class="ansi-blue-fg">.</span>memoize<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 821</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_batch_setitems<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">.</span>items<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    822</span> 
<span class="ansi-green-intense-fg ansi-bold">    823</span>     dispatch<span class="ansi-blue-fg">[</span>dict<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> save_dict

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">_batch_setitems</span><span class="ansi-blue-fg">(self, items)</span>
<span class="ansi-green-intense-fg ansi-bold">    845</span>                 <span class="ansi-green-fg">for</span> k<span class="ansi-blue-fg">,</span> v <span class="ansi-green-fg">in</span> tmp<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    846</span>                     save<span class="ansi-blue-fg">(</span>k<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 847</span><span class="ansi-red-fg">                     </span>save<span class="ansi-blue-fg">(</span>v<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    848</span>                 write<span class="ansi-blue-fg">(</span>SETITEMS<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    849</span>             <span class="ansi-green-fg">elif</span> n<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    519</span> 
<span class="ansi-green-intense-fg ansi-bold">    520</span>         <span class="ansi-red-fg"># Save the reduce() output and finally memoize the object</span>
<span class="ansi-green-fg">--&gt; 521</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>save_reduce<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">=</span>obj<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>rv<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    522</span> 
<span class="ansi-green-intense-fg ansi-bold">    523</span>     <span class="ansi-green-fg">def</span> persistent_id<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_reduce</span><span class="ansi-blue-fg">(self, func, args, state, listitems, dictitems, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    632</span> 
<span class="ansi-green-intense-fg ansi-bold">    633</span>         <span class="ansi-green-fg">if</span> state <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 634</span><span class="ansi-red-fg">             </span>save<span class="ansi-blue-fg">(</span>state<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    635</span>             write<span class="ansi-blue-fg">(</span>BUILD<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    636</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         f <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dispatch<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span>t<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    475</span>         <span class="ansi-green-fg">if</span> f <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 476</span><span class="ansi-red-fg">             </span>f<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># Call unbound method with explicit self</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    478</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_dict</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    819</span> 
<span class="ansi-green-intense-fg ansi-bold">    820</span>         self<span class="ansi-blue-fg">.</span>memoize<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 821</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_batch_setitems<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">.</span>items<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    822</span> 
<span class="ansi-green-intense-fg ansi-bold">    823</span>     dispatch<span class="ansi-blue-fg">[</span>dict<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> save_dict

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">_batch_setitems</span><span class="ansi-blue-fg">(self, items)</span>
<span class="ansi-green-intense-fg ansi-bold">    845</span>                 <span class="ansi-green-fg">for</span> k<span class="ansi-blue-fg">,</span> v <span class="ansi-green-fg">in</span> tmp<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    846</span>                     save<span class="ansi-blue-fg">(</span>k<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 847</span><span class="ansi-red-fg">                     </span>save<span class="ansi-blue-fg">(</span>v<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    848</span>                 write<span class="ansi-blue-fg">(</span>SETITEMS<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    849</span>             <span class="ansi-green-fg">elif</span> n<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    519</span> 
<span class="ansi-green-intense-fg ansi-bold">    520</span>         <span class="ansi-red-fg"># Save the reduce() output and finally memoize the object</span>
<span class="ansi-green-fg">--&gt; 521</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>save_reduce<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">=</span>obj<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>rv<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    522</span> 
<span class="ansi-green-intense-fg ansi-bold">    523</span>     <span class="ansi-green-fg">def</span> persistent_id<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_reduce</span><span class="ansi-blue-fg">(self, func, args, state, listitems, dictitems, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    632</span> 
<span class="ansi-green-intense-fg ansi-bold">    633</span>         <span class="ansi-green-fg">if</span> state <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 634</span><span class="ansi-red-fg">             </span>save<span class="ansi-blue-fg">(</span>state<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    635</span>             write<span class="ansi-blue-fg">(</span>BUILD<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    636</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         f <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dispatch<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span>t<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    475</span>         <span class="ansi-green-fg">if</span> f <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 476</span><span class="ansi-red-fg">             </span>f<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># Call unbound method with explicit self</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    478</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_dict</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    819</span> 
<span class="ansi-green-intense-fg ansi-bold">    820</span>         self<span class="ansi-blue-fg">.</span>memoize<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 821</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_batch_setitems<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">.</span>items<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    822</span> 
<span class="ansi-green-intense-fg ansi-bold">    823</span>     dispatch<span class="ansi-blue-fg">[</span>dict<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> save_dict

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">_batch_setitems</span><span class="ansi-blue-fg">(self, items)</span>
<span class="ansi-green-intense-fg ansi-bold">    845</span>                 <span class="ansi-green-fg">for</span> k<span class="ansi-blue-fg">,</span> v <span class="ansi-green-fg">in</span> tmp<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    846</span>                     save<span class="ansi-blue-fg">(</span>k<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 847</span><span class="ansi-red-fg">                     </span>save<span class="ansi-blue-fg">(</span>v<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    848</span>                 write<span class="ansi-blue-fg">(</span>SETITEMS<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    849</span>             <span class="ansi-green-fg">elif</span> n<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    519</span> 
<span class="ansi-green-intense-fg ansi-bold">    520</span>         <span class="ansi-red-fg"># Save the reduce() output and finally memoize the object</span>
<span class="ansi-green-fg">--&gt; 521</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>save_reduce<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">=</span>obj<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>rv<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    522</span> 
<span class="ansi-green-intense-fg ansi-bold">    523</span>     <span class="ansi-green-fg">def</span> persistent_id<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_reduce</span><span class="ansi-blue-fg">(self, func, args, state, listitems, dictitems, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    632</span> 
<span class="ansi-green-intense-fg ansi-bold">    633</span>         <span class="ansi-green-fg">if</span> state <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 634</span><span class="ansi-red-fg">             </span>save<span class="ansi-blue-fg">(</span>state<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    635</span>             write<span class="ansi-blue-fg">(</span>BUILD<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    636</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         f <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dispatch<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span>t<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    475</span>         <span class="ansi-green-fg">if</span> f <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 476</span><span class="ansi-red-fg">             </span>f<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># Call unbound method with explicit self</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    478</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save_dict</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    819</span> 
<span class="ansi-green-intense-fg ansi-bold">    820</span>         self<span class="ansi-blue-fg">.</span>memoize<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 821</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_batch_setitems<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">.</span>items<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    822</span> 
<span class="ansi-green-intense-fg ansi-bold">    823</span>     dispatch<span class="ansi-blue-fg">[</span>dict<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> save_dict

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">_batch_setitems</span><span class="ansi-blue-fg">(self, items)</span>
<span class="ansi-green-intense-fg ansi-bold">    845</span>                 <span class="ansi-green-fg">for</span> k<span class="ansi-blue-fg">,</span> v <span class="ansi-green-fg">in</span> tmp<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    846</span>                     save<span class="ansi-blue-fg">(</span>k<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 847</span><span class="ansi-red-fg">                     </span>save<span class="ansi-blue-fg">(</span>v<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    848</span>                 write<span class="ansi-blue-fg">(</span>SETITEMS<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    849</span>             <span class="ansi-green-fg">elif</span> n<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj)</span>
<span class="ansi-green-intense-fg ansi-bold">    279</span>             <span class="ansi-green-fg">return</span>
<span class="ansi-green-intense-fg ansi-bold">    280</span> 
<span class="ansi-green-fg">--&gt; 281</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> Pickler<span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    282</span> 
<span class="ansi-green-intense-fg ansi-bold">    283</span> 

<span class="ansi-green-fg">/anaconda/lib/python3.6/pickle.py</span> in <span class="ansi-cyan-fg">save</span><span class="ansi-blue-fg">(self, obj, save_persistent_id)</span>
<span class="ansi-green-intense-fg ansi-bold">    494</span>             reduce <span class="ansi-blue-fg">=</span> getattr<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#34;__reduce_ex__&#34;</span><span class="ansi-blue-fg">,</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    495</span>             <span class="ansi-green-fg">if</span> reduce <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 496</span><span class="ansi-red-fg">                 </span>rv <span class="ansi-blue-fg">=</span> reduce<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>proto<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    497</span>             <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    498</span>                 reduce <span class="ansi-blue-fg">=</span> getattr<span class="ansi-blue-fg">(</span>obj<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#34;__reduce__&#34;</span><span class="ansi-blue-fg">,</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">TypeError</span>: can&#39;t pickle _thread.lock objects</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@memory</span><span class="o">.</span><span class="n">cache</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-cyan-fg">  File </span><span class="ansi-green-fg">&#34;&lt;ipython-input-36-4d5419ecfbca&gt;&#34;</span><span class="ansi-cyan-fg">, line </span><span class="ansi-green-fg">2</span>
<span class="ansi-red-fg">    model.fit(</span>
        ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> invalid syntax
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="7.-Result-on-training-set-and-testing-set">7. Result on training set and testing set<a class="anchor-link" href="#7.-Result-on-training-set-and-testing-set">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[52]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@memory</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">model_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">trainScore</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Score: </span><span class="si">%.5f</span><span class="s1"> MSE (</span><span class="si">%.2f</span><span class="s1"> RMSE)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">trainScore</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">trainScore</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>

    <span class="n">testScore</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test Score: </span><span class="si">%.5f</span><span class="s1"> MSE (</span><span class="si">%.2f</span><span class="s1"> RMSE)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">testScore</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">testScore</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
    <span class="k">return</span> <span class="n">trainScore</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">testScore</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[53]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train Score: 0.00025 MSE (0.02 RMSE)
Test Score: 0.00031 MSE (0.02 RMSE)
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[53]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>(0.00024660477642904745, 0.00030625113549688526)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="8.-Prediction-vs-Real-results">8. Prediction vs Real results<a class="anchor-link" href="#8.-Prediction-vs-Real-results">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[54]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@memory</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">percentage_difference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">percentage_diff</span><span class="o">=</span><span class="p">[]</span>

    <span class="n">p</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)):</span> <span class="c1"># for each data index in test data</span>
        <span class="n">pr</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">u</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># pr = prediction on day u</span>

        <span class="n">percentage_diff</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">pr</span><span class="o">-</span><span class="n">y_test</span><span class="p">[</span><span class="n">u</span><span class="p">]</span><span class="o">/</span><span class="n">pr</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[55]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">percentage_difference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="9.-Plot-out-prediction">9. Plot out prediction<a class="anchor-link" href="#9.-Plot-out-prediction">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[56]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">denormalize</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">normalized_value</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">date</span><span class="o">.</span><span class="n">today</span><span class="p">()</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">read_csv</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;activity_level&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">normalized_value</span> <span class="o">=</span> <span class="n">normalized_value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1">#return df.shape, p.shape</span>
    <span class="n">min_max_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">min_max_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">new</span> <span class="o">=</span> <span class="n">min_max_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">normalized_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[57]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot_result</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">normalized_value_p</span><span class="p">,</span> <span class="n">normalized_value_y_test</span><span class="p">):</span>
    <span class="n">newp</span> <span class="o">=</span> <span class="n">denormalize</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">normalized_value_p</span><span class="p">)</span>
    <span class="n">newy_test</span> <span class="o">=</span> <span class="n">denormalize</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">normalized_value_y_test</span><span class="p">)</span>
    <span class="n">plt2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">newp</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prediction&#39;</span><span class="p">)</span>
    <span class="n">plt2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">newy_test</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Actual&#39;</span><span class="p">)</span>
    <span class="n">plt2</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt2</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;The test result for </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;ILI activity&#39;</span><span class="p">))</span>
    <span class="n">plt2</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Days&#39;</span><span class="p">)</span>
    <span class="n">plt2</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity_level&#39;</span><span class="p">)</span>
    <span class="n">plt2</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[59]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_result</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-59-65cfa5f70715&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>plot_result<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">,</span> p<span class="ansi-blue-fg">,</span> y_test<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-57-1e74c6c03a20&gt;</span> in <span class="ansi-cyan-fg">plot_result</span><span class="ansi-blue-fg">(data, normalized_value_p, normalized_value_y_test)</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-green-fg">def</span> plot_result<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">,</span> normalized_value_p<span class="ansi-blue-fg">,</span> normalized_value_y_test<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg">     </span>newp <span class="ansi-blue-fg">=</span> denormalize<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">,</span> normalized_value_p<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>     newy_test <span class="ansi-blue-fg">=</span> denormalize<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">,</span> normalized_value_y_test<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>     plt2<span class="ansi-blue-fg">.</span>plot<span class="ansi-blue-fg">(</span>newp<span class="ansi-blue-fg">,</span> color<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&#39;red&#39;</span><span class="ansi-blue-fg">,</span> label<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&#39;Prediction&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span>     plt2<span class="ansi-blue-fg">.</span>plot<span class="ansi-blue-fg">(</span>newy_test<span class="ansi-blue-fg">,</span>color<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&#39;blue&#39;</span><span class="ansi-blue-fg">,</span> label<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&#39;Actual&#39;</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-56-e8bf44b2c81e&gt;</span> in <span class="ansi-cyan-fg">denormalize</span><span class="ansi-blue-fg">(data, normalized_value)</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-green-fg">def</span> denormalize<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">,</span> normalized_value<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg">     </span>start <span class="ansi-blue-fg">=</span> datetime<span class="ansi-blue-fg">.</span>datetime<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">2000</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>     end <span class="ansi-blue-fg">=</span> datetime<span class="ansi-blue-fg">.</span>date<span class="ansi-blue-fg">.</span>today<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>     df <span class="ansi-blue-fg">=</span> read_csv<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">,</span> index_col<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> 

<span class="ansi-red-fg">AttributeError</span>: type object &#39;datetime.datetime&#39; has no attribute &#39;datetime&#39;</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="10.-Save-for-consistency">10. Save for consistency<a class="anchor-link" href="#10.-Save-for-consistency">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># model.save(&#39;LSTM_Stock_prediction-20170429.h5&#39;)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Part-2.-Fine-tune-model">Part 2. Fine tune model<a class="anchor-link" href="#Part-2.-Fine-tune-model">&#182;</a></h1><h1 id="11.-Function-to-load-data,-train-model-and-see-score">11. Function to load data, train model and see score<a class="anchor-link" href="#11.-Function-to-load-data,-train-model-and-see-score">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stock_name</span> <span class="o">=</span> <span class="s1">&#39;^GSPC&#39;</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># feature, window, output</span>
<span class="n">neurons</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">300</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">quick_measure</span><span class="p">(</span><span class="n">stock_name</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">get_stock_data</span><span class="p">(</span><span class="n">stock_name</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">build_model2</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># model.save(&#39;LSTM_Stock_prediction-20170429.h5&#39;)</span>
    <span class="n">trainScore</span><span class="p">,</span> <span class="n">testScore</span> <span class="o">=</span> <span class="n">model_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">trainScore</span><span class="p">,</span> <span class="n">testScore</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="12.-Fine-tune-hyperparameter">12. Fine tune hyperparameter<a class="anchor-link" href="#12.-Fine-tune-hyperparameter">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>12.1 Optimial Dropout value</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dlist</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span>
<span class="n">neurons_LSTM</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">]</span>
<span class="n">dropout_result</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dlist</span><span class="p">:</span>    
    <span class="n">trainScore</span><span class="p">,</span> <span class="n">testScore</span> <span class="o">=</span> <span class="n">quick_measure</span><span class="p">(</span><span class="n">stock_name</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">epochs</span><span class="p">)</span>
    <span class="n">dropout_result</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">testScore</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_17 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_17 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_18 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_18 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_17 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_18 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/300
13702/13702 [==============================] - 3s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.0042 - val_acc: 0.0000e+000
Epoch 2/300
13702/13702 [==============================] - 1s - loss: 7.1804e-04 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 3/300
13702/13702 [==============================] - 1s - loss: 2.5058e-04 - acc: 0.0000e+00 - val_loss: 5.7324e-04 - val_acc: 0.0000e+00
Epoch 4/300
13702/13702 [==============================] - 1s - loss: 1.6854e-04 - acc: 0.0000e+00 - val_loss: 3.3299e-04 - val_acc: 0.0000e+00
Epoch 5/300
13702/13702 [==============================] - 1s - loss: 1.3460e-04 - acc: 0.0000e+00 - val_loss: 2.6412e-04 - val_acc: 0.0000e+00
Epoch 6/300
13702/13702 [==============================] - 1s - loss: 1.4755e-04 - acc: 0.0000e+00 - val_loss: 3.4630e-04 - val_acc: 0.0000e+00
Epoch 7/300
13702/13702 [==============================] - 1s - loss: 1.4434e-04 - acc: 0.0000e+00 - val_loss: 2.6879e-04 - val_acc: 0.0000e+00
Epoch 8/300
13702/13702 [==============================] - 1s - loss: 1.3127e-04 - acc: 0.0000e+00 - val_loss: 2.3772e-04 - val_acc: 0.0000e+00
Epoch 9/300
13702/13702 [==============================] - 1s - loss: 1.2684e-04 - acc: 0.0000e+00 - val_loss: 3.5324e-04 - val_acc: 0.0000e+00
Epoch 10/300
13702/13702 [==============================] - 1s - loss: 1.2834e-04 - acc: 0.0000e+00 - val_loss: 2.5947e-04 - val_acc: 0.0000e+00
Epoch 11/300
13702/13702 [==============================] - 1s - loss: 1.2607e-04 - acc: 0.0000e+00 - val_loss: 2.4281e-04 - val_acc: 0.0000e+00
Epoch 12/300
13702/13702 [==============================] - 1s - loss: 1.1852e-04 - acc: 0.0000e+00 - val_loss: 3.0913e-04 - val_acc: 0.0000e+00
Epoch 13/300
13702/13702 [==============================] - 1s - loss: 1.2246e-04 - acc: 0.0000e+00 - val_loss: 3.9577e-04 - val_acc: 0.0000e+00
Epoch 14/300
13702/13702 [==============================] - 1s - loss: 1.2297e-04 - acc: 0.0000e+00 - val_loss: 2.1565e-04 - val_acc: 0.0000e+00
Epoch 15/300
13702/13702 [==============================] - 1s - loss: 1.1701e-04 - acc: 0.0000e+00 - val_loss: 3.2281e-04 - val_acc: 0.0000e+00
Epoch 16/300
13702/13702 [==============================] - 1s - loss: 1.1031e-04 - acc: 0.0000e+00 - val_loss: 2.1198e-04 - val_acc: 0.0000e+00
Epoch 17/300
13702/13702 [==============================] - 1s - loss: 1.0591e-04 - acc: 0.0000e+00 - val_loss: 2.7185e-04 - val_acc: 0.0000e+00
Epoch 18/300
13702/13702 [==============================] - 1s - loss: 1.1723e-04 - acc: 0.0000e+00 - val_loss: 4.2473e-04 - val_acc: 0.0000e+00
Epoch 19/300
13702/13702 [==============================] - 1s - loss: 1.0234e-04 - acc: 0.0000e+00 - val_loss: 2.0022e-04 - val_acc: 0.0000e+00
Epoch 20/300
13702/13702 [==============================] - 1s - loss: 1.1399e-04 - acc: 0.0000e+00 - val_loss: 3.9876e-04 - val_acc: 0.0000e+00
Epoch 21/300
13702/13702 [==============================] - 1s - loss: 1.2373e-04 - acc: 0.0000e+00 - val_loss: 2.4653e-04 - val_acc: 0.0000e+00
Epoch 22/300
13702/13702 [==============================] - 1s - loss: 1.1187e-04 - acc: 0.0000e+00 - val_loss: 4.2809e-04 - val_acc: 0.0000e+00
Epoch 23/300
13702/13702 [==============================] - 1s - loss: 1.3623e-04 - acc: 0.0000e+00 - val_loss: 2.3799e-04 - val_acc: 0.0000e+00
Epoch 24/300
13702/13702 [==============================] - 1s - loss: 1.0914e-04 - acc: 0.0000e+00 - val_loss: 2.4966e-04 - val_acc: 0.0000e+00
Epoch 25/300
13702/13702 [==============================] - 1s - loss: 1.0163e-04 - acc: 0.0000e+00 - val_loss: 1.7917e-04 - val_acc: 0.0000e+00
Epoch 26/300
13702/13702 [==============================] - 1s - loss: 1.0467e-04 - acc: 0.0000e+00 - val_loss: 2.2979e-04 - val_acc: 0.0000e+00
Epoch 27/300
13702/13702 [==============================] - 1s - loss: 1.0334e-04 - acc: 0.0000e+00 - val_loss: 1.7885e-04 - val_acc: 0.0000e+00
Epoch 28/300
13702/13702 [==============================] - 1s - loss: 1.0457e-04 - acc: 0.0000e+00 - val_loss: 1.8090e-04 - val_acc: 0.0000e+00
Epoch 29/300
13702/13702 [==============================] - 1s - loss: 1.0538e-04 - acc: 0.0000e+00 - val_loss: 1.7517e-04 - val_acc: 0.0000e+00
Epoch 30/300
13702/13702 [==============================] - 1s - loss: 1.1634e-04 - acc: 0.0000e+00 - val_loss: 1.8726e-04 - val_acc: 0.0000e+00
Epoch 31/300
13702/13702 [==============================] - 1s - loss: 1.0294e-04 - acc: 0.0000e+00 - val_loss: 1.7027e-04 - val_acc: 0.0000e+00
Epoch 32/300
13702/13702 [==============================] - 1s - loss: 9.2477e-05 - acc: 0.0000e+00 - val_loss: 2.0702e-04 - val_acc: 0.0000e+00
Epoch 33/300
13702/13702 [==============================] - 1s - loss: 1.0008e-04 - acc: 0.0000e+00 - val_loss: 2.7150e-04 - val_acc: 0.0000e+00
Epoch 34/300
13702/13702 [==============================] - 1s - loss: 1.1197e-04 - acc: 0.0000e+00 - val_loss: 2.9585e-04 - val_acc: 0.0000e+00
Epoch 35/300
13702/13702 [==============================] - 1s - loss: 9.9549e-05 - acc: 0.0000e+00 - val_loss: 3.1780e-04 - val_acc: 0.0000e+00
Epoch 36/300
13702/13702 [==============================] - 1s - loss: 1.1413e-04 - acc: 0.0000e+00 - val_loss: 3.9311e-04 - val_acc: 0.0000e+00
Epoch 37/300
13702/13702 [==============================] - 1s - loss: 9.4315e-05 - acc: 0.0000e+00 - val_loss: 1.6544e-04 - val_acc: 0.0000e+00
Epoch 38/300
13702/13702 [==============================] - 1s - loss: 9.3231e-05 - acc: 0.0000e+00 - val_loss: 1.9984e-04 - val_acc: 0.0000e+00
Epoch 39/300
13702/13702 [==============================] - 1s - loss: 9.7269e-05 - acc: 0.0000e+00 - val_loss: 2.7382e-04 - val_acc: 0.0000e+00
Epoch 40/300
13702/13702 [==============================] - 1s - loss: 1.2887e-04 - acc: 0.0000e+00 - val_loss: 2.5933e-04 - val_acc: 0.0000e+00
Epoch 41/300
13702/13702 [==============================] - 1s - loss: 1.1389e-04 - acc: 0.0000e+00 - val_loss: 4.4738e-04 - val_acc: 0.0000e+00
Epoch 42/300
13702/13702 [==============================] - 1s - loss: 9.3520e-05 - acc: 0.0000e+00 - val_loss: 2.5505e-04 - val_acc: 0.0000e+00
Epoch 43/300
13702/13702 [==============================] - 1s - loss: 9.1500e-05 - acc: 0.0000e+00 - val_loss: 1.7357e-04 - val_acc: 0.0000e+00
Epoch 44/300
13702/13702 [==============================] - 1s - loss: 9.1550e-05 - acc: 0.0000e+00 - val_loss: 1.4911e-04 - val_acc: 0.0000e+00
Epoch 45/300
13702/13702 [==============================] - 1s - loss: 8.8440e-05 - acc: 0.0000e+00 - val_loss: 1.9152e-04 - val_acc: 0.0000e+00
Epoch 46/300
13702/13702 [==============================] - 1s - loss: 8.1258e-05 - acc: 0.0000e+00 - val_loss: 1.6780e-04 - val_acc: 0.0000e+00
Epoch 47/300
13702/13702 [==============================] - 1s - loss: 9.9805e-05 - acc: 0.0000e+00 - val_loss: 2.6091e-04 - val_acc: 0.0000e+00
Epoch 48/300
13702/13702 [==============================] - 1s - loss: 9.2405e-05 - acc: 0.0000e+00 - val_loss: 2.0304e-04 - val_acc: 0.0000e+00
Epoch 49/300
13702/13702 [==============================] - 1s - loss: 8.1249e-05 - acc: 0.0000e+00 - val_loss: 1.6803e-04 - val_acc: 0.0000e+00
Epoch 50/300
13702/13702 [==============================] - 1s - loss: 9.1792e-05 - acc: 0.0000e+00 - val_loss: 4.1523e-04 - val_acc: 0.0000e+00
Epoch 51/300
13702/13702 [==============================] - 1s - loss: 9.8591e-05 - acc: 0.0000e+00 - val_loss: 1.4510e-04 - val_acc: 0.0000e+00
Epoch 52/300
13702/13702 [==============================] - 1s - loss: 1.0371e-04 - acc: 0.0000e+00 - val_loss: 1.5422e-04 - val_acc: 0.0000e+00
Epoch 53/300
13702/13702 [==============================] - 1s - loss: 8.6938e-05 - acc: 0.0000e+00 - val_loss: 2.7897e-04 - val_acc: 0.0000e+00
Epoch 54/300
13702/13702 [==============================] - 1s - loss: 9.1394e-05 - acc: 0.0000e+00 - val_loss: 1.8889e-04 - val_acc: 0.0000e+00
Epoch 55/300
13702/13702 [==============================] - 1s - loss: 8.7931e-05 - acc: 0.0000e+00 - val_loss: 1.3869e-04 - val_acc: 0.0000e+00
Epoch 56/300
13702/13702 [==============================] - 1s - loss: 8.0916e-05 - acc: 0.0000e+00 - val_loss: 1.3568e-04 - val_acc: 0.0000e+00
Epoch 57/300
13702/13702 [==============================] - 1s - loss: 8.3131e-05 - acc: 0.0000e+00 - val_loss: 1.4042e-04 - val_acc: 0.0000e+00
Epoch 58/300
13702/13702 [==============================] - 1s - loss: 8.3213e-05 - acc: 0.0000e+00 - val_loss: 1.4419e-04 - val_acc: 0.0000e+00
Epoch 59/300
13702/13702 [==============================] - 1s - loss: 8.7929e-05 - acc: 0.0000e+00 - val_loss: 1.3178e-04 - val_acc: 0.0000e+00
Epoch 60/300
13702/13702 [==============================] - 1s - loss: 7.9790e-05 - acc: 0.0000e+00 - val_loss: 1.7678e-04 - val_acc: 0.0000e+00
Epoch 61/300
13702/13702 [==============================] - 1s - loss: 7.9061e-05 - acc: 0.0000e+00 - val_loss: 1.3136e-04 - val_acc: 0.0000e+00
Epoch 62/300
13702/13702 [==============================] - 1s - loss: 8.4272e-05 - acc: 0.0000e+00 - val_loss: 1.3351e-04 - val_acc: 0.0000e+00
Epoch 63/300
13702/13702 [==============================] - 1s - loss: 9.8430e-05 - acc: 0.0000e+00 - val_loss: 1.3938e-04 - val_acc: 0.0000e+00
Epoch 64/300
13702/13702 [==============================] - 1s - loss: 8.3179e-05 - acc: 0.0000e+00 - val_loss: 1.3191e-04 - val_acc: 0.0000e+00
Epoch 65/300
13702/13702 [==============================] - 1s - loss: 8.0624e-05 - acc: 0.0000e+00 - val_loss: 1.6184e-04 - val_acc: 0.0000e+00
Epoch 66/300
13702/13702 [==============================] - 1s - loss: 8.5314e-05 - acc: 0.0000e+00 - val_loss: 1.2594e-04 - val_acc: 0.0000e+00
Epoch 67/300
13702/13702 [==============================] - 1s - loss: 7.9620e-05 - acc: 0.0000e+00 - val_loss: 4.9358e-04 - val_acc: 0.0000e+00
Epoch 68/300
13702/13702 [==============================] - 1s - loss: 8.1658e-05 - acc: 0.0000e+00 - val_loss: 1.2793e-04 - val_acc: 0.0000e+00
Epoch 69/300
13702/13702 [==============================] - 1s - loss: 7.9868e-05 - acc: 0.0000e+00 - val_loss: 1.7053e-04 - val_acc: 0.0000e+00
Epoch 70/300
13702/13702 [==============================] - 1s - loss: 8.0058e-05 - acc: 0.0000e+00 - val_loss: 2.7771e-04 - val_acc: 0.0000e+00
Epoch 71/300
13702/13702 [==============================] - 1s - loss: 9.6162e-05 - acc: 0.0000e+00 - val_loss: 1.5282e-04 - val_acc: 0.0000e+00
Epoch 72/300
13702/13702 [==============================] - 1s - loss: 9.0946e-05 - acc: 0.0000e+00 - val_loss: 1.4062e-04 - val_acc: 0.0000e+00
Epoch 73/300
13702/13702 [==============================] - 1s - loss: 7.5089e-05 - acc: 0.0000e+00 - val_loss: 1.2988e-04 - val_acc: 0.0000e+00
Epoch 74/300
13702/13702 [==============================] - 1s - loss: 7.4359e-05 - acc: 0.0000e+00 - val_loss: 2.0560e-04 - val_acc: 0.0000e+00
Epoch 75/300
13702/13702 [==============================] - 1s - loss: 7.4188e-05 - acc: 0.0000e+00 - val_loss: 1.6070e-04 - val_acc: 0.0000e+00
Epoch 76/300
13702/13702 [==============================] - 1s - loss: 8.0277e-05 - acc: 0.0000e+00 - val_loss: 1.5223e-04 - val_acc: 0.0000e+00
Epoch 77/300
13702/13702 [==============================] - 1s - loss: 6.8316e-05 - acc: 0.0000e+00 - val_loss: 1.2364e-04 - val_acc: 0.0000e+00
Epoch 78/300
13702/13702 [==============================] - 1s - loss: 7.5891e-05 - acc: 0.0000e+00 - val_loss: 1.9934e-04 - val_acc: 0.0000e+00
Epoch 79/300
13702/13702 [==============================] - 1s - loss: 7.6773e-05 - acc: 0.0000e+00 - val_loss: 1.1844e-04 - val_acc: 0.0000e+00
Epoch 80/300
13702/13702 [==============================] - 1s - loss: 6.7297e-05 - acc: 0.0000e+00 - val_loss: 1.1918e-04 - val_acc: 0.0000e+00
Epoch 81/300
13702/13702 [==============================] - 1s - loss: 8.1163e-05 - acc: 0.0000e+00 - val_loss: 1.5928e-04 - val_acc: 0.0000e+00
Epoch 82/300
13702/13702 [==============================] - 1s - loss: 7.8433e-05 - acc: 0.0000e+00 - val_loss: 2.8398e-04 - val_acc: 0.0000e+00
Epoch 83/300
13702/13702 [==============================] - 1s - loss: 7.9775e-05 - acc: 0.0000e+00 - val_loss: 1.7573e-04 - val_acc: 0.0000e+00
Epoch 84/300
13702/13702 [==============================] - 1s - loss: 7.0252e-05 - acc: 0.0000e+00 - val_loss: 1.1541e-04 - val_acc: 0.0000e+00
Epoch 85/300
13702/13702 [==============================] - 1s - loss: 6.7584e-05 - acc: 0.0000e+00 - val_loss: 1.1207e-04 - val_acc: 0.0000e+00
Epoch 86/300
13702/13702 [==============================] - 1s - loss: 6.8474e-05 - acc: 0.0000e+00 - val_loss: 1.1507e-04 - val_acc: 0.0000e+00
Epoch 87/300
13702/13702 [==============================] - 1s - loss: 7.1574e-05 - acc: 0.0000e+00 - val_loss: 1.3375e-04 - val_acc: 0.0000e+00
Epoch 88/300
13702/13702 [==============================] - 1s - loss: 6.9767e-05 - acc: 0.0000e+00 - val_loss: 4.0851e-04 - val_acc: 0.0000e+00
Epoch 89/300
13702/13702 [==============================] - 1s - loss: 7.2278e-05 - acc: 0.0000e+00 - val_loss: 1.1002e-04 - val_acc: 0.0000e+00
Epoch 90/300
13702/13702 [==============================] - 1s - loss: 6.7948e-05 - acc: 0.0000e+00 - val_loss: 1.7005e-04 - val_acc: 0.0000e+00
Epoch 91/300
13702/13702 [==============================] - 1s - loss: 7.4060e-05 - acc: 0.0000e+00 - val_loss: 1.0776e-04 - val_acc: 0.0000e+00
Epoch 92/300
13702/13702 [==============================] - 1s - loss: 6.5775e-05 - acc: 0.0000e+00 - val_loss: 5.0202e-04 - val_acc: 0.0000e+00
Epoch 93/300
13702/13702 [==============================] - 1s - loss: 7.5600e-05 - acc: 0.0000e+00 - val_loss: 1.0738e-04 - val_acc: 0.0000e+00
Epoch 94/300
13702/13702 [==============================] - 1s - loss: 7.3064e-05 - acc: 0.0000e+00 - val_loss: 1.1468e-04 - val_acc: 0.0000e+00
Epoch 95/300
13702/13702 [==============================] - 1s - loss: 6.4515e-05 - acc: 0.0000e+00 - val_loss: 1.0644e-04 - val_acc: 0.0000e+00
Epoch 96/300
13702/13702 [==============================] - 1s - loss: 7.1642e-05 - acc: 0.0000e+00 - val_loss: 1.2517e-04 - val_acc: 0.0000e+00
Epoch 97/300
13702/13702 [==============================] - 1s - loss: 7.4470e-05 - acc: 0.0000e+00 - val_loss: 3.9936e-04 - val_acc: 0.0000e+00
Epoch 98/300
13702/13702 [==============================] - 1s - loss: 7.4401e-05 - acc: 0.0000e+00 - val_loss: 1.2387e-04 - val_acc: 0.0000e+00
Epoch 99/300
13702/13702 [==============================] - 1s - loss: 7.0160e-05 - acc: 0.0000e+00 - val_loss: 1.1937e-04 - val_acc: 0.0000e+00
Epoch 100/300
13702/13702 [==============================] - 1s - loss: 6.6937e-05 - acc: 0.0000e+00 - val_loss: 1.2306e-04 - val_acc: 0.0000e+00
Epoch 101/300
13702/13702 [==============================] - 1s - loss: 6.7407e-05 - acc: 0.0000e+00 - val_loss: 1.2269e-04 - val_acc: 0.0000e+00
Epoch 102/300
13702/13702 [==============================] - 1s - loss: 6.8668e-05 - acc: 0.0000e+00 - val_loss: 1.0703e-04 - val_acc: 0.0000e+00
Epoch 103/300
13702/13702 [==============================] - 1s - loss: 6.4473e-05 - acc: 0.0000e+00 - val_loss: 1.0153e-04 - val_acc: 0.0000e+00
Epoch 104/300
13702/13702 [==============================] - 1s - loss: 6.3955e-05 - acc: 0.0000e+00 - val_loss: 9.8830e-05 - val_acc: 0.0000e+00
Epoch 105/300
13702/13702 [==============================] - 1s - loss: 6.1786e-05 - acc: 0.0000e+00 - val_loss: 2.3667e-04 - val_acc: 0.0000e+00
Epoch 106/300
13702/13702 [==============================] - 1s - loss: 6.4980e-05 - acc: 0.0000e+00 - val_loss: 1.2261e-04 - val_acc: 0.0000e+00
Epoch 107/300
13702/13702 [==============================] - 1s - loss: 6.8195e-05 - acc: 0.0000e+00 - val_loss: 1.3106e-04 - val_acc: 0.0000e+00
Epoch 108/300
13702/13702 [==============================] - 1s - loss: 6.2237e-05 - acc: 0.0000e+00 - val_loss: 1.2377e-04 - val_acc: 0.0000e+00
Epoch 109/300
13702/13702 [==============================] - 1s - loss: 6.1468e-05 - acc: 0.0000e+00 - val_loss: 1.0991e-04 - val_acc: 0.0000e+00
Epoch 110/300
13702/13702 [==============================] - 1s - loss: 6.7530e-05 - acc: 0.0000e+00 - val_loss: 1.1433e-04 - val_acc: 0.0000e+00
Epoch 111/300
13702/13702 [==============================] - 1s - loss: 6.1319e-05 - acc: 0.0000e+00 - val_loss: 9.6058e-05 - val_acc: 0.0000e+00
Epoch 112/300
13702/13702 [==============================] - 1s - loss: 6.2212e-05 - acc: 0.0000e+00 - val_loss: 9.4320e-05 - val_acc: 0.0000e+00
Epoch 113/300
13702/13702 [==============================] - 1s - loss: 5.5771e-05 - acc: 0.0000e+00 - val_loss: 1.6074e-04 - val_acc: 0.0000e+00
Epoch 114/300
13702/13702 [==============================] - 1s - loss: 6.8031e-05 - acc: 0.0000e+00 - val_loss: 1.2573e-04 - val_acc: 0.0000e+00
Epoch 115/300
13702/13702 [==============================] - 1s - loss: 6.1193e-05 - acc: 0.0000e+00 - val_loss: 9.4208e-05 - val_acc: 0.0000e+00
Epoch 116/300
13702/13702 [==============================] - 1s - loss: 6.7699e-05 - acc: 0.0000e+00 - val_loss: 2.1584e-04 - val_acc: 0.0000e+00
Epoch 117/300
13702/13702 [==============================] - 1s - loss: 6.7743e-05 - acc: 0.0000e+00 - val_loss: 9.3458e-05 - val_acc: 0.0000e+00
Epoch 118/300
13702/13702 [==============================] - 1s - loss: 6.4026e-05 - acc: 0.0000e+00 - val_loss: 1.2577e-04 - val_acc: 0.0000e+000
Epoch 119/300
13702/13702 [==============================] - 1s - loss: 5.8184e-05 - acc: 0.0000e+00 - val_loss: 9.8625e-05 - val_acc: 0.0000e+00
Epoch 120/300
13702/13702 [==============================] - 1s - loss: 6.1009e-05 - acc: 0.0000e+00 - val_loss: 9.4831e-05 - val_acc: 0.0000e+00
Epoch 121/300
13702/13702 [==============================] - 1s - loss: 7.1501e-05 - acc: 0.0000e+00 - val_loss: 1.4569e-04 - val_acc: 0.0000e+00
Epoch 122/300
13702/13702 [==============================] - 1s - loss: 7.2453e-05 - acc: 0.0000e+00 - val_loss: 1.1958e-04 - val_acc: 0.0000e+00
Epoch 123/300
13702/13702 [==============================] - 1s - loss: 5.8398e-05 - acc: 0.0000e+00 - val_loss: 2.0335e-04 - val_acc: 0.0000e+00
Epoch 124/300
13702/13702 [==============================] - 1s - loss: 6.5667e-05 - acc: 0.0000e+00 - val_loss: 9.9490e-05 - val_acc: 0.0000e+00
Epoch 125/300
13702/13702 [==============================] - 1s - loss: 5.8007e-05 - acc: 0.0000e+00 - val_loss: 2.0423e-04 - val_acc: 0.0000e+00
Epoch 126/300
13702/13702 [==============================] - 1s - loss: 6.2200e-05 - acc: 0.0000e+00 - val_loss: 9.0454e-05 - val_acc: 0.0000e+00
Epoch 127/300
13702/13702 [==============================] - 1s - loss: 5.4705e-05 - acc: 0.0000e+00 - val_loss: 8.8123e-05 - val_acc: 0.0000e+00
Epoch 128/300
13702/13702 [==============================] - 1s - loss: 5.5682e-05 - acc: 0.0000e+00 - val_loss: 8.8921e-05 - val_acc: 0.0000e+00
Epoch 129/300
13702/13702 [==============================] - 1s - loss: 5.7765e-05 - acc: 0.0000e+00 - val_loss: 1.0062e-04 - val_acc: 0.0000e+00
Epoch 130/300
13702/13702 [==============================] - 1s - loss: 5.5074e-05 - acc: 0.0000e+00 - val_loss: 1.4419e-04 - val_acc: 0.0000e+00
Epoch 131/300
13702/13702 [==============================] - 1s - loss: 5.6703e-05 - acc: 0.0000e+00 - val_loss: 1.0327e-04 - val_acc: 0.0000e+00
Epoch 132/300
13702/13702 [==============================] - 1s - loss: 7.0477e-05 - acc: 0.0000e+00 - val_loss: 1.2665e-04 - val_acc: 0.0000e+00
Epoch 133/300
13702/13702 [==============================] - 1s - loss: 6.3568e-05 - acc: 0.0000e+00 - val_loss: 1.2759e-04 - val_acc: 0.0000e+00
Epoch 134/300
13702/13702 [==============================] - 1s - loss: 5.8359e-05 - acc: 0.0000e+00 - val_loss: 1.2939e-04 - val_acc: 0.0000e+00
Epoch 135/300
13702/13702 [==============================] - 1s - loss: 5.7925e-05 - acc: 0.0000e+00 - val_loss: 8.4155e-05 - val_acc: 0.0000e+00
Epoch 136/300
13702/13702 [==============================] - 1s - loss: 5.3628e-05 - acc: 0.0000e+00 - val_loss: 8.6437e-05 - val_acc: 0.0000e+00
Epoch 137/300
13702/13702 [==============================] - 1s - loss: 5.6880e-05 - acc: 0.0000e+00 - val_loss: 9.7719e-05 - val_acc: 0.0000e+00
Epoch 138/300
13702/13702 [==============================] - 1s - loss: 5.5505e-05 - acc: 0.0000e+00 - val_loss: 1.7345e-04 - val_acc: 0.0000e+00
Epoch 139/300
13702/13702 [==============================] - 1s - loss: 5.3116e-05 - acc: 0.0000e+00 - val_loss: 1.1106e-04 - val_acc: 0.0000e+00
Epoch 140/300
13702/13702 [==============================] - 1s - loss: 5.6271e-05 - acc: 0.0000e+00 - val_loss: 2.3537e-04 - val_acc: 0.0000e+00
Epoch 141/300
13702/13702 [==============================] - 1s - loss: 5.9915e-05 - acc: 0.0000e+00 - val_loss: 8.6074e-05 - val_acc: 0.0000e+00
Epoch 142/300
13702/13702 [==============================] - 1s - loss: 5.2216e-05 - acc: 0.0000e+00 - val_loss: 9.3876e-05 - val_acc: 0.0000e+00
Epoch 143/300
13702/13702 [==============================] - 1s - loss: 5.6524e-05 - acc: 0.0000e+00 - val_loss: 8.8528e-05 - val_acc: 0.0000e+00
Epoch 144/300
13702/13702 [==============================] - 1s - loss: 5.4530e-05 - acc: 0.0000e+00 - val_loss: 8.6354e-05 - val_acc: 0.0000e+00
Epoch 145/300
13702/13702 [==============================] - 1s - loss: 5.4430e-05 - acc: 0.0000e+00 - val_loss: 8.5768e-05 - val_acc: 0.0000e+00
Epoch 146/300
13702/13702 [==============================] - 1s - loss: 5.8469e-05 - acc: 0.0000e+00 - val_loss: 8.1132e-05 - val_acc: 0.0000e+00
Epoch 147/300
13702/13702 [==============================] - 1s - loss: 5.1446e-05 - acc: 0.0000e+00 - val_loss: 8.1386e-05 - val_acc: 0.0000e+00
Epoch 148/300
13702/13702 [==============================] - 1s - loss: 5.8661e-05 - acc: 0.0000e+00 - val_loss: 3.1817e-04 - val_acc: 0.0000e+00
Epoch 149/300
13702/13702 [==============================] - 1s - loss: 5.3277e-05 - acc: 0.0000e+00 - val_loss: 1.0148e-04 - val_acc: 0.0000e+00
Epoch 150/300
13702/13702 [==============================] - 1s - loss: 4.9854e-05 - acc: 0.0000e+00 - val_loss: 1.3226e-04 - val_acc: 0.0000e+00
Epoch 151/300
13702/13702 [==============================] - 1s - loss: 5.2323e-05 - acc: 0.0000e+00 - val_loss: 8.9075e-05 - val_acc: 0.0000e+00
Epoch 152/300
13702/13702 [==============================] - 1s - loss: 5.2540e-05 - acc: 0.0000e+00 - val_loss: 2.1459e-04 - val_acc: 0.0000e+00
Epoch 153/300
13702/13702 [==============================] - 1s - loss: 4.8624e-05 - acc: 0.0000e+00 - val_loss: 1.1674e-04 - val_acc: 0.0000e+00
Epoch 154/300
13702/13702 [==============================] - 1s - loss: 4.7031e-05 - acc: 0.0000e+00 - val_loss: 1.0599e-04 - val_acc: 0.0000e+00
Epoch 155/300
13702/13702 [==============================] - 1s - loss: 5.3968e-05 - acc: 0.0000e+00 - val_loss: 1.1632e-04 - val_acc: 0.0000e+00
Epoch 156/300
13702/13702 [==============================] - 1s - loss: 4.9123e-05 - acc: 0.0000e+00 - val_loss: 2.2355e-04 - val_acc: 0.0000e+00
Epoch 157/300
13702/13702 [==============================] - 1s - loss: 4.7779e-05 - acc: 0.0000e+00 - val_loss: 7.7570e-05 - val_acc: 0.0000e+00
Epoch 158/300
13702/13702 [==============================] - 1s - loss: 4.4991e-05 - acc: 0.0000e+00 - val_loss: 9.6279e-05 - val_acc: 0.0000e+00
Epoch 159/300
13702/13702 [==============================] - 1s - loss: 4.5642e-05 - acc: 0.0000e+00 - val_loss: 1.3818e-04 - val_acc: 0.0000e+00
Epoch 160/300
13702/13702 [==============================] - 1s - loss: 5.8025e-05 - acc: 0.0000e+00 - val_loss: 1.0016e-04 - val_acc: 0.0000e+00
Epoch 161/300
13702/13702 [==============================] - 1s - loss: 4.3952e-05 - acc: 0.0000e+00 - val_loss: 8.8537e-05 - val_acc: 0.0000e+00
Epoch 162/300
13702/13702 [==============================] - 1s - loss: 4.4084e-05 - acc: 0.0000e+00 - val_loss: 9.0497e-05 - val_acc: 0.0000e+00
Epoch 163/300
13702/13702 [==============================] - 1s - loss: 5.0225e-05 - acc: 0.0000e+00 - val_loss: 8.4298e-05 - val_acc: 0.0000e+00
Epoch 164/300
13702/13702 [==============================] - 1s - loss: 4.6301e-05 - acc: 0.0000e+00 - val_loss: 1.5655e-04 - val_acc: 0.0000e+00
Epoch 165/300
13702/13702 [==============================] - 1s - loss: 5.1345e-05 - acc: 0.0000e+00 - val_loss: 8.2517e-05 - val_acc: 0.0000e+00
Epoch 166/300
13702/13702 [==============================] - 1s - loss: 4.9469e-05 - acc: 0.0000e+00 - val_loss: 1.1771e-04 - val_acc: 0.0000e+00
Epoch 167/300
13702/13702 [==============================] - 1s - loss: 5.0253e-05 - acc: 0.0000e+00 - val_loss: 7.3709e-05 - val_acc: 0.0000e+00
Epoch 168/300
13702/13702 [==============================] - 1s - loss: 5.3160e-05 - acc: 0.0000e+00 - val_loss: 1.1318e-04 - val_acc: 0.0000e+0000e+0
Epoch 169/300
13702/13702 [==============================] - 1s - loss: 4.0338e-05 - acc: 0.0000e+00 - val_loss: 7.5163e-05 - val_acc: 0.0000e+00
Epoch 170/300
13702/13702 [==============================] - 1s - loss: 4.5219e-05 - acc: 0.0000e+00 - val_loss: 7.6992e-05 - val_acc: 0.0000e+00
Epoch 171/300
13702/13702 [==============================] - 1s - loss: 4.7646e-05 - acc: 0.0000e+00 - val_loss: 1.3472e-04 - val_acc: 0.0000e+00
Epoch 172/300
13702/13702 [==============================] - 1s - loss: 5.1729e-05 - acc: 0.0000e+00 - val_loss: 1.4647e-04 - val_acc: 0.0000e+00
Epoch 173/300
13702/13702 [==============================] - 1s - loss: 4.5697e-05 - acc: 0.0000e+00 - val_loss: 1.0430e-04 - val_acc: 0.0000e+00
Epoch 174/300
13702/13702 [==============================] - 1s - loss: 4.9417e-05 - acc: 0.0000e+00 - val_loss: 1.0401e-04 - val_acc: 0.0000e+00
Epoch 175/300
13702/13702 [==============================] - 1s - loss: 4.3279e-05 - acc: 0.0000e+00 - val_loss: 7.7741e-05 - val_acc: 0.0000e+00
Epoch 176/300
13702/13702 [==============================] - 1s - loss: 4.3057e-05 - acc: 0.0000e+00 - val_loss: 2.2752e-04 - val_acc: 0.0000e+00
Epoch 177/300
13702/13702 [==============================] - 1s - loss: 5.3651e-05 - acc: 0.0000e+00 - val_loss: 8.5790e-05 - val_acc: 0.0000e+00
Epoch 178/300
13702/13702 [==============================] - 1s - loss: 4.4848e-05 - acc: 0.0000e+00 - val_loss: 8.4086e-05 - val_acc: 0.0000e+00
Epoch 179/300
13702/13702 [==============================] - 1s - loss: 4.2553e-05 - acc: 0.0000e+00 - val_loss: 7.3760e-05 - val_acc: 0.0000e+00
Epoch 180/300
13702/13702 [==============================] - 1s - loss: 4.0531e-05 - acc: 0.0000e+00 - val_loss: 8.0072e-05 - val_acc: 0.0000e+00
Epoch 181/300
13702/13702 [==============================] - 1s - loss: 4.5025e-05 - acc: 0.0000e+00 - val_loss: 7.7309e-05 - val_acc: 0.0000e+00
Epoch 182/300
13702/13702 [==============================] - 1s - loss: 4.1298e-05 - acc: 0.0000e+00 - val_loss: 1.4066e-04 - val_acc: 0.0000e+00
Epoch 183/300
13702/13702 [==============================] - 1s - loss: 4.2260e-05 - acc: 0.0000e+00 - val_loss: 8.5297e-05 - val_acc: 0.0000e+00
Epoch 184/300
13702/13702 [==============================] - 1s - loss: 4.1828e-05 - acc: 0.0000e+00 - val_loss: 8.2775e-05 - val_acc: 0.0000e+00
Epoch 185/300
13702/13702 [==============================] - 1s - loss: 4.2156e-05 - acc: 0.0000e+00 - val_loss: 7.7451e-05 - val_acc: 0.0000e+00
Epoch 186/300
13702/13702 [==============================] - 1s - loss: 4.6815e-05 - acc: 0.0000e+00 - val_loss: 7.2587e-05 - val_acc: 0.0000e+00
Epoch 187/300
13702/13702 [==============================] - 1s - loss: 4.2749e-05 - acc: 0.0000e+00 - val_loss: 8.9476e-05 - val_acc: 0.0000e+00
Epoch 188/300
13702/13702 [==============================] - 1s - loss: 4.5322e-05 - acc: 0.0000e+00 - val_loss: 9.5393e-05 - val_acc: 0.0000e+00
Epoch 189/300
13702/13702 [==============================] - 1s - loss: 4.3667e-05 - acc: 0.0000e+00 - val_loss: 7.1974e-05 - val_acc: 0.0000e+00
Epoch 190/300
13702/13702 [==============================] - 1s - loss: 4.3043e-05 - acc: 0.0000e+00 - val_loss: 8.0163e-05 - val_acc: 0.0000e+000
Epoch 191/300
13702/13702 [==============================] - 1s - loss: 4.0775e-05 - acc: 0.0000e+00 - val_loss: 7.2108e-05 - val_acc: 0.0000e+00
Epoch 192/300
13702/13702 [==============================] - 1s - loss: 4.1936e-05 - acc: 0.0000e+00 - val_loss: 8.0526e-05 - val_acc: 0.0000e+00
Epoch 193/300
13702/13702 [==============================] - 1s - loss: 3.9858e-05 - acc: 0.0000e+00 - val_loss: 7.6074e-05 - val_acc: 0.0000e+00
Epoch 194/300
13702/13702 [==============================] - 1s - loss: 4.0554e-05 - acc: 0.0000e+00 - val_loss: 7.5793e-05 - val_acc: 0.0000e+00
Epoch 195/300
13702/13702 [==============================] - 1s - loss: 4.0123e-05 - acc: 0.0000e+00 - val_loss: 1.4093e-04 - val_acc: 0.0000e+00
Epoch 196/300
13702/13702 [==============================] - 1s - loss: 4.1319e-05 - acc: 0.0000e+00 - val_loss: 7.1386e-05 - val_acc: 0.0000e+00
Epoch 197/300
13702/13702 [==============================] - 1s - loss: 4.8639e-05 - acc: 0.0000e+00 - val_loss: 8.0636e-05 - val_acc: 0.0000e+00
Epoch 198/300
13702/13702 [==============================] - 1s - loss: 3.7618e-05 - acc: 0.0000e+00 - val_loss: 9.2821e-05 - val_acc: 0.0000e+00
Epoch 199/300
13702/13702 [==============================] - 1s - loss: 4.3011e-05 - acc: 0.0000e+00 - val_loss: 7.5260e-05 - val_acc: 0.0000e+00
Epoch 200/300
13702/13702 [==============================] - 1s - loss: 4.0749e-05 - acc: 0.0000e+00 - val_loss: 8.2678e-05 - val_acc: 0.0000e+00
Epoch 201/300
13702/13702 [==============================] - 1s - loss: 4.3301e-05 - acc: 0.0000e+00 - val_loss: 1.2248e-04 - val_acc: 0.0000e+00
Epoch 202/300
13702/13702 [==============================] - 1s - loss: 4.1240e-05 - acc: 0.0000e+00 - val_loss: 8.5474e-05 - val_acc: 0.0000e+00
Epoch 203/300
13702/13702 [==============================] - 1s - loss: 4.0708e-05 - acc: 0.0000e+00 - val_loss: 1.0901e-04 - val_acc: 0.0000e+00
Epoch 204/300
13702/13702 [==============================] - 1s - loss: 3.8734e-05 - acc: 0.0000e+00 - val_loss: 1.3232e-04 - val_acc: 0.0000e+00
Epoch 205/300
13702/13702 [==============================] - 1s - loss: 4.0359e-05 - acc: 0.0000e+00 - val_loss: 7.5599e-05 - val_acc: 0.0000e+00
Epoch 206/300
13702/13702 [==============================] - 1s - loss: 4.1969e-05 - acc: 0.0000e+00 - val_loss: 7.0787e-05 - val_acc: 0.0000e+00
Epoch 207/300
13702/13702 [==============================] - 1s - loss: 4.0159e-05 - acc: 0.0000e+00 - val_loss: 7.3812e-05 - val_acc: 0.0000e+00
Epoch 208/300
13702/13702 [==============================] - 1s - loss: 3.6949e-05 - acc: 0.0000e+00 - val_loss: 9.3191e-05 - val_acc: 0.0000e+00
Epoch 209/300
13702/13702 [==============================] - 1s - loss: 4.0511e-05 - acc: 0.0000e+00 - val_loss: 8.2620e-05 - val_acc: 0.0000e+00
Epoch 210/300
13702/13702 [==============================] - 1s - loss: 4.0671e-05 - acc: 0.0000e+00 - val_loss: 8.2122e-05 - val_acc: 0.0000e+00
Epoch 211/300
13702/13702 [==============================] - 1s - loss: 3.7721e-05 - acc: 0.0000e+00 - val_loss: 8.8690e-05 - val_acc: 0.0000e+00
Epoch 212/300
13702/13702 [==============================] - 1s - loss: 3.6845e-05 - acc: 0.0000e+00 - val_loss: 1.0747e-04 - val_acc: 0.0000e+00
Epoch 213/300
13702/13702 [==============================] - 1s - loss: 3.7450e-05 - acc: 0.0000e+00 - val_loss: 1.1586e-04 - val_acc: 0.0000e+00
Epoch 214/300
13702/13702 [==============================] - 1s - loss: 3.9962e-05 - acc: 0.0000e+00 - val_loss: 1.3950e-04 - val_acc: 0.0000e+00
Epoch 215/300
13702/13702 [==============================] - 1s - loss: 5.0234e-05 - acc: 0.0000e+00 - val_loss: 8.8173e-05 - val_acc: 0.0000e+00
Epoch 216/300
13702/13702 [==============================] - 1s - loss: 4.5143e-05 - acc: 0.0000e+00 - val_loss: 7.4419e-05 - val_acc: 0.0000e+00
Epoch 217/300
13702/13702 [==============================] - 1s - loss: 3.9481e-05 - acc: 0.0000e+00 - val_loss: 8.4440e-05 - val_acc: 0.0000e+00
Epoch 218/300
13702/13702 [==============================] - 1s - loss: 3.7271e-05 - acc: 0.0000e+00 - val_loss: 7.6987e-05 - val_acc: 0.0000e+00
Epoch 219/300
13702/13702 [==============================] - 1s - loss: 3.8756e-05 - acc: 0.0000e+00 - val_loss: 7.4955e-05 - val_acc: 0.0000e+00
Epoch 220/300
13702/13702 [==============================] - 1s - loss: 3.6757e-05 - acc: 0.0000e+00 - val_loss: 7.2061e-05 - val_acc: 0.0000e+00
Epoch 221/300
13702/13702 [==============================] - 1s - loss: 3.7897e-05 - acc: 0.0000e+00 - val_loss: 8.6368e-05 - val_acc: 0.0000e+00
Epoch 222/300
13702/13702 [==============================] - 1s - loss: 4.0011e-05 - acc: 0.0000e+00 - val_loss: 6.7412e-05 - val_acc: 0.0000e+00
Epoch 223/300
13702/13702 [==============================] - 1s - loss: 3.6490e-05 - acc: 0.0000e+00 - val_loss: 1.0807e-04 - val_acc: 0.0000e+00
Epoch 224/300
13702/13702 [==============================] - 1s - loss: 4.1692e-05 - acc: 0.0000e+00 - val_loss: 1.4258e-04 - val_acc: 0.0000e+00
Epoch 225/300
13702/13702 [==============================] - 1s - loss: 4.2036e-05 - acc: 0.0000e+00 - val_loss: 1.8816e-04 - val_acc: 0.0000e+00
Epoch 226/300
13702/13702 [==============================] - 1s - loss: 4.0426e-05 - acc: 0.0000e+00 - val_loss: 7.8484e-05 - val_acc: 0.0000e+00
Epoch 227/300
13702/13702 [==============================] - 1s - loss: 4.6608e-05 - acc: 0.0000e+00 - val_loss: 9.1184e-05 - val_acc: 0.0000e+00
Epoch 228/300
13702/13702 [==============================] - 1s - loss: 4.1540e-05 - acc: 0.0000e+00 - val_loss: 8.1842e-05 - val_acc: 0.0000e+00
Epoch 229/300
13702/13702 [==============================] - 1s - loss: 4.1842e-05 - acc: 0.0000e+00 - val_loss: 1.1252e-04 - val_acc: 0.0000e+00
Epoch 230/300
13702/13702 [==============================] - 1s - loss: 3.6730e-05 - acc: 0.0000e+00 - val_loss: 7.4458e-05 - val_acc: 0.0000e+00
Epoch 231/300
13702/13702 [==============================] - 1s - loss: 3.5798e-05 - acc: 0.0000e+00 - val_loss: 7.8105e-05 - val_acc: 0.0000e+00
Epoch 232/300
13702/13702 [==============================] - 1s - loss: 3.7380e-05 - acc: 0.0000e+00 - val_loss: 6.7593e-05 - val_acc: 0.0000e+00
Epoch 233/300
13702/13702 [==============================] - 1s - loss: 3.8452e-05 - acc: 0.0000e+00 - val_loss: 7.6469e-05 - val_acc: 0.0000e+00
Epoch 234/300
13702/13702 [==============================] - 1s - loss: 3.7764e-05 - acc: 0.0000e+00 - val_loss: 1.2494e-04 - val_acc: 0.0000e+00
Epoch 235/300
13702/13702 [==============================] - 1s - loss: 3.7386e-05 - acc: 0.0000e+00 - val_loss: 7.5680e-05 - val_acc: 0.0000e+00
Epoch 236/300
13702/13702 [==============================] - 1s - loss: 4.4086e-05 - acc: 0.0000e+00 - val_loss: 1.2589e-04 - val_acc: 0.0000e+00
Epoch 237/300
13702/13702 [==============================] - 1s - loss: 4.0590e-05 - acc: 0.0000e+00 - val_loss: 8.1866e-05 - val_acc: 0.0000e+00
Epoch 238/300
13702/13702 [==============================] - 1s - loss: 4.0282e-05 - acc: 0.0000e+00 - val_loss: 7.8637e-05 - val_acc: 0.0000e+00
Epoch 239/300
13702/13702 [==============================] - 1s - loss: 3.7946e-05 - acc: 0.0000e+00 - val_loss: 1.2135e-04 - val_acc: 0.0000e+00
Epoch 240/300
13702/13702 [==============================] - 1s - loss: 3.6448e-05 - acc: 0.0000e+00 - val_loss: 1.0066e-04 - val_acc: 0.0000e+00
Epoch 241/300
13702/13702 [==============================] - 1s - loss: 3.7334e-05 - acc: 0.0000e+00 - val_loss: 6.7278e-05 - val_acc: 0.0000e+00
Epoch 242/300
13702/13702 [==============================] - 1s - loss: 3.5057e-05 - acc: 0.0000e+00 - val_loss: 6.4258e-05 - val_acc: 0.0000e+00
Epoch 243/300
13702/13702 [==============================] - 1s - loss: 3.3849e-05 - acc: 0.0000e+00 - val_loss: 6.7222e-05 - val_acc: 0.0000e+00
Epoch 244/300
13702/13702 [==============================] - 1s - loss: 3.9607e-05 - acc: 0.0000e+00 - val_loss: 8.1532e-05 - val_acc: 0.0000e+00
Epoch 245/300
13702/13702 [==============================] - 1s - loss: 3.7583e-05 - acc: 0.0000e+00 - val_loss: 6.4113e-05 - val_acc: 0.0000e+00
Epoch 246/300
13702/13702 [==============================] - 1s - loss: 3.7323e-05 - acc: 0.0000e+00 - val_loss: 6.3504e-05 - val_acc: 0.0000e+00
Epoch 247/300
13702/13702 [==============================] - 1s - loss: 3.7508e-05 - acc: 0.0000e+00 - val_loss: 7.6140e-05 - val_acc: 0.0000e+00
Epoch 248/300
13702/13702 [==============================] - 1s - loss: 4.0294e-05 - acc: 0.0000e+00 - val_loss: 1.5945e-04 - val_acc: 0.0000e+000
Epoch 249/300
13702/13702 [==============================] - 1s - loss: 3.8781e-05 - acc: 0.0000e+00 - val_loss: 1.3159e-04 - val_acc: 0.0000e+00
Epoch 250/300
13702/13702 [==============================] - 1s - loss: 3.4642e-05 - acc: 0.0000e+00 - val_loss: 6.6366e-05 - val_acc: 0.0000e+00
Epoch 251/300
13702/13702 [==============================] - 1s - loss: 3.5554e-05 - acc: 0.0000e+00 - val_loss: 8.8685e-05 - val_acc: 0.0000e+00
Epoch 252/300
13702/13702 [==============================] - 1s - loss: 3.3626e-05 - acc: 0.0000e+00 - val_loss: 8.5942e-05 - val_acc: 0.0000e+00
Epoch 253/300
13702/13702 [==============================] - 1s - loss: 3.3702e-05 - acc: 0.0000e+00 - val_loss: 6.8794e-05 - val_acc: 0.0000e+00
Epoch 254/300
13702/13702 [==============================] - 1s - loss: 3.4759e-05 - acc: 0.0000e+00 - val_loss: 8.6218e-05 - val_acc: 0.0000e+00
Epoch 255/300
13702/13702 [==============================] - 1s - loss: 3.6676e-05 - acc: 0.0000e+00 - val_loss: 7.7165e-05 - val_acc: 0.0000e+00
Epoch 256/300
13702/13702 [==============================] - 1s - loss: 3.7643e-05 - acc: 0.0000e+00 - val_loss: 1.2712e-04 - val_acc: 0.0000e+00
Epoch 257/300
13702/13702 [==============================] - 1s - loss: 3.9754e-05 - acc: 0.0000e+00 - val_loss: 1.0928e-04 - val_acc: 0.0000e+00
Epoch 258/300
13702/13702 [==============================] - 1s - loss: 3.7634e-05 - acc: 0.0000e+00 - val_loss: 6.2637e-05 - val_acc: 0.0000e+00
Epoch 259/300
13702/13702 [==============================] - 1s - loss: 3.4605e-05 - acc: 0.0000e+00 - val_loss: 8.5917e-05 - val_acc: 0.0000e+00
Epoch 260/300
13702/13702 [==============================] - 1s - loss: 3.4135e-05 - acc: 0.0000e+00 - val_loss: 1.0912e-04 - val_acc: 0.0000e+00
Epoch 261/300
13702/13702 [==============================] - 1s - loss: 4.0075e-05 - acc: 0.0000e+00 - val_loss: 1.3387e-04 - val_acc: 0.0000e+00
Epoch 262/300
13702/13702 [==============================] - 1s - loss: 4.0382e-05 - acc: 0.0000e+00 - val_loss: 6.0703e-05 - val_acc: 0.0000e+00
Epoch 263/300
13702/13702 [==============================] - 1s - loss: 3.4520e-05 - acc: 0.0000e+00 - val_loss: 6.2738e-05 - val_acc: 0.0000e+00
Epoch 264/300
13702/13702 [==============================] - 1s - loss: 3.4062e-05 - acc: 0.0000e+00 - val_loss: 6.8128e-05 - val_acc: 0.0000e+00
Epoch 265/300
13702/13702 [==============================] - 1s - loss: 3.5214e-05 - acc: 0.0000e+00 - val_loss: 1.0929e-04 - val_acc: 0.0000e+00
Epoch 266/300
13702/13702 [==============================] - 1s - loss: 3.3945e-05 - acc: 0.0000e+00 - val_loss: 6.0944e-05 - val_acc: 0.0000e+00
Epoch 267/300
13702/13702 [==============================] - 1s - loss: 3.4340e-05 - acc: 0.0000e+00 - val_loss: 9.2843e-05 - val_acc: 0.0000e+00
Epoch 268/300
13702/13702 [==============================] - 1s - loss: 3.5414e-05 - acc: 0.0000e+00 - val_loss: 9.1970e-05 - val_acc: 0.0000e+00
Epoch 269/300
13702/13702 [==============================] - 1s - loss: 3.5919e-05 - acc: 0.0000e+00 - val_loss: 6.6458e-05 - val_acc: 0.0000e+00
Epoch 270/300
13702/13702 [==============================] - 1s - loss: 3.4528e-05 - acc: 0.0000e+00 - val_loss: 1.1939e-04 - val_acc: 0.0000e+00
Epoch 271/300
13702/13702 [==============================] - 1s - loss: 3.2757e-05 - acc: 0.0000e+00 - val_loss: 6.1442e-05 - val_acc: 0.0000e+00
Epoch 272/300
13702/13702 [==============================] - 1s - loss: 3.2306e-05 - acc: 0.0000e+00 - val_loss: 8.4997e-05 - val_acc: 0.0000e+00
Epoch 273/300
13702/13702 [==============================] - 1s - loss: 3.4690e-05 - acc: 0.0000e+00 - val_loss: 6.1618e-05 - val_acc: 0.0000e+00
Epoch 274/300
13702/13702 [==============================] - 1s - loss: 3.1589e-05 - acc: 0.0000e+00 - val_loss: 6.4427e-05 - val_acc: 0.0000e+00
Epoch 275/300
13702/13702 [==============================] - 1s - loss: 3.5297e-05 - acc: 0.0000e+00 - val_loss: 5.9463e-05 - val_acc: 0.0000e+00
Epoch 276/300
13702/13702 [==============================] - 1s - loss: 3.1783e-05 - acc: 0.0000e+00 - val_loss: 7.4361e-05 - val_acc: 0.0000e+00
Epoch 277/300
13702/13702 [==============================] - 1s - loss: 3.4933e-05 - acc: 0.0000e+00 - val_loss: 9.4194e-05 - val_acc: 0.0000e+00
Epoch 278/300
13702/13702 [==============================] - 1s - loss: 3.2558e-05 - acc: 0.0000e+00 - val_loss: 6.1318e-05 - val_acc: 0.0000e+00
Epoch 279/300
13702/13702 [==============================] - 1s - loss: 3.8294e-05 - acc: 0.0000e+00 - val_loss: 8.8355e-05 - val_acc: 0.0000e+00
Epoch 280/300
13702/13702 [==============================] - 1s - loss: 3.3186e-05 - acc: 0.0000e+00 - val_loss: 5.9376e-05 - val_acc: 0.0000e+00
Epoch 281/300
13702/13702 [==============================] - 1s - loss: 3.3565e-05 - acc: 0.0000e+00 - val_loss: 6.9707e-05 - val_acc: 0.0000e+00
Epoch 282/300
13702/13702 [==============================] - 1s - loss: 3.3402e-05 - acc: 0.0000e+00 - val_loss: 9.9120e-05 - val_acc: 0.0000e+00
Epoch 283/300
13702/13702 [==============================] - 1s - loss: 3.2211e-05 - acc: 0.0000e+00 - val_loss: 8.1142e-05 - val_acc: 0.0000e+00
Epoch 284/300
13702/13702 [==============================] - 1s - loss: 3.5021e-05 - acc: 0.0000e+00 - val_loss: 1.0247e-04 - val_acc: 0.0000e+00
Epoch 285/300
13702/13702 [==============================] - 1s - loss: 3.7491e-05 - acc: 0.0000e+00 - val_loss: 9.0636e-05 - val_acc: 0.0000e+00
Epoch 286/300
13702/13702 [==============================] - 1s - loss: 3.5320e-05 - acc: 0.0000e+00 - val_loss: 1.1155e-04 - val_acc: 0.0000e+00
Epoch 287/300
13702/13702 [==============================] - 1s - loss: 3.4860e-05 - acc: 0.0000e+00 - val_loss: 6.0941e-05 - val_acc: 0.0000e+00
Epoch 288/300
13702/13702 [==============================] - 1s - loss: 3.3913e-05 - acc: 0.0000e+00 - val_loss: 6.2435e-05 - val_acc: 0.0000e+00
Epoch 289/300
13702/13702 [==============================] - 1s - loss: 3.3048e-05 - acc: 0.0000e+00 - val_loss: 6.6532e-05 - val_acc: 0.0000e+00
Epoch 290/300
13702/13702 [==============================] - 1s - loss: 3.2583e-05 - acc: 0.0000e+00 - val_loss: 6.3181e-05 - val_acc: 0.0000e+00
Epoch 291/300
13702/13702 [==============================] - 1s - loss: 3.0371e-05 - acc: 0.0000e+00 - val_loss: 6.9182e-05 - val_acc: 0.0000e+00
Epoch 292/300
13702/13702 [==============================] - 1s - loss: 3.7278e-05 - acc: 0.0000e+00 - val_loss: 5.7825e-05 - val_acc: 0.0000e+00
Epoch 293/300
13702/13702 [==============================] - 1s - loss: 3.7501e-05 - acc: 0.0000e+00 - val_loss: 6.0554e-05 - val_acc: 0.0000e+00
Epoch 294/300
13702/13702 [==============================] - 1s - loss: 3.5414e-05 - acc: 0.0000e+00 - val_loss: 6.7766e-05 - val_acc: 0.0000e+00
Epoch 295/300
13702/13702 [==============================] - 1s - loss: 3.3548e-05 - acc: 0.0000e+00 - val_loss: 1.3253e-04 - val_acc: 0.0000e+00
Epoch 296/300
13702/13702 [==============================] - 1s - loss: 3.4441e-05 - acc: 0.0000e+00 - val_loss: 2.0411e-04 - val_acc: 0.0000e+00
Epoch 297/300
13702/13702 [==============================] - 1s - loss: 3.8004e-05 - acc: 0.0000e+00 - val_loss: 6.6944e-05 - val_acc: 0.0000e+00
Epoch 298/300
13702/13702 [==============================] - 1s - loss: 3.1322e-05 - acc: 0.0000e+00 - val_loss: 8.6315e-05 - val_acc: 0.0000e+00
Epoch 299/300
13702/13702 [==============================] - 1s - loss: 3.7320e-05 - acc: 0.0000e+00 - val_loss: 1.9673e-04 - val_acc: 0.0000e+00
Epoch 300/300
13702/13702 [==============================] - 1s - loss: 4.2219e-05 - acc: 0.0000e+00 - val_loss: 8.7802e-05 - val_acc: 0.0000e+00
Train Score: 0.00002 MSE (0.00 RMSE)
Test Score: 0.00009 MSE (0.01 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_19 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_19 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_20 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_20 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_19 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_20 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/300
13702/13702 [==============================] - 3s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.0056 - val_acc: 0.0000e+00
Epoch 2/300
13702/13702 [==============================] - 1s - loss: 5.9329e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 3/300
13702/13702 [==============================] - 1s - loss: 3.1191e-04 - acc: 0.0000e+00 - val_loss: 7.3650e-04 - val_acc: 0.0000e+00
Epoch 4/300
13702/13702 [==============================] - 1s - loss: 2.2691e-04 - acc: 0.0000e+00 - val_loss: 3.9015e-04 - val_acc: 0.0000e+00
Epoch 5/300
13702/13702 [==============================] - 1s - loss: 2.1393e-04 - acc: 0.0000e+00 - val_loss: 3.4606e-04 - val_acc: 0.0000e+00
Epoch 6/300
13702/13702 [==============================] - 1s - loss: 1.9696e-04 - acc: 0.0000e+00 - val_loss: 3.3240e-04 - val_acc: 0.0000e+00
Epoch 7/300
13702/13702 [==============================] - 1s - loss: 2.0872e-04 - acc: 0.0000e+00 - val_loss: 6.2244e-04 - val_acc: 0.0000e+00
Epoch 8/300
13702/13702 [==============================] - 1s - loss: 1.8165e-04 - acc: 0.0000e+00 - val_loss: 4.1681e-04 - val_acc: 0.0000e+00
Epoch 9/300
13702/13702 [==============================] - 1s - loss: 1.8759e-04 - acc: 0.0000e+00 - val_loss: 2.3860e-04 - val_acc: 0.0000e+00
Epoch 10/300
13702/13702 [==============================] - 1s - loss: 1.8010e-04 - acc: 0.0000e+00 - val_loss: 2.9187e-04 - val_acc: 0.0000e+00
Epoch 11/300
13702/13702 [==============================] - 1s - loss: 1.6749e-04 - acc: 0.0000e+00 - val_loss: 2.2259e-04 - val_acc: 0.0000e+00
Epoch 12/300
13702/13702 [==============================] - 1s - loss: 1.6794e-04 - acc: 0.0000e+00 - val_loss: 2.7659e-04 - val_acc: 0.0000e+00
Epoch 13/300
13702/13702 [==============================] - 1s - loss: 1.7119e-04 - acc: 0.0000e+00 - val_loss: 3.4938e-04 - val_acc: 0.0000e+00
Epoch 14/300
13702/13702 [==============================] - 1s - loss: 1.7055e-04 - acc: 0.0000e+00 - val_loss: 2.5959e-04 - val_acc: 0.0000e+00
Epoch 15/300
13702/13702 [==============================] - 1s - loss: 1.6964e-04 - acc: 0.0000e+00 - val_loss: 2.1851e-04 - val_acc: 0.0000e+00
Epoch 16/300
13702/13702 [==============================] - 1s - loss: 1.7247e-04 - acc: 0.0000e+00 - val_loss: 2.7470e-04 - val_acc: 0.0000e+00
Epoch 17/300
13702/13702 [==============================] - 1s - loss: 1.8090e-04 - acc: 0.0000e+00 - val_loss: 2.5204e-04 - val_acc: 0.0000e+00
Epoch 18/300
13702/13702 [==============================] - 1s - loss: 1.6944e-04 - acc: 0.0000e+00 - val_loss: 3.0941e-04 - val_acc: 0.0000e+00
Epoch 19/300
13702/13702 [==============================] - 1s - loss: 1.6486e-04 - acc: 0.0000e+00 - val_loss: 2.9131e-04 - val_acc: 0.0000e+00
Epoch 20/300
13702/13702 [==============================] - 1s - loss: 1.5562e-04 - acc: 0.0000e+00 - val_loss: 6.4125e-04 - val_acc: 0.0000e+00
Epoch 21/300
13702/13702 [==============================] - 1s - loss: 1.6698e-04 - acc: 0.0000e+00 - val_loss: 3.1872e-04 - val_acc: 0.0000e+00
Epoch 22/300
13702/13702 [==============================] - 1s - loss: 1.7413e-04 - acc: 0.0000e+00 - val_loss: 3.2942e-04 - val_acc: 0.0000e+00
Epoch 23/300
13702/13702 [==============================] - 1s - loss: 1.4956e-04 - acc: 0.0000e+00 - val_loss: 2.0891e-04 - val_acc: 0.0000e+00
Epoch 24/300
13702/13702 [==============================] - 1s - loss: 1.5216e-04 - acc: 0.0000e+00 - val_loss: 5.3458e-04 - val_acc: 0.0000e+00
Epoch 25/300
13702/13702 [==============================] - 1s - loss: 1.4924e-04 - acc: 0.0000e+00 - val_loss: 1.9061e-04 - val_acc: 0.0000e+00
Epoch 26/300
13702/13702 [==============================] - 1s - loss: 1.5519e-04 - acc: 0.0000e+00 - val_loss: 3.3778e-04 - val_acc: 0.0000e+00
Epoch 27/300
13702/13702 [==============================] - 1s - loss: 1.5656e-04 - acc: 0.0000e+00 - val_loss: 1.9585e-04 - val_acc: 0.0000e+00
Epoch 28/300
13702/13702 [==============================] - 1s - loss: 1.4528e-04 - acc: 0.0000e+00 - val_loss: 2.6235e-04 - val_acc: 0.0000e+00
Epoch 29/300
13702/13702 [==============================] - 1s - loss: 1.4306e-04 - acc: 0.0000e+00 - val_loss: 2.9175e-04 - val_acc: 0.0000e+00
Epoch 30/300
13702/13702 [==============================] - 1s - loss: 1.5168e-04 - acc: 0.0000e+00 - val_loss: 4.3717e-04 - val_acc: 0.0000e+00
Epoch 31/300
13702/13702 [==============================] - 1s - loss: 1.6246e-04 - acc: 0.0000e+00 - val_loss: 2.1126e-04 - val_acc: 0.0000e+00
Epoch 32/300
13702/13702 [==============================] - 1s - loss: 1.3043e-04 - acc: 0.0000e+00 - val_loss: 1.9429e-04 - val_acc: 0.0000e+00
Epoch 33/300
13702/13702 [==============================] - 1s - loss: 1.4148e-04 - acc: 0.0000e+00 - val_loss: 8.9457e-04 - val_acc: 0.0000e+00
Epoch 34/300
13702/13702 [==============================] - 1s - loss: 1.6039e-04 - acc: 0.0000e+00 - val_loss: 2.5770e-04 - val_acc: 0.0000e+00
Epoch 35/300
13702/13702 [==============================] - 1s - loss: 1.4683e-04 - acc: 0.0000e+00 - val_loss: 2.3320e-04 - val_acc: 0.0000e+00
Epoch 36/300
13702/13702 [==============================] - 1s - loss: 1.4566e-04 - acc: 0.0000e+00 - val_loss: 1.9916e-04 - val_acc: 0.0000e+00
Epoch 37/300
13702/13702 [==============================] - 1s - loss: 1.4164e-04 - acc: 0.0000e+00 - val_loss: 1.7462e-04 - val_acc: 0.0000e+00
Epoch 38/300
13702/13702 [==============================] - 1s - loss: 1.3509e-04 - acc: 0.0000e+00 - val_loss: 8.0245e-04 - val_acc: 0.0000e+00
Epoch 39/300
13702/13702 [==============================] - 1s - loss: 1.4891e-04 - acc: 0.0000e+00 - val_loss: 1.8696e-04 - val_acc: 0.0000e+00
Epoch 40/300
13702/13702 [==============================] - 1s - loss: 1.3936e-04 - acc: 0.0000e+00 - val_loss: 2.0472e-04 - val_acc: 0.0000e+00
Epoch 41/300
13702/13702 [==============================] - 1s - loss: 1.2747e-04 - acc: 0.0000e+00 - val_loss: 2.3549e-04 - val_acc: 0.0000e+00
Epoch 42/300
13702/13702 [==============================] - 1s - loss: 1.3513e-04 - acc: 0.0000e+00 - val_loss: 2.0077e-04 - val_acc: 0.0000e+00
Epoch 43/300
13702/13702 [==============================] - 1s - loss: 1.1995e-04 - acc: 0.0000e+00 - val_loss: 1.8170e-04 - val_acc: 0.0000e+00
Epoch 44/300
13702/13702 [==============================] - 1s - loss: 1.2854e-04 - acc: 0.0000e+00 - val_loss: 1.7043e-04 - val_acc: 0.0000e+00
Epoch 45/300
13702/13702 [==============================] - 1s - loss: 1.2401e-04 - acc: 0.0000e+00 - val_loss: 1.6290e-04 - val_acc: 0.0000e+00
Epoch 46/300
13702/13702 [==============================] - 1s - loss: 1.3657e-04 - acc: 0.0000e+00 - val_loss: 2.6007e-04 - val_acc: 0.0000e+00
Epoch 47/300
13702/13702 [==============================] - 1s - loss: 1.2806e-04 - acc: 0.0000e+00 - val_loss: 2.5595e-04 - val_acc: 0.0000e+00
Epoch 48/300
13702/13702 [==============================] - 1s - loss: 1.2807e-04 - acc: 0.0000e+00 - val_loss: 1.6139e-04 - val_acc: 0.0000e+00
Epoch 49/300
13702/13702 [==============================] - 1s - loss: 1.3190e-04 - acc: 0.0000e+00 - val_loss: 5.6456e-04 - val_acc: 0.0000e+00
Epoch 50/300
13702/13702 [==============================] - 1s - loss: 1.4592e-04 - acc: 0.0000e+00 - val_loss: 6.4682e-04 - val_acc: 0.0000e+00
Epoch 51/300
13702/13702 [==============================] - 1s - loss: 1.2683e-04 - acc: 0.0000e+00 - val_loss: 1.7727e-04 - val_acc: 0.0000e+00
Epoch 52/300
13702/13702 [==============================] - 1s - loss: 1.1944e-04 - acc: 0.0000e+00 - val_loss: 3.9883e-04 - val_acc: 0.0000e+00
Epoch 53/300
13702/13702 [==============================] - 1s - loss: 1.1767e-04 - acc: 0.0000e+00 - val_loss: 1.6873e-04 - val_acc: 0.0000e+00
Epoch 54/300
13702/13702 [==============================] - 1s - loss: 1.1247e-04 - acc: 0.0000e+00 - val_loss: 2.7337e-04 - val_acc: 0.0000e+00
Epoch 55/300
13702/13702 [==============================] - 1s - loss: 1.2147e-04 - acc: 0.0000e+00 - val_loss: 1.9300e-04 - val_acc: 0.0000e+00
Epoch 56/300
13702/13702 [==============================] - 1s - loss: 1.3180e-04 - acc: 0.0000e+00 - val_loss: 3.2518e-04 - val_acc: 0.0000e+00
Epoch 57/300
13702/13702 [==============================] - 1s - loss: 1.1381e-04 - acc: 0.0000e+00 - val_loss: 1.6086e-04 - val_acc: 0.0000e+000
Epoch 58/300
13702/13702 [==============================] - 1s - loss: 1.0861e-04 - acc: 0.0000e+00 - val_loss: 2.3723e-04 - val_acc: 0.0000e+00
Epoch 59/300
13702/13702 [==============================] - 1s - loss: 1.1677e-04 - acc: 0.0000e+00 - val_loss: 2.9337e-04 - val_acc: 0.0000e+00
Epoch 60/300
13702/13702 [==============================] - 1s - loss: 1.2100e-04 - acc: 0.0000e+00 - val_loss: 2.2275e-04 - val_acc: 0.0000e+00
Epoch 61/300
13702/13702 [==============================] - 1s - loss: 1.0981e-04 - acc: 0.0000e+00 - val_loss: 1.4173e-04 - val_acc: 0.0000e+00
Epoch 62/300
13702/13702 [==============================] - 1s - loss: 1.0934e-04 - acc: 0.0000e+00 - val_loss: 1.5673e-04 - val_acc: 0.0000e+00
Epoch 63/300
13702/13702 [==============================] - 1s - loss: 1.0645e-04 - acc: 0.0000e+00 - val_loss: 4.4192e-04 - val_acc: 0.0000e+00
Epoch 64/300
13702/13702 [==============================] - 1s - loss: 1.3360e-04 - acc: 0.0000e+00 - val_loss: 3.0335e-04 - val_acc: 0.0000e+00
Epoch 65/300
13702/13702 [==============================] - 1s - loss: 1.0373e-04 - acc: 0.0000e+00 - val_loss: 1.4227e-04 - val_acc: 0.0000e+00
Epoch 66/300
13702/13702 [==============================] - 1s - loss: 9.8791e-05 - acc: 0.0000e+00 - val_loss: 1.3383e-04 - val_acc: 0.0000e+00
Epoch 67/300
13702/13702 [==============================] - 1s - loss: 1.0771e-04 - acc: 0.0000e+00 - val_loss: 1.3564e-04 - val_acc: 0.0000e+00
Epoch 68/300
13702/13702 [==============================] - 1s - loss: 1.0725e-04 - acc: 0.0000e+00 - val_loss: 1.3180e-04 - val_acc: 0.0000e+00
Epoch 69/300
13702/13702 [==============================] - 1s - loss: 1.0281e-04 - acc: 0.0000e+00 - val_loss: 2.4815e-04 - val_acc: 0.0000e+00
Epoch 70/300
13702/13702 [==============================] - 1s - loss: 1.1194e-04 - acc: 0.0000e+00 - val_loss: 1.6667e-04 - val_acc: 0.0000e+00
Epoch 71/300
13702/13702 [==============================] - 1s - loss: 1.0393e-04 - acc: 0.0000e+00 - val_loss: 1.3261e-04 - val_acc: 0.0000e+00
Epoch 72/300
13702/13702 [==============================] - 1s - loss: 1.0219e-04 - acc: 0.0000e+00 - val_loss: 1.2947e-04 - val_acc: 0.0000e+00
Epoch 73/300
13702/13702 [==============================] - 1s - loss: 1.0024e-04 - acc: 0.0000e+00 - val_loss: 1.2918e-04 - val_acc: 0.0000e+00
Epoch 74/300
13702/13702 [==============================] - 1s - loss: 1.0023e-04 - acc: 0.0000e+00 - val_loss: 2.9635e-04 - val_acc: 0.0000e+00
Epoch 75/300
13702/13702 [==============================] - 1s - loss: 1.0363e-04 - acc: 0.0000e+00 - val_loss: 1.8837e-04 - val_acc: 0.0000e+00
Epoch 76/300
13702/13702 [==============================] - 1s - loss: 9.9489e-05 - acc: 0.0000e+00 - val_loss: 1.5120e-04 - val_acc: 0.0000e+0000e
Epoch 77/300
13702/13702 [==============================] - 1s - loss: 9.4651e-05 - acc: 0.0000e+00 - val_loss: 1.7162e-04 - val_acc: 0.0000e+00
Epoch 78/300
13702/13702 [==============================] - 1s - loss: 9.6063e-05 - acc: 0.0000e+00 - val_loss: 1.5722e-04 - val_acc: 0.0000e+00
Epoch 79/300
13702/13702 [==============================] - 1s - loss: 1.0355e-04 - acc: 0.0000e+00 - val_loss: 1.2651e-04 - val_acc: 0.0000e+0000e
Epoch 80/300
13702/13702 [==============================] - 1s - loss: 9.6740e-05 - acc: 0.0000e+00 - val_loss: 1.9936e-04 - val_acc: 0.0000e+00
Epoch 81/300
13702/13702 [==============================] - 1s - loss: 9.6821e-05 - acc: 0.0000e+00 - val_loss: 3.3484e-04 - val_acc: 0.0000e+00
Epoch 82/300
13702/13702 [==============================] - 1s - loss: 1.0375e-04 - acc: 0.0000e+00 - val_loss: 1.2630e-04 - val_acc: 0.0000e+00
Epoch 83/300
13702/13702 [==============================] - 1s - loss: 1.0363e-04 - acc: 0.0000e+00 - val_loss: 1.4048e-04 - val_acc: 0.0000e+00
Epoch 84/300
13702/13702 [==============================] - 1s - loss: 1.0136e-04 - acc: 0.0000e+00 - val_loss: 1.3304e-04 - val_acc: 0.0000e+00
Epoch 85/300
13702/13702 [==============================] - 1s - loss: 9.2367e-05 - acc: 0.0000e+00 - val_loss: 1.5024e-04 - val_acc: 0.0000e+00
Epoch 86/300
13702/13702 [==============================] - 1s - loss: 9.1556e-05 - acc: 0.0000e+00 - val_loss: 1.9688e-04 - val_acc: 0.0000e+00
Epoch 87/300
13702/13702 [==============================] - 1s - loss: 9.4373e-05 - acc: 0.0000e+00 - val_loss: 1.3368e-04 - val_acc: 0.0000e+00
Epoch 88/300
13702/13702 [==============================] - 1s - loss: 8.3195e-05 - acc: 0.0000e+00 - val_loss: 1.3044e-04 - val_acc: 0.0000e+00
Epoch 89/300
13702/13702 [==============================] - 1s - loss: 8.8044e-05 - acc: 0.0000e+00 - val_loss: 1.7294e-04 - val_acc: 0.0000e+00
Epoch 90/300
13702/13702 [==============================] - 1s - loss: 9.9616e-05 - acc: 0.0000e+00 - val_loss: 1.2766e-04 - val_acc: 0.0000e+00
Epoch 91/300
13702/13702 [==============================] - 1s - loss: 1.0064e-04 - acc: 0.0000e+00 - val_loss: 2.7610e-04 - val_acc: 0.0000e+00
Epoch 92/300
13702/13702 [==============================] - 1s - loss: 1.0367e-04 - acc: 0.0000e+00 - val_loss: 1.1928e-04 - val_acc: 0.0000e+00
Epoch 93/300
13702/13702 [==============================] - 1s - loss: 8.4559e-05 - acc: 0.0000e+00 - val_loss: 2.3508e-04 - val_acc: 0.0000e+00
Epoch 94/300
13702/13702 [==============================] - 1s - loss: 8.7259e-05 - acc: 0.0000e+00 - val_loss: 1.1885e-04 - val_acc: 0.0000e+00
Epoch 95/300
13702/13702 [==============================] - 1s - loss: 8.1769e-05 - acc: 0.0000e+00 - val_loss: 1.1800e-04 - val_acc: 0.0000e+00
Epoch 96/300
13702/13702 [==============================] - 1s - loss: 8.3196e-05 - acc: 0.0000e+00 - val_loss: 1.2475e-04 - val_acc: 0.0000e+00
Epoch 97/300
13702/13702 [==============================] - 1s - loss: 8.2697e-05 - acc: 0.0000e+00 - val_loss: 1.3474e-04 - val_acc: 0.0000e+00
Epoch 98/300
13702/13702 [==============================] - 1s - loss: 8.5655e-05 - acc: 0.0000e+00 - val_loss: 1.2235e-04 - val_acc: 0.0000e+00
Epoch 99/300
13702/13702 [==============================] - 1s - loss: 7.8750e-05 - acc: 0.0000e+00 - val_loss: 1.3564e-04 - val_acc: 0.0000e+00
Epoch 100/300
13702/13702 [==============================] - 1s - loss: 7.8939e-05 - acc: 0.0000e+00 - val_loss: 1.1622e-04 - val_acc: 0.0000e+00
Epoch 101/300
13702/13702 [==============================] - 1s - loss: 7.9343e-05 - acc: 0.0000e+00 - val_loss: 1.2561e-04 - val_acc: 0.0000e+00
Epoch 102/300
13702/13702 [==============================] - 1s - loss: 7.5494e-05 - acc: 0.0000e+00 - val_loss: 1.1863e-04 - val_acc: 0.0000e+00
Epoch 103/300
13702/13702 [==============================] - 1s - loss: 8.0968e-05 - acc: 0.0000e+00 - val_loss: 1.2094e-04 - val_acc: 0.0000e+00
Epoch 104/300
13702/13702 [==============================] - 1s - loss: 7.9299e-05 - acc: 0.0000e+00 - val_loss: 2.6767e-04 - val_acc: 0.0000e+00
Epoch 105/300
13702/13702 [==============================] - 1s - loss: 8.1161e-05 - acc: 0.0000e+00 - val_loss: 1.5001e-04 - val_acc: 0.0000e+00
Epoch 106/300
13702/13702 [==============================] - 1s - loss: 7.3458e-05 - acc: 0.0000e+00 - val_loss: 1.2214e-04 - val_acc: 0.0000e+00
Epoch 107/300
13702/13702 [==============================] - 1s - loss: 7.1224e-05 - acc: 0.0000e+00 - val_loss: 2.9244e-04 - val_acc: 0.0000e+00
Epoch 108/300
13702/13702 [==============================] - 1s - loss: 9.7165e-05 - acc: 0.0000e+00 - val_loss: 1.1771e-04 - val_acc: 0.0000e+00
Epoch 109/300
13702/13702 [==============================] - 1s - loss: 7.8073e-05 - acc: 0.0000e+00 - val_loss: 1.2206e-04 - val_acc: 0.0000e+00
Epoch 110/300
13702/13702 [==============================] - 1s - loss: 7.4322e-05 - acc: 0.0000e+00 - val_loss: 1.1852e-04 - val_acc: 0.0000e+00
Epoch 111/300
13702/13702 [==============================] - 1s - loss: 7.6335e-05 - acc: 0.0000e+00 - val_loss: 1.4050e-04 - val_acc: 0.0000e+00
Epoch 112/300
13702/13702 [==============================] - 1s - loss: 8.0061e-05 - acc: 0.0000e+00 - val_loss: 2.0121e-04 - val_acc: 0.0000e+00
Epoch 113/300
13702/13702 [==============================] - 1s - loss: 7.5407e-05 - acc: 0.0000e+00 - val_loss: 1.6877e-04 - val_acc: 0.0000e+00
Epoch 114/300
13702/13702 [==============================] - 1s - loss: 7.5449e-05 - acc: 0.0000e+00 - val_loss: 2.5030e-04 - val_acc: 0.0000e+00
Epoch 115/300
13702/13702 [==============================] - 1s - loss: 7.6449e-05 - acc: 0.0000e+00 - val_loss: 1.7870e-04 - val_acc: 0.0000e+00
Epoch 116/300
13702/13702 [==============================] - 1s - loss: 7.0849e-05 - acc: 0.0000e+00 - val_loss: 2.3038e-04 - val_acc: 0.0000e+00
Epoch 117/300
13702/13702 [==============================] - 1s - loss: 7.0409e-05 - acc: 0.0000e+00 - val_loss: 2.4584e-04 - val_acc: 0.0000e+00
Epoch 118/300
13702/13702 [==============================] - 1s - loss: 7.5467e-05 - acc: 0.0000e+00 - val_loss: 2.5907e-04 - val_acc: 0.0000e+00
Epoch 119/300
13702/13702 [==============================] - 1s - loss: 8.0237e-05 - acc: 0.0000e+00 - val_loss: 1.1274e-04 - val_acc: 0.0000e+00
Epoch 120/300
13702/13702 [==============================] - 1s - loss: 7.0812e-05 - acc: 0.0000e+00 - val_loss: 1.1094e-04 - val_acc: 0.0000e+00
Epoch 121/300
13702/13702 [==============================] - 1s - loss: 8.6055e-05 - acc: 0.0000e+00 - val_loss: 1.1444e-04 - val_acc: 0.0000e+00
Epoch 122/300
13702/13702 [==============================] - 1s - loss: 7.7686e-05 - acc: 0.0000e+00 - val_loss: 2.2315e-04 - val_acc: 0.0000e+00
Epoch 123/300
13702/13702 [==============================] - 1s - loss: 7.1818e-05 - acc: 0.0000e+00 - val_loss: 1.6184e-04 - val_acc: 0.0000e+00
Epoch 124/300
13702/13702 [==============================] - 1s - loss: 6.9144e-05 - acc: 0.0000e+00 - val_loss: 1.4485e-04 - val_acc: 0.0000e+00
Epoch 125/300
13702/13702 [==============================] - 1s - loss: 7.1295e-05 - acc: 0.0000e+00 - val_loss: 2.8457e-04 - val_acc: 0.0000e+00
Epoch 126/300
13702/13702 [==============================] - 1s - loss: 6.7553e-05 - acc: 0.0000e+00 - val_loss: 2.1662e-04 - val_acc: 0.0000e+00
Epoch 127/300
13702/13702 [==============================] - 1s - loss: 6.4941e-05 - acc: 0.0000e+00 - val_loss: 1.4468e-04 - val_acc: 0.0000e+00
Epoch 128/300
13702/13702 [==============================] - 1s - loss: 7.1972e-05 - acc: 0.0000e+00 - val_loss: 1.6068e-04 - val_acc: 0.0000e+00
Epoch 129/300
13702/13702 [==============================] - 1s - loss: 7.0898e-05 - acc: 0.0000e+00 - val_loss: 1.5473e-04 - val_acc: 0.0000e+00
Epoch 130/300
13702/13702 [==============================] - 1s - loss: 6.3745e-05 - acc: 0.0000e+00 - val_loss: 1.0981e-04 - val_acc: 0.0000e+00
Epoch 131/300
13702/13702 [==============================] - 1s - loss: 6.4579e-05 - acc: 0.0000e+00 - val_loss: 1.0658e-04 - val_acc: 0.0000e+00
Epoch 132/300
13702/13702 [==============================] - 1s - loss: 6.3433e-05 - acc: 0.0000e+00 - val_loss: 2.7105e-04 - val_acc: 0.0000e+00
Epoch 133/300
13702/13702 [==============================] - 1s - loss: 7.4739e-05 - acc: 0.0000e+00 - val_loss: 2.7288e-04 - val_acc: 0.0000e+00
Epoch 134/300
13702/13702 [==============================] - 1s - loss: 6.7838e-05 - acc: 0.0000e+00 - val_loss: 1.0321e-04 - val_acc: 0.0000e+00
Epoch 135/300
13702/13702 [==============================] - 1s - loss: 6.9862e-05 - acc: 0.0000e+00 - val_loss: 1.3938e-04 - val_acc: 0.0000e+00
Epoch 136/300
13702/13702 [==============================] - 1s - loss: 7.9317e-05 - acc: 0.0000e+00 - val_loss: 1.3731e-04 - val_acc: 0.0000e+00
Epoch 137/300
13702/13702 [==============================] - 1s - loss: 7.0154e-05 - acc: 0.0000e+00 - val_loss: 1.9247e-04 - val_acc: 0.0000e+00
Epoch 138/300
13702/13702 [==============================] - 1s - loss: 6.4486e-05 - acc: 0.0000e+00 - val_loss: 1.5125e-04 - val_acc: 0.0000e+00
Epoch 139/300
13702/13702 [==============================] - 1s - loss: 6.1072e-05 - acc: 0.0000e+00 - val_loss: 1.0958e-04 - val_acc: 0.0000e+00
Epoch 140/300
13702/13702 [==============================] - 1s - loss: 5.9677e-05 - acc: 0.0000e+00 - val_loss: 1.0424e-04 - val_acc: 0.0000e+00
Epoch 141/300
13702/13702 [==============================] - 1s - loss: 6.7725e-05 - acc: 0.0000e+00 - val_loss: 1.7454e-04 - val_acc: 0.0000e+00
Epoch 142/300
13702/13702 [==============================] - 1s - loss: 6.2214e-05 - acc: 0.0000e+00 - val_loss: 1.4798e-04 - val_acc: 0.0000e+00
Epoch 143/300
13702/13702 [==============================] - 1s - loss: 6.0318e-05 - acc: 0.0000e+00 - val_loss: 1.1087e-04 - val_acc: 0.0000e+00
Epoch 144/300
13702/13702 [==============================] - 1s - loss: 6.4102e-05 - acc: 0.0000e+00 - val_loss: 1.1551e-04 - val_acc: 0.0000e+00
Epoch 145/300
13702/13702 [==============================] - 1s - loss: 5.9907e-05 - acc: 0.0000e+00 - val_loss: 1.0189e-04 - val_acc: 0.0000e+00
Epoch 146/300
13702/13702 [==============================] - 1s - loss: 5.9387e-05 - acc: 0.0000e+00 - val_loss: 1.5805e-04 - val_acc: 0.0000e+00
Epoch 147/300
13702/13702 [==============================] - 1s - loss: 6.3418e-05 - acc: 0.0000e+00 - val_loss: 1.0517e-04 - val_acc: 0.0000e+00
Epoch 148/300
13702/13702 [==============================] - 1s - loss: 6.0501e-05 - acc: 0.0000e+00 - val_loss: 1.2135e-04 - val_acc: 0.0000e+00
Epoch 149/300
13702/13702 [==============================] - 1s - loss: 6.2552e-05 - acc: 0.0000e+00 - val_loss: 1.0421e-04 - val_acc: 0.0000e+00
Epoch 150/300
13702/13702 [==============================] - 1s - loss: 6.6797e-05 - acc: 0.0000e+00 - val_loss: 3.8725e-04 - val_acc: 0.0000e+00
Epoch 151/300
13702/13702 [==============================] - 1s - loss: 6.0978e-05 - acc: 0.0000e+00 - val_loss: 2.2032e-04 - val_acc: 0.0000e+00
Epoch 152/300
13702/13702 [==============================] - 1s - loss: 5.9420e-05 - acc: 0.0000e+00 - val_loss: 1.1117e-04 - val_acc: 0.0000e+00
Epoch 153/300
13702/13702 [==============================] - 1s - loss: 6.1105e-05 - acc: 0.0000e+00 - val_loss: 1.1714e-04 - val_acc: 0.0000e+00
Epoch 154/300
13702/13702 [==============================] - 1s - loss: 6.2313e-05 - acc: 0.0000e+00 - val_loss: 9.8602e-05 - val_acc: 0.0000e+00
Epoch 155/300
13702/13702 [==============================] - 1s - loss: 6.3480e-05 - acc: 0.0000e+00 - val_loss: 2.6229e-04 - val_acc: 0.0000e+00
Epoch 156/300
13702/13702 [==============================] - 1s - loss: 6.3138e-05 - acc: 0.0000e+00 - val_loss: 1.3430e-04 - val_acc: 0.0000e+00
Epoch 157/300
13702/13702 [==============================] - 1s - loss: 6.3961e-05 - acc: 0.0000e+00 - val_loss: 1.1436e-04 - val_acc: 0.0000e+00
Epoch 158/300
13702/13702 [==============================] - 1s - loss: 6.2608e-05 - acc: 0.0000e+00 - val_loss: 1.6103e-04 - val_acc: 0.0000e+00
Epoch 159/300
13702/13702 [==============================] - 1s - loss: 5.9241e-05 - acc: 0.0000e+00 - val_loss: 1.1385e-04 - val_acc: 0.0000e+00
Epoch 160/300
13702/13702 [==============================] - 1s - loss: 5.2693e-05 - acc: 0.0000e+00 - val_loss: 1.1039e-04 - val_acc: 0.0000e+00
Epoch 161/300
13702/13702 [==============================] - 1s - loss: 5.9654e-05 - acc: 0.0000e+00 - val_loss: 1.4374e-04 - val_acc: 0.0000e+00
Epoch 162/300
13702/13702 [==============================] - 1s - loss: 5.8400e-05 - acc: 0.0000e+00 - val_loss: 3.0981e-04 - val_acc: 0.0000e+00
Epoch 163/300
13702/13702 [==============================] - 1s - loss: 6.0255e-05 - acc: 0.0000e+00 - val_loss: 2.3080e-04 - val_acc: 0.0000e+00
Epoch 164/300
13702/13702 [==============================] - 1s - loss: 5.6711e-05 - acc: 0.0000e+00 - val_loss: 2.1065e-04 - val_acc: 0.0000e+00
Epoch 165/300
13702/13702 [==============================] - 1s - loss: 6.2729e-05 - acc: 0.0000e+00 - val_loss: 7.7048e-04 - val_acc: 0.0000e+00
Epoch 166/300
13702/13702 [==============================] - 1s - loss: 6.7525e-05 - acc: 0.0000e+00 - val_loss: 1.0705e-04 - val_acc: 0.0000e+00
Epoch 167/300
13702/13702 [==============================] - 1s - loss: 6.2871e-05 - acc: 0.0000e+00 - val_loss: 3.4541e-04 - val_acc: 0.0000e+00
Epoch 168/300
13702/13702 [==============================] - 1s - loss: 5.6869e-05 - acc: 0.0000e+00 - val_loss: 2.8644e-04 - val_acc: 0.0000e+00
Epoch 169/300
13702/13702 [==============================] - 1s - loss: 5.8582e-05 - acc: 0.0000e+00 - val_loss: 9.7128e-05 - val_acc: 0.0000e+00
Epoch 170/300
13702/13702 [==============================] - 1s - loss: 5.4799e-05 - acc: 0.0000e+00 - val_loss: 9.4729e-05 - val_acc: 0.0000e+00
Epoch 171/300
13702/13702 [==============================] - 1s - loss: 5.3059e-05 - acc: 0.0000e+00 - val_loss: 1.2937e-04 - val_acc: 0.0000e+00
Epoch 172/300
13702/13702 [==============================] - 1s - loss: 5.5151e-05 - acc: 0.0000e+00 - val_loss: 9.5984e-05 - val_acc: 0.0000e+00
Epoch 173/300
13702/13702 [==============================] - 1s - loss: 5.4372e-05 - acc: 0.0000e+00 - val_loss: 2.1266e-04 - val_acc: 0.0000e+00
Epoch 174/300
13702/13702 [==============================] - 1s - loss: 5.2838e-05 - acc: 0.0000e+00 - val_loss: 2.1060e-04 - val_acc: 0.0000e+00
Epoch 175/300
13702/13702 [==============================] - 1s - loss: 5.0908e-05 - acc: 0.0000e+00 - val_loss: 1.3188e-04 - val_acc: 0.0000e+00
Epoch 176/300
13702/13702 [==============================] - 1s - loss: 5.3162e-05 - acc: 0.0000e+00 - val_loss: 8.8861e-05 - val_acc: 0.0000e+00
Epoch 177/300
13702/13702 [==============================] - 1s - loss: 5.1070e-05 - acc: 0.0000e+00 - val_loss: 1.3267e-04 - val_acc: 0.0000e+00
Epoch 178/300
13702/13702 [==============================] - 1s - loss: 5.3744e-05 - acc: 0.0000e+00 - val_loss: 1.2478e-04 - val_acc: 0.0000e+00
Epoch 179/300
13702/13702 [==============================] - 1s - loss: 5.0087e-05 - acc: 0.0000e+00 - val_loss: 1.1719e-04 - val_acc: 0.0000e+00
Epoch 180/300
13702/13702 [==============================] - 1s - loss: 5.5939e-05 - acc: 0.0000e+00 - val_loss: 2.2297e-04 - val_acc: 0.0000e+00
Epoch 181/300
13702/13702 [==============================] - 1s - loss: 5.2949e-05 - acc: 0.0000e+00 - val_loss: 1.5936e-04 - val_acc: 0.0000e+00
Epoch 182/300
13702/13702 [==============================] - 1s - loss: 5.5697e-05 - acc: 0.0000e+00 - val_loss: 4.1625e-04 - val_acc: 0.0000e+00
Epoch 183/300
13702/13702 [==============================] - 1s - loss: 5.6044e-05 - acc: 0.0000e+00 - val_loss: 1.4011e-04 - val_acc: 0.0000e+00- loss: 5.6600e-05 - acc: 0.0000e+
Epoch 184/300
13702/13702 [==============================] - 1s - loss: 5.5740e-05 - acc: 0.0000e+00 - val_loss: 9.5011e-05 - val_acc: 0.0000e+00
Epoch 185/300
13702/13702 [==============================] - 1s - loss: 5.1277e-05 - acc: 0.0000e+00 - val_loss: 1.2430e-04 - val_acc: 0.0000e+00
Epoch 186/300
13702/13702 [==============================] - 1s - loss: 4.7618e-05 - acc: 0.0000e+00 - val_loss: 1.6166e-04 - val_acc: 0.0000e+00
Epoch 187/300
13702/13702 [==============================] - 1s - loss: 5.0599e-05 - acc: 0.0000e+00 - val_loss: 1.2909e-04 - val_acc: 0.0000e+00
Epoch 188/300
13702/13702 [==============================] - 1s - loss: 5.0851e-05 - acc: 0.0000e+00 - val_loss: 1.1446e-04 - val_acc: 0.0000e+00
Epoch 189/300
13702/13702 [==============================] - 1s - loss: 5.2853e-05 - acc: 0.0000e+00 - val_loss: 1.2426e-04 - val_acc: 0.0000e+00
Epoch 190/300
13702/13702 [==============================] - 1s - loss: 4.9944e-05 - acc: 0.0000e+00 - val_loss: 1.5018e-04 - val_acc: 0.0000e+00
Epoch 191/300
13702/13702 [==============================] - 1s - loss: 5.1618e-05 - acc: 0.0000e+00 - val_loss: 1.7918e-04 - val_acc: 0.0000e+00
Epoch 192/300
13702/13702 [==============================] - 1s - loss: 5.4237e-05 - acc: 0.0000e+00 - val_loss: 1.4274e-04 - val_acc: 0.0000e+00
Epoch 193/300
13702/13702 [==============================] - 1s - loss: 4.6970e-05 - acc: 0.0000e+00 - val_loss: 1.9071e-04 - val_acc: 0.0000e+00
Epoch 194/300
13702/13702 [==============================] - 1s - loss: 4.9245e-05 - acc: 0.0000e+00 - val_loss: 5.2862e-04 - val_acc: 0.0000e+00
Epoch 195/300
13702/13702 [==============================] - 1s - loss: 5.7839e-05 - acc: 0.0000e+00 - val_loss: 4.4721e-04 - val_acc: 0.0000e+00
Epoch 196/300
13702/13702 [==============================] - 1s - loss: 4.9013e-05 - acc: 0.0000e+00 - val_loss: 2.6655e-04 - val_acc: 0.0000e+00
Epoch 197/300
13702/13702 [==============================] - 1s - loss: 5.6635e-05 - acc: 0.0000e+00 - val_loss: 1.5327e-04 - val_acc: 0.0000e+00
Epoch 198/300
13702/13702 [==============================] - 1s - loss: 5.0156e-05 - acc: 0.0000e+00 - val_loss: 3.9753e-04 - val_acc: 0.0000e+00
Epoch 199/300
13702/13702 [==============================] - 1s - loss: 4.6178e-05 - acc: 0.0000e+00 - val_loss: 1.4531e-04 - val_acc: 0.0000e+00
Epoch 200/300
13702/13702 [==============================] - 1s - loss: 5.0364e-05 - acc: 0.0000e+00 - val_loss: 2.2693e-04 - val_acc: 0.0000e+00
Epoch 201/300
13702/13702 [==============================] - 1s - loss: 5.0929e-05 - acc: 0.0000e+00 - val_loss: 2.5645e-04 - val_acc: 0.0000e+00
Epoch 202/300
13702/13702 [==============================] - 1s - loss: 4.5530e-05 - acc: 0.0000e+00 - val_loss: 2.2211e-04 - val_acc: 0.0000e+00
Epoch 203/300
13702/13702 [==============================] - 1s - loss: 5.1057e-05 - acc: 0.0000e+00 - val_loss: 2.2898e-04 - val_acc: 0.0000e+00
Epoch 204/300
13702/13702 [==============================] - 1s - loss: 5.3156e-05 - acc: 0.0000e+00 - val_loss: 4.0469e-04 - val_acc: 0.0000e+00
Epoch 205/300
13702/13702 [==============================] - 1s - loss: 4.6560e-05 - acc: 0.0000e+00 - val_loss: 3.1460e-04 - val_acc: 0.0000e+00
Epoch 206/300
13702/13702 [==============================] - 1s - loss: 5.2696e-05 - acc: 0.0000e+00 - val_loss: 3.0310e-04 - val_acc: 0.0000e+00
Epoch 207/300
13702/13702 [==============================] - 1s - loss: 4.4534e-05 - acc: 0.0000e+00 - val_loss: 2.4806e-04 - val_acc: 0.0000e+00
Epoch 208/300
13702/13702 [==============================] - 1s - loss: 4.3620e-05 - acc: 0.0000e+00 - val_loss: 2.6803e-04 - val_acc: 0.0000e+00
Epoch 209/300
13702/13702 [==============================] - 1s - loss: 4.8282e-05 - acc: 0.0000e+00 - val_loss: 4.5365e-04 - val_acc: 0.0000e+00
Epoch 210/300
13702/13702 [==============================] - 1s - loss: 4.9243e-05 - acc: 0.0000e+00 - val_loss: 2.9860e-04 - val_acc: 0.0000e+00
Epoch 211/300
13702/13702 [==============================] - 1s - loss: 4.9562e-05 - acc: 0.0000e+00 - val_loss: 3.7185e-04 - val_acc: 0.0000e+00
Epoch 212/300
13702/13702 [==============================] - 1s - loss: 4.4081e-05 - acc: 0.0000e+00 - val_loss: 1.9620e-04 - val_acc: 0.0000e+00
Epoch 213/300
13702/13702 [==============================] - 1s - loss: 4.5634e-05 - acc: 0.0000e+00 - val_loss: 3.7626e-04 - val_acc: 0.0000e+00
Epoch 214/300
13702/13702 [==============================] - 1s - loss: 4.3095e-05 - acc: 0.0000e+00 - val_loss: 4.4241e-04 - val_acc: 0.0000e+00
Epoch 215/300
13702/13702 [==============================] - 1s - loss: 4.8171e-05 - acc: 0.0000e+00 - val_loss: 1.5349e-04 - val_acc: 0.0000e+00
Epoch 216/300
13702/13702 [==============================] - 1s - loss: 4.9549e-05 - acc: 0.0000e+00 - val_loss: 3.5179e-04 - val_acc: 0.0000e+00
Epoch 217/300
13702/13702 [==============================] - 1s - loss: 4.4267e-05 - acc: 0.0000e+00 - val_loss: 3.2184e-04 - val_acc: 0.0000e+00
Epoch 218/300
13702/13702 [==============================] - 1s - loss: 4.8565e-05 - acc: 0.0000e+00 - val_loss: 4.6950e-04 - val_acc: 0.0000e+00
Epoch 219/300
13702/13702 [==============================] - 1s - loss: 4.3311e-05 - acc: 0.0000e+00 - val_loss: 7.0130e-04 - val_acc: 0.0000e+00
Epoch 220/300
13702/13702 [==============================] - 1s - loss: 4.4961e-05 - acc: 0.0000e+00 - val_loss: 5.7976e-04 - val_acc: 0.0000e+00
Epoch 221/300
13702/13702 [==============================] - 1s - loss: 4.9112e-05 - acc: 0.0000e+00 - val_loss: 6.3798e-04 - val_acc: 0.0000e+00
Epoch 222/300
13702/13702 [==============================] - 1s - loss: 4.5547e-05 - acc: 0.0000e+00 - val_loss: 3.8607e-04 - val_acc: 0.0000e+00
Epoch 223/300
13702/13702 [==============================] - 1s - loss: 4.0416e-05 - acc: 0.0000e+00 - val_loss: 4.5483e-04 - val_acc: 0.0000e+00
Epoch 224/300
13702/13702 [==============================] - 1s - loss: 4.5231e-05 - acc: 0.0000e+00 - val_loss: 3.7021e-04 - val_acc: 0.0000e+00
Epoch 225/300
13702/13702 [==============================] - 1s - loss: 4.7328e-05 - acc: 0.0000e+00 - val_loss: 2.9372e-04 - val_acc: 0.0000e+00
Epoch 226/300
13702/13702 [==============================] - 1s - loss: 4.2335e-05 - acc: 0.0000e+00 - val_loss: 5.3925e-04 - val_acc: 0.0000e+00
Epoch 227/300
13702/13702 [==============================] - 1s - loss: 4.0794e-05 - acc: 0.0000e+00 - val_loss: 3.1190e-04 - val_acc: 0.0000e+00
Epoch 228/300
13702/13702 [==============================] - 1s - loss: 4.6578e-05 - acc: 0.0000e+00 - val_loss: 4.9661e-04 - val_acc: 0.0000e+00
Epoch 229/300
13702/13702 [==============================] - 1s - loss: 4.7412e-05 - acc: 0.0000e+00 - val_loss: 5.9925e-04 - val_acc: 0.0000e+00
Epoch 230/300
13702/13702 [==============================] - 1s - loss: 4.6688e-05 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 231/300
13702/13702 [==============================] - 1s - loss: 4.6717e-05 - acc: 0.0000e+00 - val_loss: 4.9957e-04 - val_acc: 0.0000e+00
Epoch 232/300
13702/13702 [==============================] - 1s - loss: 4.9908e-05 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 233/300
13702/13702 [==============================] - 1s - loss: 4.3331e-05 - acc: 0.0000e+00 - val_loss: 6.7110e-04 - val_acc: 0.0000e+00
Epoch 234/300
13702/13702 [==============================] - 1s - loss: 3.9649e-05 - acc: 0.0000e+00 - val_loss: 3.8010e-04 - val_acc: 0.0000e+00
Epoch 235/300
13702/13702 [==============================] - 1s - loss: 3.9112e-05 - acc: 0.0000e+00 - val_loss: 6.3234e-04 - val_acc: 0.0000e+00
Epoch 236/300
13702/13702 [==============================] - 1s - loss: 4.0891e-05 - acc: 0.0000e+00 - val_loss: 3.3754e-04 - val_acc: 0.0000e+00
Epoch 237/300
13702/13702 [==============================] - 1s - loss: 4.2709e-05 - acc: 0.0000e+00 - val_loss: 8.6715e-04 - val_acc: 0.0000e+00
Epoch 238/300
13702/13702 [==============================] - 1s - loss: 4.6157e-05 - acc: 0.0000e+00 - val_loss: 5.4450e-04 - val_acc: 0.0000e+00
Epoch 239/300
13702/13702 [==============================] - 1s - loss: 4.9385e-05 - acc: 0.0000e+00 - val_loss: 4.4275e-04 - val_acc: 0.0000e+00
Epoch 240/300
13702/13702 [==============================] - 1s - loss: 4.7859e-05 - acc: 0.0000e+00 - val_loss: 6.4428e-04 - val_acc: 0.0000e+00
Epoch 241/300
13702/13702 [==============================] - 1s - loss: 4.2141e-05 - acc: 0.0000e+00 - val_loss: 8.1975e-04 - val_acc: 0.0000e+00
Epoch 242/300
13702/13702 [==============================] - 1s - loss: 4.5560e-05 - acc: 0.0000e+00 - val_loss: 3.4242e-04 - val_acc: 0.0000e+00
Epoch 243/300
13702/13702 [==============================] - 1s - loss: 3.9698e-05 - acc: 0.0000e+00 - val_loss: 8.6679e-04 - val_acc: 0.0000e+00
Epoch 244/300
13702/13702 [==============================] - 1s - loss: 4.0289e-05 - acc: 0.0000e+00 - val_loss: 6.7704e-04 - val_acc: 0.0000e+00
Epoch 245/300
13702/13702 [==============================] - 1s - loss: 4.5648e-05 - acc: 0.0000e+00 - val_loss: 5.4294e-04 - val_acc: 0.0000e+00
Epoch 246/300
13702/13702 [==============================] - 1s - loss: 4.4502e-05 - acc: 0.0000e+00 - val_loss: 3.3888e-04 - val_acc: 0.0000e+00
Epoch 247/300
13702/13702 [==============================] - 1s - loss: 4.0186e-05 - acc: 0.0000e+00 - val_loss: 3.5092e-04 - val_acc: 0.0000e+00
Epoch 248/300
13702/13702 [==============================] - 1s - loss: 4.7147e-05 - acc: 0.0000e+00 - val_loss: 4.3610e-04 - val_acc: 0.0000e+00
Epoch 249/300
13702/13702 [==============================] - 1s - loss: 3.8760e-05 - acc: 0.0000e+00 - val_loss: 6.7867e-04 - val_acc: 0.0000e+00
Epoch 250/300
13702/13702 [==============================] - 1s - loss: 3.8832e-05 - acc: 0.0000e+00 - val_loss: 3.6523e-04 - val_acc: 0.0000e+00
Epoch 251/300
13702/13702 [==============================] - 1s - loss: 4.0648e-05 - acc: 0.0000e+00 - val_loss: 9.3012e-04 - val_acc: 0.0000e+00
Epoch 252/300
13702/13702 [==============================] - 1s - loss: 4.4399e-05 - acc: 0.0000e+00 - val_loss: 3.8663e-04 - val_acc: 0.0000e+00
Epoch 253/300
13702/13702 [==============================] - 1s - loss: 4.3185e-05 - acc: 0.0000e+00 - val_loss: 4.9197e-04 - val_acc: 0.0000e+00
Epoch 254/300
13702/13702 [==============================] - 1s - loss: 4.3924e-05 - acc: 0.0000e+00 - val_loss: 3.2221e-04 - val_acc: 0.0000e+00
Epoch 255/300
13702/13702 [==============================] - 1s - loss: 4.2961e-05 - acc: 0.0000e+00 - val_loss: 5.6797e-04 - val_acc: 0.0000e+00
Epoch 256/300
13702/13702 [==============================] - 1s - loss: 3.7599e-05 - acc: 0.0000e+00 - val_loss: 6.2306e-04 - val_acc: 0.0000e+00
Epoch 257/300
13702/13702 [==============================] - 1s - loss: 3.8016e-05 - acc: 0.0000e+00 - val_loss: 6.2173e-04 - val_acc: 0.0000e+00
Epoch 258/300
13702/13702 [==============================] - 1s - loss: 4.0209e-05 - acc: 0.0000e+00 - val_loss: 2.8032e-04 - val_acc: 0.0000e+00
Epoch 259/300
13702/13702 [==============================] - 1s - loss: 4.7063e-05 - acc: 0.0000e+00 - val_loss: 4.6994e-04 - val_acc: 0.0000e+00
Epoch 260/300
13702/13702 [==============================] - 1s - loss: 4.2735e-05 - acc: 0.0000e+00 - val_loss: 4.1736e-04 - val_acc: 0.0000e+00
Epoch 261/300
13702/13702 [==============================] - 1s - loss: 3.6282e-05 - acc: 0.0000e+00 - val_loss: 6.3793e-04 - val_acc: 0.0000e+00
Epoch 262/300
13702/13702 [==============================] - 1s - loss: 4.1539e-05 - acc: 0.0000e+00 - val_loss: 7.4030e-04 - val_acc: 0.0000e+00
Epoch 263/300
13702/13702 [==============================] - 1s - loss: 4.3086e-05 - acc: 0.0000e+00 - val_loss: 5.2698e-04 - val_acc: 0.0000e+00
Epoch 264/300
13702/13702 [==============================] - 1s - loss: 4.0219e-05 - acc: 0.0000e+00 - val_loss: 5.7726e-04 - val_acc: 0.0000e+00
Epoch 265/300
13702/13702 [==============================] - 1s - loss: 3.8289e-05 - acc: 0.0000e+00 - val_loss: 5.5163e-04 - val_acc: 0.0000e+00
Epoch 266/300
13702/13702 [==============================] - 1s - loss: 3.7492e-05 - acc: 0.0000e+00 - val_loss: 6.1641e-04 - val_acc: 0.0000e+00
Epoch 267/300
13702/13702 [==============================] - 1s - loss: 3.7846e-05 - acc: 0.0000e+00 - val_loss: 3.1007e-04 - val_acc: 0.0000e+00
Epoch 268/300
13702/13702 [==============================] - 1s - loss: 3.8842e-05 - acc: 0.0000e+00 - val_loss: 6.1031e-04 - val_acc: 0.0000e+00
Epoch 269/300
13702/13702 [==============================] - 1s - loss: 3.7726e-05 - acc: 0.0000e+00 - val_loss: 9.9901e-04 - val_acc: 0.0000e+00
Epoch 270/300
13702/13702 [==============================] - 1s - loss: 4.0212e-05 - acc: 0.0000e+00 - val_loss: 8.2007e-04 - val_acc: 0.0000e+00
Epoch 271/300
13702/13702 [==============================] - 1s - loss: 3.8043e-05 - acc: 0.0000e+00 - val_loss: 8.5960e-04 - val_acc: 0.0000e+00
Epoch 272/300
13702/13702 [==============================] - 1s - loss: 4.2605e-05 - acc: 0.0000e+00 - val_loss: 4.3719e-04 - val_acc: 0.0000e+00
Epoch 273/300
13702/13702 [==============================] - 1s - loss: 4.0478e-05 - acc: 0.0000e+00 - val_loss: 4.2340e-04 - val_acc: 0.0000e+00
Epoch 274/300
13702/13702 [==============================] - 1s - loss: 3.9580e-05 - acc: 0.0000e+00 - val_loss: 3.7551e-04 - val_acc: 0.0000e+00
Epoch 275/300
13702/13702 [==============================] - 1s - loss: 4.5733e-05 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 276/300
13702/13702 [==============================] - 1s - loss: 3.9769e-05 - acc: 0.0000e+00 - val_loss: 7.1827e-04 - val_acc: 0.0000e+00
Epoch 277/300
13702/13702 [==============================] - 1s - loss: 3.9066e-05 - acc: 0.0000e+00 - val_loss: 3.3978e-04 - val_acc: 0.0000e+00
Epoch 278/300
13702/13702 [==============================] - 1s - loss: 3.6380e-05 - acc: 0.0000e+00 - val_loss: 8.5174e-04 - val_acc: 0.0000e+00
Epoch 279/300
13702/13702 [==============================] - 1s - loss: 4.6578e-05 - acc: 0.0000e+00 - val_loss: 5.6161e-04 - val_acc: 0.0000e+00
Epoch 280/300
13702/13702 [==============================] - 1s - loss: 3.6997e-05 - acc: 0.0000e+00 - val_loss: 7.0406e-04 - val_acc: 0.0000e+00
Epoch 281/300
13702/13702 [==============================] - 1s - loss: 3.4575e-05 - acc: 0.0000e+00 - val_loss: 6.2905e-04 - val_acc: 0.0000e+00
Epoch 282/300
13702/13702 [==============================] - 1s - loss: 3.9156e-05 - acc: 0.0000e+00 - val_loss: 7.7921e-04 - val_acc: 0.0000e+00
Epoch 283/300
13702/13702 [==============================] - 1s - loss: 3.6416e-05 - acc: 0.0000e+00 - val_loss: 7.2014e-04 - val_acc: 0.0000e+00
Epoch 284/300
13702/13702 [==============================] - 1s - loss: 3.8637e-05 - acc: 0.0000e+00 - val_loss: 9.1494e-04 - val_acc: 0.0000e+00
Epoch 285/300
13702/13702 [==============================] - 1s - loss: 3.8647e-05 - acc: 0.0000e+00 - val_loss: 6.2667e-04 - val_acc: 0.0000e+00
Epoch 286/300
13702/13702 [==============================] - 1s - loss: 3.5288e-05 - acc: 0.0000e+00 - val_loss: 4.4665e-04 - val_acc: 0.0000e+00
Epoch 287/300
13702/13702 [==============================] - 1s - loss: 3.4588e-05 - acc: 0.0000e+00 - val_loss: 5.2453e-04 - val_acc: 0.0000e+00
Epoch 288/300
13702/13702 [==============================] - 1s - loss: 3.6369e-05 - acc: 0.0000e+00 - val_loss: 7.8817e-04 - val_acc: 0.0000e+00
Epoch 289/300
13702/13702 [==============================] - 1s - loss: 3.8175e-05 - acc: 0.0000e+00 - val_loss: 6.7251e-04 - val_acc: 0.0000e+00
Epoch 290/300
13702/13702 [==============================] - 1s - loss: 3.5433e-05 - acc: 0.0000e+00 - val_loss: 8.0439e-04 - val_acc: 0.0000e+00
Epoch 291/300
13702/13702 [==============================] - 1s - loss: 3.7328e-05 - acc: 0.0000e+00 - val_loss: 6.3759e-04 - val_acc: 0.0000e+00
Epoch 292/300
13702/13702 [==============================] - 1s - loss: 3.9048e-05 - acc: 0.0000e+00 - val_loss: 4.4872e-04 - val_acc: 0.0000e+00
Epoch 293/300
13702/13702 [==============================] - 1s - loss: 3.5619e-05 - acc: 0.0000e+00 - val_loss: 8.6864e-04 - val_acc: 0.0000e+00
Epoch 294/300
13702/13702 [==============================] - 1s - loss: 3.6799e-05 - acc: 0.0000e+00 - val_loss: 7.3828e-04 - val_acc: 0.0000e+00
Epoch 295/300
13702/13702 [==============================] - 1s - loss: 3.6257e-05 - acc: 0.0000e+00 - val_loss: 8.0887e-04 - val_acc: 0.0000e+00
Epoch 296/300
13702/13702 [==============================] - 1s - loss: 3.3260e-05 - acc: 0.0000e+00 - val_loss: 5.9632e-04 - val_acc: 0.0000e+00
Epoch 297/300
13702/13702 [==============================] - 1s - loss: 3.2596e-05 - acc: 0.0000e+00 - val_loss: 7.2372e-04 - val_acc: 0.0000e+00
Epoch 298/300
13702/13702 [==============================] - 1s - loss: 4.1466e-05 - acc: 0.0000e+00 - val_loss: 5.2079e-04 - val_acc: 0.0000e+00
Epoch 299/300
13702/13702 [==============================] - 1s - loss: 3.6802e-05 - acc: 0.0000e+00 - val_loss: 8.9757e-04 - val_acc: 0.0000e+00
Epoch 300/300
13702/13702 [==============================] - 1s - loss: 3.4732e-05 - acc: 0.0000e+00 - val_loss: 6.4950e-04 - val_acc: 0.0000e+00
Train Score: 0.00013 MSE (0.01 RMSE)
Test Score: 0.00223 MSE (0.05 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_21 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_21 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_22 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_22 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_21 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_22 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/300
13702/13702 [==============================] - 3s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+0000
Epoch 2/300
13702/13702 [==============================] - 1s - loss: 7.9394e-04 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 3/300
13702/13702 [==============================] - 1s - loss: 4.5072e-04 - acc: 0.0000e+00 - val_loss: 5.8474e-04 - val_acc: 0.0000e+00
Epoch 4/300
13702/13702 [==============================] - 1s - loss: 3.3459e-04 - acc: 0.0000e+00 - val_loss: 3.7061e-04 - val_acc: 0.0000e+00
Epoch 5/300
13702/13702 [==============================] - 1s - loss: 2.9244e-04 - acc: 0.0000e+00 - val_loss: 2.9118e-04 - val_acc: 0.0000e+00
Epoch 6/300
13702/13702 [==============================] - 1s - loss: 2.7466e-04 - acc: 0.0000e+00 - val_loss: 2.5825e-04 - val_acc: 0.0000e+00
Epoch 7/300
13702/13702 [==============================] - 1s - loss: 2.5256e-04 - acc: 0.0000e+00 - val_loss: 5.3940e-04 - val_acc: 0.0000e+00
Epoch 8/300
13702/13702 [==============================] - 1s - loss: 2.5238e-04 - acc: 0.0000e+00 - val_loss: 2.4065e-04 - val_acc: 0.0000e+00
Epoch 9/300
13702/13702 [==============================] - 1s - loss: 2.3486e-04 - acc: 0.0000e+00 - val_loss: 5.0852e-04 - val_acc: 0.0000e+00
Epoch 10/300
13702/13702 [==============================] - 1s - loss: 2.5004e-04 - acc: 0.0000e+00 - val_loss: 6.2537e-04 - val_acc: 0.0000e+00
Epoch 11/300
13702/13702 [==============================] - 1s - loss: 2.4572e-04 - acc: 0.0000e+00 - val_loss: 2.3107e-04 - val_acc: 0.0000e+00
Epoch 12/300
13702/13702 [==============================] - 1s - loss: 2.3060e-04 - acc: 0.0000e+00 - val_loss: 2.8896e-04 - val_acc: 0.0000e+00
Epoch 13/300
13702/13702 [==============================] - 1s - loss: 2.5588e-04 - acc: 0.0000e+00 - val_loss: 8.0898e-04 - val_acc: 0.0000e+00
Epoch 14/300
13702/13702 [==============================] - 1s - loss: 2.2104e-04 - acc: 0.0000e+00 - val_loss: 2.1620e-04 - val_acc: 0.0000e+00
Epoch 15/300
13702/13702 [==============================] - 1s - loss: 2.3444e-04 - acc: 0.0000e+00 - val_loss: 5.0109e-04 - val_acc: 0.0000e+00
Epoch 16/300
13702/13702 [==============================] - 1s - loss: 2.1459e-04 - acc: 0.0000e+00 - val_loss: 2.1089e-04 - val_acc: 0.0000e+00
Epoch 17/300
13702/13702 [==============================] - 1s - loss: 2.2692e-04 - acc: 0.0000e+00 - val_loss: 2.3349e-04 - val_acc: 0.0000e+00
Epoch 18/300
13702/13702 [==============================] - 1s - loss: 1.9929e-04 - acc: 0.0000e+00 - val_loss: 2.3987e-04 - val_acc: 0.0000e+00
Epoch 19/300
13702/13702 [==============================] - 1s - loss: 2.0766e-04 - acc: 0.0000e+00 - val_loss: 2.0840e-04 - val_acc: 0.0000e+00
Epoch 20/300
13702/13702 [==============================] - 1s - loss: 2.0740e-04 - acc: 0.0000e+00 - val_loss: 2.0594e-04 - val_acc: 0.0000e+00
Epoch 21/300
13702/13702 [==============================] - 1s - loss: 2.1705e-04 - acc: 0.0000e+00 - val_loss: 2.4459e-04 - val_acc: 0.0000e+00
Epoch 22/300
13702/13702 [==============================] - 1s - loss: 2.0874e-04 - acc: 0.0000e+00 - val_loss: 2.0697e-04 - val_acc: 0.0000e+00
Epoch 23/300
13702/13702 [==============================] - 1s - loss: 1.9554e-04 - acc: 0.0000e+00 - val_loss: 4.6310e-04 - val_acc: 0.0000e+00
Epoch 24/300
13702/13702 [==============================] - 1s - loss: 2.1650e-04 - acc: 0.0000e+00 - val_loss: 2.1705e-04 - val_acc: 0.0000e+00
Epoch 25/300
13702/13702 [==============================] - 1s - loss: 2.1688e-04 - acc: 0.0000e+00 - val_loss: 2.9588e-04 - val_acc: 0.0000e+00
Epoch 26/300
13702/13702 [==============================] - 1s - loss: 1.9864e-04 - acc: 0.0000e+00 - val_loss: 2.3274e-04 - val_acc: 0.0000e+00
Epoch 27/300
13702/13702 [==============================] - 1s - loss: 2.0670e-04 - acc: 0.0000e+00 - val_loss: 2.3590e-04 - val_acc: 0.0000e+00
Epoch 28/300
13702/13702 [==============================] - 1s - loss: 2.1198e-04 - acc: 0.0000e+00 - val_loss: 2.2135e-04 - val_acc: 0.0000e+00
Epoch 29/300
13702/13702 [==============================] - 1s - loss: 1.9162e-04 - acc: 0.0000e+00 - val_loss: 2.1607e-04 - val_acc: 0.0000e+00
Epoch 30/300
13702/13702 [==============================] - 1s - loss: 1.8902e-04 - acc: 0.0000e+00 - val_loss: 4.6749e-04 - val_acc: 0.0000e+00
Epoch 31/300
13702/13702 [==============================] - 1s - loss: 1.9698e-04 - acc: 0.0000e+00 - val_loss: 2.5019e-04 - val_acc: 0.0000e+00
Epoch 32/300
13702/13702 [==============================] - 1s - loss: 1.8899e-04 - acc: 0.0000e+00 - val_loss: 1.8822e-04 - val_acc: 0.0000e+00
Epoch 33/300
13702/13702 [==============================] - 1s - loss: 1.9212e-04 - acc: 0.0000e+00 - val_loss: 2.0576e-04 - val_acc: 0.0000e+00
Epoch 34/300
13702/13702 [==============================] - 1s - loss: 1.8264e-04 - acc: 0.0000e+00 - val_loss: 1.9257e-04 - val_acc: 0.0000e+00
Epoch 35/300
13702/13702 [==============================] - 1s - loss: 1.7753e-04 - acc: 0.0000e+00 - val_loss: 3.4138e-04 - val_acc: 0.0000e+00
Epoch 36/300
13702/13702 [==============================] - 1s - loss: 1.8731e-04 - acc: 0.0000e+00 - val_loss: 2.1759e-04 - val_acc: 0.0000e+00
Epoch 37/300
13702/13702 [==============================] - 1s - loss: 1.8391e-04 - acc: 0.0000e+00 - val_loss: 1.9001e-04 - val_acc: 0.0000e+00
Epoch 38/300
13702/13702 [==============================] - 1s - loss: 1.8392e-04 - acc: 0.0000e+00 - val_loss: 7.6525e-04 - val_acc: 0.0000e+0000e+0
Epoch 39/300
13702/13702 [==============================] - 1s - loss: 2.0947e-04 - acc: 0.0000e+00 - val_loss: 1.7666e-04 - val_acc: 0.0000e+00
Epoch 40/300
13702/13702 [==============================] - 1s - loss: 1.7135e-04 - acc: 0.0000e+00 - val_loss: 1.7554e-04 - val_acc: 0.0000e+00
Epoch 41/300
13702/13702 [==============================] - 1s - loss: 1.7852e-04 - acc: 0.0000e+00 - val_loss: 2.5486e-04 - val_acc: 0.0000e+00
Epoch 42/300
13702/13702 [==============================] - 1s - loss: 1.6611e-04 - acc: 0.0000e+00 - val_loss: 2.6239e-04 - val_acc: 0.0000e+00
Epoch 43/300
13702/13702 [==============================] - 1s - loss: 1.8182e-04 - acc: 0.0000e+00 - val_loss: 1.8036e-04 - val_acc: 0.0000e+00
Epoch 44/300
13702/13702 [==============================] - 1s - loss: 1.7199e-04 - acc: 0.0000e+00 - val_loss: 1.7577e-04 - val_acc: 0.0000e+00
Epoch 45/300
13702/13702 [==============================] - 1s - loss: 1.5637e-04 - acc: 0.0000e+00 - val_loss: 2.1600e-04 - val_acc: 0.0000e+00
Epoch 46/300
13702/13702 [==============================] - 1s - loss: 1.8695e-04 - acc: 0.0000e+00 - val_loss: 7.4549e-04 - val_acc: 0.0000e+00
Epoch 47/300
13702/13702 [==============================] - 1s - loss: 1.8026e-04 - acc: 0.0000e+00 - val_loss: 2.4794e-04 - val_acc: 0.0000e+00
Epoch 48/300
13702/13702 [==============================] - 1s - loss: 1.7000e-04 - acc: 0.0000e+00 - val_loss: 3.5737e-04 - val_acc: 0.0000e+00
Epoch 49/300
13702/13702 [==============================] - 1s - loss: 1.6763e-04 - acc: 0.0000e+00 - val_loss: 1.6839e-04 - val_acc: 0.0000e+00
Epoch 50/300
13702/13702 [==============================] - 1s - loss: 1.5622e-04 - acc: 0.0000e+00 - val_loss: 1.7348e-04 - val_acc: 0.0000e+00
Epoch 51/300
13702/13702 [==============================] - 1s - loss: 1.5985e-04 - acc: 0.0000e+00 - val_loss: 1.6267e-04 - val_acc: 0.0000e+00
Epoch 52/300
13702/13702 [==============================] - 1s - loss: 1.6144e-04 - acc: 0.0000e+00 - val_loss: 3.1402e-04 - val_acc: 0.0000e+00
Epoch 53/300
13702/13702 [==============================] - 1s - loss: 1.7031e-04 - acc: 0.0000e+00 - val_loss: 8.4756e-04 - val_acc: 0.0000e+00
Epoch 54/300
13702/13702 [==============================] - 1s - loss: 1.6498e-04 - acc: 0.0000e+00 - val_loss: 1.6553e-04 - val_acc: 0.0000e+00
Epoch 55/300
13702/13702 [==============================] - 1s - loss: 1.5236e-04 - acc: 0.0000e+00 - val_loss: 1.5752e-04 - val_acc: 0.0000e+00
Epoch 56/300
13702/13702 [==============================] - 1s - loss: 1.6381e-04 - acc: 0.0000e+00 - val_loss: 1.5817e-04 - val_acc: 0.0000e+00
Epoch 57/300
13702/13702 [==============================] - 1s - loss: 1.6023e-04 - acc: 0.0000e+00 - val_loss: 2.7356e-04 - val_acc: 0.0000e+00
Epoch 58/300
13702/13702 [==============================] - 1s - loss: 1.3247e-04 - acc: 0.0000e+00 - val_loss: 1.6422e-04 - val_acc: 0.0000e+00
Epoch 59/300
13702/13702 [==============================] - 1s - loss: 1.4644e-04 - acc: 0.0000e+00 - val_loss: 1.5575e-04 - val_acc: 0.0000e+00
Epoch 60/300
13702/13702 [==============================] - 1s - loss: 1.6397e-04 - acc: 0.0000e+00 - val_loss: 1.5144e-04 - val_acc: 0.0000e+00
Epoch 61/300
13702/13702 [==============================] - 1s - loss: 1.5445e-04 - acc: 0.0000e+00 - val_loss: 2.7695e-04 - val_acc: 0.0000e+00
Epoch 62/300
13702/13702 [==============================] - 1s - loss: 1.4848e-04 - acc: 0.0000e+00 - val_loss: 1.7248e-04 - val_acc: 0.0000e+00
Epoch 63/300
13702/13702 [==============================] - 1s - loss: 1.3311e-04 - acc: 0.0000e+00 - val_loss: 4.0052e-04 - val_acc: 0.0000e+00
Epoch 64/300
13702/13702 [==============================] - 1s - loss: 1.4159e-04 - acc: 0.0000e+00 - val_loss: 2.3234e-04 - val_acc: 0.0000e+00
Epoch 65/300
13702/13702 [==============================] - 1s - loss: 1.4738e-04 - acc: 0.0000e+00 - val_loss: 1.4232e-04 - val_acc: 0.0000e+00
Epoch 66/300
13702/13702 [==============================] - 1s - loss: 1.4297e-04 - acc: 0.0000e+00 - val_loss: 1.4645e-04 - val_acc: 0.0000e+00
Epoch 67/300
13702/13702 [==============================] - 1s - loss: 1.3418e-04 - acc: 0.0000e+00 - val_loss: 1.4097e-04 - val_acc: 0.0000e+00
Epoch 68/300
13702/13702 [==============================] - 1s - loss: 1.3199e-04 - acc: 0.0000e+00 - val_loss: 3.6554e-04 - val_acc: 0.0000e+00
Epoch 69/300
13702/13702 [==============================] - 1s - loss: 1.4490e-04 - acc: 0.0000e+00 - val_loss: 1.7464e-04 - val_acc: 0.0000e+00
Epoch 70/300
13702/13702 [==============================] - 1s - loss: 1.2879e-04 - acc: 0.0000e+00 - val_loss: 1.6936e-04 - val_acc: 0.0000e+00
Epoch 71/300
13702/13702 [==============================] - 1s - loss: 1.2345e-04 - acc: 0.0000e+00 - val_loss: 1.6393e-04 - val_acc: 0.0000e+00
Epoch 72/300
13702/13702 [==============================] - 1s - loss: 1.2255e-04 - acc: 0.0000e+00 - val_loss: 1.8113e-04 - val_acc: 0.0000e+00
Epoch 73/300
13702/13702 [==============================] - 1s - loss: 1.3485e-04 - acc: 0.0000e+00 - val_loss: 1.3567e-04 - val_acc: 0.0000e+00
Epoch 74/300
13702/13702 [==============================] - 1s - loss: 1.2201e-04 - acc: 0.0000e+00 - val_loss: 1.4130e-04 - val_acc: 0.0000e+00
Epoch 75/300
13702/13702 [==============================] - 1s - loss: 1.2188e-04 - acc: 0.0000e+00 - val_loss: 1.9105e-04 - val_acc: 0.0000e+0000
Epoch 76/300
13702/13702 [==============================] - 1s - loss: 1.1391e-04 - acc: 0.0000e+00 - val_loss: 2.3136e-04 - val_acc: 0.0000e+00
Epoch 77/300
13702/13702 [==============================] - 1s - loss: 1.2012e-04 - acc: 0.0000e+00 - val_loss: 1.8512e-04 - val_acc: 0.0000e+00
Epoch 78/300
13702/13702 [==============================] - 1s - loss: 1.1108e-04 - acc: 0.0000e+00 - val_loss: 1.5274e-04 - val_acc: 0.0000e+00
Epoch 79/300
13702/13702 [==============================] - 1s - loss: 1.2238e-04 - acc: 0.0000e+00 - val_loss: 1.3438e-04 - val_acc: 0.0000e+00
Epoch 80/300
13702/13702 [==============================] - 1s - loss: 1.2524e-04 - acc: 0.0000e+00 - val_loss: 4.8044e-04 - val_acc: 0.0000e+00
Epoch 81/300
13702/13702 [==============================] - 1s - loss: 1.2214e-04 - acc: 0.0000e+00 - val_loss: 1.4596e-04 - val_acc: 0.0000e+00
Epoch 82/300
13702/13702 [==============================] - 1s - loss: 1.1457e-04 - acc: 0.0000e+00 - val_loss: 3.0914e-04 - val_acc: 0.0000e+00
Epoch 83/300
13702/13702 [==============================] - 1s - loss: 1.1937e-04 - acc: 0.0000e+00 - val_loss: 1.3383e-04 - val_acc: 0.0000e+00
Epoch 84/300
13702/13702 [==============================] - 1s - loss: 1.1249e-04 - acc: 0.0000e+00 - val_loss: 1.3054e-04 - val_acc: 0.0000e+00
Epoch 85/300
13702/13702 [==============================] - 1s - loss: 1.0861e-04 - acc: 0.0000e+00 - val_loss: 3.2082e-04 - val_acc: 0.0000e+00
Epoch 86/300
13702/13702 [==============================] - 1s - loss: 1.3097e-04 - acc: 0.0000e+00 - val_loss: 1.5170e-04 - val_acc: 0.0000e+00
Epoch 87/300
13702/13702 [==============================] - 1s - loss: 1.1506e-04 - acc: 0.0000e+00 - val_loss: 2.0245e-04 - val_acc: 0.0000e+00
Epoch 88/300
13702/13702 [==============================] - 1s - loss: 1.2407e-04 - acc: 0.0000e+00 - val_loss: 2.0201e-04 - val_acc: 0.0000e+00
Epoch 89/300
13702/13702 [==============================] - 1s - loss: 1.0012e-04 - acc: 0.0000e+00 - val_loss: 1.2907e-04 - val_acc: 0.0000e+00
Epoch 90/300
13702/13702 [==============================] - 1s - loss: 1.1059e-04 - acc: 0.0000e+00 - val_loss: 1.3377e-04 - val_acc: 0.0000e+00
Epoch 91/300
13702/13702 [==============================] - 1s - loss: 1.1127e-04 - acc: 0.0000e+00 - val_loss: 2.1563e-04 - val_acc: 0.0000e+00
Epoch 92/300
13702/13702 [==============================] - 1s - loss: 1.0752e-04 - acc: 0.0000e+00 - val_loss: 1.2963e-04 - val_acc: 0.0000e+00
Epoch 93/300
13702/13702 [==============================] - 1s - loss: 1.0548e-04 - acc: 0.0000e+00 - val_loss: 1.4898e-04 - val_acc: 0.0000e+00
Epoch 94/300
13702/13702 [==============================] - 1s - loss: 9.7504e-05 - acc: 0.0000e+00 - val_loss: 1.2921e-04 - val_acc: 0.0000e+00
Epoch 95/300
13702/13702 [==============================] - 1s - loss: 1.0204e-04 - acc: 0.0000e+00 - val_loss: 1.8416e-04 - val_acc: 0.0000e+00
Epoch 96/300
13702/13702 [==============================] - 1s - loss: 9.4265e-05 - acc: 0.0000e+00 - val_loss: 1.2894e-04 - val_acc: 0.0000e+00
Epoch 97/300
13702/13702 [==============================] - 1s - loss: 9.3720e-05 - acc: 0.0000e+00 - val_loss: 1.2774e-04 - val_acc: 0.0000e+00
Epoch 98/300
13702/13702 [==============================] - 1s - loss: 1.0131e-04 - acc: 0.0000e+00 - val_loss: 1.5685e-04 - val_acc: 0.0000e+00
Epoch 99/300
13702/13702 [==============================] - 1s - loss: 9.4044e-05 - acc: 0.0000e+00 - val_loss: 1.4545e-04 - val_acc: 0.0000e+00
Epoch 100/300
13702/13702 [==============================] - 1s - loss: 9.6335e-05 - acc: 0.0000e+00 - val_loss: 2.2784e-04 - val_acc: 0.0000e+00
Epoch 101/300
13702/13702 [==============================] - 1s - loss: 9.3834e-05 - acc: 0.0000e+00 - val_loss: 1.9397e-04 - val_acc: 0.0000e+00
Epoch 102/300
13702/13702 [==============================] - 1s - loss: 9.2517e-05 - acc: 0.0000e+00 - val_loss: 1.2676e-04 - val_acc: 0.0000e+00
Epoch 103/300
13702/13702 [==============================] - 1s - loss: 1.0039e-04 - acc: 0.0000e+00 - val_loss: 1.4698e-04 - val_acc: 0.0000e+00
Epoch 104/300
13702/13702 [==============================] - 1s - loss: 8.6438e-05 - acc: 0.0000e+00 - val_loss: 2.8614e-04 - val_acc: 0.0000e+00
Epoch 105/300
13702/13702 [==============================] - 1s - loss: 1.0214e-04 - acc: 0.0000e+00 - val_loss: 1.2785e-04 - val_acc: 0.0000e+00
Epoch 106/300
13702/13702 [==============================] - 1s - loss: 8.3759e-05 - acc: 0.0000e+00 - val_loss: 1.7592e-04 - val_acc: 0.0000e+00
Epoch 107/300
13702/13702 [==============================] - 1s - loss: 8.7740e-05 - acc: 0.0000e+00 - val_loss: 1.8724e-04 - val_acc: 0.0000e+00
Epoch 108/300
13702/13702 [==============================] - 1s - loss: 8.6387e-05 - acc: 0.0000e+00 - val_loss: 1.3339e-04 - val_acc: 0.0000e+00
Epoch 109/300
13702/13702 [==============================] - 1s - loss: 9.2106e-05 - acc: 0.0000e+00 - val_loss: 3.1184e-04 - val_acc: 0.0000e+00
Epoch 110/300
13702/13702 [==============================] - 1s - loss: 8.9505e-05 - acc: 0.0000e+00 - val_loss: 1.6738e-04 - val_acc: 0.0000e+00
Epoch 111/300
13702/13702 [==============================] - 1s - loss: 8.4567e-05 - acc: 0.0000e+00 - val_loss: 1.5068e-04 - val_acc: 0.0000e+00
Epoch 112/300
13702/13702 [==============================] - 1s - loss: 8.8620e-05 - acc: 0.0000e+00 - val_loss: 1.5387e-04 - val_acc: 0.0000e+00
Epoch 113/300
13702/13702 [==============================] - 1s - loss: 8.8195e-05 - acc: 0.0000e+00 - val_loss: 2.4338e-04 - val_acc: 0.0000e+00
Epoch 114/300
13702/13702 [==============================] - 1s - loss: 8.6478e-05 - acc: 0.0000e+00 - val_loss: 1.3077e-04 - val_acc: 0.0000e+00
Epoch 115/300
13702/13702 [==============================] - 1s - loss: 7.6121e-05 - acc: 0.0000e+00 - val_loss: 1.2244e-04 - val_acc: 0.0000e+00
Epoch 116/300
13702/13702 [==============================] - 1s - loss: 8.2299e-05 - acc: 0.0000e+00 - val_loss: 1.3208e-04 - val_acc: 0.0000e+00
Epoch 117/300
13702/13702 [==============================] - 1s - loss: 9.2623e-05 - acc: 0.0000e+00 - val_loss: 1.2590e-04 - val_acc: 0.0000e+00
Epoch 118/300
13702/13702 [==============================] - 1s - loss: 8.3200e-05 - acc: 0.0000e+00 - val_loss: 1.4197e-04 - val_acc: 0.0000e+00
Epoch 119/300
13702/13702 [==============================] - 1s - loss: 8.0783e-05 - acc: 0.0000e+00 - val_loss: 1.2855e-04 - val_acc: 0.0000e+00
Epoch 120/300
13702/13702 [==============================] - 1s - loss: 9.2233e-05 - acc: 0.0000e+00 - val_loss: 1.3273e-04 - val_acc: 0.0000e+00
Epoch 121/300
13702/13702 [==============================] - 1s - loss: 8.0185e-05 - acc: 0.0000e+00 - val_loss: 2.2177e-04 - val_acc: 0.0000e+00
Epoch 122/300
13702/13702 [==============================] - 1s - loss: 8.1807e-05 - acc: 0.0000e+00 - val_loss: 1.7995e-04 - val_acc: 0.0000e+00
Epoch 123/300
13702/13702 [==============================] - 1s - loss: 8.0771e-05 - acc: 0.0000e+00 - val_loss: 1.2267e-04 - val_acc: 0.0000e+00
Epoch 124/300
13702/13702 [==============================] - 1s - loss: 7.1813e-05 - acc: 0.0000e+00 - val_loss: 1.3659e-04 - val_acc: 0.0000e+00
Epoch 125/300
13702/13702 [==============================] - 1s - loss: 7.2489e-05 - acc: 0.0000e+00 - val_loss: 1.3445e-04 - val_acc: 0.0000e+00
Epoch 126/300
13702/13702 [==============================] - 1s - loss: 7.7506e-05 - acc: 0.0000e+00 - val_loss: 3.8466e-04 - val_acc: 0.0000e+00
Epoch 127/300
13702/13702 [==============================] - 1s - loss: 7.9159e-05 - acc: 0.0000e+00 - val_loss: 2.2810e-04 - val_acc: 0.0000e+00
Epoch 128/300
13702/13702 [==============================] - 1s - loss: 7.8937e-05 - acc: 0.0000e+00 - val_loss: 1.2509e-04 - val_acc: 0.0000e+00
Epoch 129/300
13702/13702 [==============================] - 1s - loss: 7.7215e-05 - acc: 0.0000e+00 - val_loss: 1.3817e-04 - val_acc: 0.0000e+00
Epoch 130/300
13702/13702 [==============================] - 1s - loss: 7.0868e-05 - acc: 0.0000e+00 - val_loss: 1.2075e-04 - val_acc: 0.0000e+00
Epoch 131/300
13702/13702 [==============================] - 1s - loss: 7.7859e-05 - acc: 0.0000e+00 - val_loss: 1.5659e-04 - val_acc: 0.0000e+00
Epoch 132/300
13702/13702 [==============================] - 1s - loss: 6.9857e-05 - acc: 0.0000e+00 - val_loss: 1.1981e-04 - val_acc: 0.0000e+00
Epoch 133/300
13702/13702 [==============================] - 1s - loss: 7.9683e-05 - acc: 0.0000e+00 - val_loss: 1.4242e-04 - val_acc: 0.0000e+00
Epoch 134/300
13702/13702 [==============================] - 1s - loss: 7.3464e-05 - acc: 0.0000e+00 - val_loss: 1.1791e-04 - val_acc: 0.0000e+00
Epoch 135/300
13702/13702 [==============================] - 1s - loss: 7.7422e-05 - acc: 0.0000e+00 - val_loss: 1.7474e-04 - val_acc: 0.0000e+00
Epoch 136/300
13702/13702 [==============================] - 1s - loss: 6.8859e-05 - acc: 0.0000e+00 - val_loss: 1.2569e-04 - val_acc: 0.0000e+00
Epoch 137/300
13702/13702 [==============================] - 1s - loss: 7.5241e-05 - acc: 0.0000e+00 - val_loss: 1.1227e-04 - val_acc: 0.0000e+00
Epoch 138/300
13702/13702 [==============================] - 1s - loss: 7.0622e-05 - acc: 0.0000e+00 - val_loss: 1.9055e-04 - val_acc: 0.0000e+00
Epoch 139/300
13702/13702 [==============================] - 1s - loss: 7.2562e-05 - acc: 0.0000e+00 - val_loss: 1.1668e-04 - val_acc: 0.0000e+00
Epoch 140/300
13702/13702 [==============================] - 1s - loss: 7.0783e-05 - acc: 0.0000e+00 - val_loss: 1.7992e-04 - val_acc: 0.0000e+00
Epoch 141/300
13702/13702 [==============================] - 1s - loss: 7.1424e-05 - acc: 0.0000e+00 - val_loss: 1.9834e-04 - val_acc: 0.0000e+00
Epoch 142/300
13702/13702 [==============================] - 1s - loss: 7.1905e-05 - acc: 0.0000e+00 - val_loss: 1.1222e-04 - val_acc: 0.0000e+00
Epoch 143/300
13702/13702 [==============================] - 1s - loss: 6.6254e-05 - acc: 0.0000e+00 - val_loss: 1.5660e-04 - val_acc: 0.0000e+00
Epoch 144/300
13702/13702 [==============================] - 1s - loss: 7.5019e-05 - acc: 0.0000e+00 - val_loss: 1.8028e-04 - val_acc: 0.0000e+00
Epoch 145/300
13702/13702 [==============================] - 1s - loss: 7.8853e-05 - acc: 0.0000e+00 - val_loss: 1.1951e-04 - val_acc: 0.0000e+00
Epoch 146/300
13702/13702 [==============================] - 1s - loss: 6.9495e-05 - acc: 0.0000e+00 - val_loss: 1.2143e-04 - val_acc: 0.0000e+00
Epoch 147/300
13702/13702 [==============================] - 1s - loss: 7.1706e-05 - acc: 0.0000e+00 - val_loss: 2.0505e-04 - val_acc: 0.0000e+00
Epoch 148/300
13702/13702 [==============================] - 1s - loss: 6.6048e-05 - acc: 0.0000e+00 - val_loss: 1.3667e-04 - val_acc: 0.0000e+00
Epoch 149/300
13702/13702 [==============================] - 1s - loss: 7.2882e-05 - acc: 0.0000e+00 - val_loss: 1.2253e-04 - val_acc: 0.0000e+00
Epoch 150/300
13702/13702 [==============================] - 1s - loss: 7.4999e-05 - acc: 0.0000e+00 - val_loss: 1.2375e-04 - val_acc: 0.0000e+00
Epoch 151/300
13702/13702 [==============================] - 1s - loss: 6.8232e-05 - acc: 0.0000e+00 - val_loss: 1.3332e-04 - val_acc: 0.0000e+00
Epoch 152/300
13702/13702 [==============================] - 1s - loss: 7.0559e-05 - acc: 0.0000e+00 - val_loss: 1.1764e-04 - val_acc: 0.0000e+00
Epoch 153/300
13702/13702 [==============================] - 1s - loss: 6.6864e-05 - acc: 0.0000e+00 - val_loss: 1.4618e-04 - val_acc: 0.0000e+00
Epoch 154/300
13702/13702 [==============================] - 1s - loss: 6.6028e-05 - acc: 0.0000e+00 - val_loss: 1.1745e-04 - val_acc: 0.0000e+00
Epoch 155/300
13702/13702 [==============================] - 1s - loss: 6.1782e-05 - acc: 0.0000e+00 - val_loss: 1.2184e-04 - val_acc: 0.0000e+00
Epoch 156/300
13702/13702 [==============================] - 1s - loss: 6.6450e-05 - acc: 0.0000e+00 - val_loss: 1.2035e-04 - val_acc: 0.0000e+00
Epoch 157/300
13702/13702 [==============================] - 1s - loss: 6.4612e-05 - acc: 0.0000e+00 - val_loss: 1.2221e-04 - val_acc: 0.0000e+00
Epoch 158/300
13702/13702 [==============================] - 1s - loss: 7.0593e-05 - acc: 0.0000e+00 - val_loss: 2.7241e-04 - val_acc: 0.0000e+00
Epoch 159/300
13702/13702 [==============================] - 1s - loss: 7.1296e-05 - acc: 0.0000e+00 - val_loss: 1.4057e-04 - val_acc: 0.0000e+00
Epoch 160/300
13702/13702 [==============================] - 1s - loss: 6.1691e-05 - acc: 0.0000e+00 - val_loss: 1.1298e-04 - val_acc: 0.0000e+00
Epoch 161/300
13702/13702 [==============================] - 1s - loss: 6.9301e-05 - acc: 0.0000e+00 - val_loss: 1.1585e-04 - val_acc: 0.0000e+00
Epoch 162/300
13702/13702 [==============================] - 1s - loss: 6.4087e-05 - acc: 0.0000e+00 - val_loss: 1.5222e-04 - val_acc: 0.0000e+00
Epoch 163/300
13702/13702 [==============================] - 1s - loss: 6.3793e-05 - acc: 0.0000e+00 - val_loss: 1.7010e-04 - val_acc: 0.0000e+00
Epoch 164/300
13702/13702 [==============================] - 1s - loss: 6.8608e-05 - acc: 0.0000e+00 - val_loss: 3.1628e-04 - val_acc: 0.0000e+00
Epoch 165/300
13702/13702 [==============================] - 1s - loss: 7.0132e-05 - acc: 0.0000e+00 - val_loss: 1.1609e-04 - val_acc: 0.0000e+00
Epoch 166/300
13702/13702 [==============================] - 1s - loss: 6.9550e-05 - acc: 0.0000e+00 - val_loss: 1.1385e-04 - val_acc: 0.0000e+00
Epoch 167/300
13702/13702 [==============================] - 1s - loss: 6.2328e-05 - acc: 0.0000e+00 - val_loss: 2.0685e-04 - val_acc: 0.0000e+00
Epoch 168/300
13702/13702 [==============================] - 1s - loss: 5.8998e-05 - acc: 0.0000e+00 - val_loss: 1.1409e-04 - val_acc: 0.0000e+00
Epoch 169/300
13702/13702 [==============================] - 1s - loss: 5.8152e-05 - acc: 0.0000e+00 - val_loss: 1.6447e-04 - val_acc: 0.0000e+0000e+
Epoch 170/300
13702/13702 [==============================] - 1s - loss: 6.3882e-05 - acc: 0.0000e+00 - val_loss: 1.2515e-04 - val_acc: 0.0000e+00
Epoch 171/300
13702/13702 [==============================] - 1s - loss: 5.8797e-05 - acc: 0.0000e+00 - val_loss: 1.3338e-04 - val_acc: 0.0000e+00
Epoch 172/300
13702/13702 [==============================] - 1s - loss: 6.3242e-05 - acc: 0.0000e+00 - val_loss: 1.1289e-04 - val_acc: 0.0000e+00
Epoch 173/300
13702/13702 [==============================] - 1s - loss: 6.1616e-05 - acc: 0.0000e+00 - val_loss: 1.5281e-04 - val_acc: 0.0000e+00
Epoch 174/300
13702/13702 [==============================] - 1s - loss: 5.9882e-05 - acc: 0.0000e+00 - val_loss: 1.2248e-04 - val_acc: 0.0000e+00
Epoch 175/300
13702/13702 [==============================] - 1s - loss: 6.4397e-05 - acc: 0.0000e+00 - val_loss: 1.1416e-04 - val_acc: 0.0000e+00
Epoch 176/300
13702/13702 [==============================] - 1s - loss: 6.0084e-05 - acc: 0.0000e+00 - val_loss: 1.2345e-04 - val_acc: 0.0000e+00
Epoch 177/300
13702/13702 [==============================] - 1s - loss: 6.2322e-05 - acc: 0.0000e+00 - val_loss: 1.0766e-04 - val_acc: 0.0000e+00
Epoch 178/300
13702/13702 [==============================] - 1s - loss: 5.6023e-05 - acc: 0.0000e+00 - val_loss: 1.1570e-04 - val_acc: 0.0000e+00
Epoch 179/300
13702/13702 [==============================] - 1s - loss: 6.7515e-05 - acc: 0.0000e+00 - val_loss: 1.1179e-04 - val_acc: 0.0000e+00
Epoch 180/300
13702/13702 [==============================] - 1s - loss: 5.7967e-05 - acc: 0.0000e+00 - val_loss: 1.2051e-04 - val_acc: 0.0000e+00
Epoch 181/300
13702/13702 [==============================] - 1s - loss: 6.5422e-05 - acc: 0.0000e+00 - val_loss: 1.7609e-04 - val_acc: 0.0000e+00
Epoch 182/300
13702/13702 [==============================] - 1s - loss: 6.1599e-05 - acc: 0.0000e+00 - val_loss: 1.5782e-04 - val_acc: 0.0000e+00
Epoch 183/300
13702/13702 [==============================] - 1s - loss: 6.3628e-05 - acc: 0.0000e+00 - val_loss: 1.2488e-04 - val_acc: 0.0000e+00
Epoch 184/300
13702/13702 [==============================] - 1s - loss: 6.2825e-05 - acc: 0.0000e+00 - val_loss: 1.1283e-04 - val_acc: 0.0000e+00
Epoch 185/300
13702/13702 [==============================] - 1s - loss: 5.9425e-05 - acc: 0.0000e+00 - val_loss: 1.2524e-04 - val_acc: 0.0000e+00
Epoch 186/300
13702/13702 [==============================] - 1s - loss: 6.4888e-05 - acc: 0.0000e+00 - val_loss: 1.2252e-04 - val_acc: 0.0000e+00
Epoch 187/300
13702/13702 [==============================] - 1s - loss: 5.8506e-05 - acc: 0.0000e+00 - val_loss: 1.4287e-04 - val_acc: 0.0000e+00
Epoch 188/300
13702/13702 [==============================] - 1s - loss: 5.7745e-05 - acc: 0.0000e+00 - val_loss: 2.0512e-04 - val_acc: 0.0000e+00
Epoch 189/300
13702/13702 [==============================] - 1s - loss: 6.1121e-05 - acc: 0.0000e+00 - val_loss: 1.9467e-04 - val_acc: 0.0000e+00
Epoch 190/300
13702/13702 [==============================] - 1s - loss: 6.2846e-05 - acc: 0.0000e+00 - val_loss: 1.2329e-04 - val_acc: 0.0000e+00
Epoch 191/300
13702/13702 [==============================] - 1s - loss: 5.5664e-05 - acc: 0.0000e+00 - val_loss: 1.9110e-04 - val_acc: 0.0000e+00
Epoch 192/300
13702/13702 [==============================] - 1s - loss: 5.8697e-05 - acc: 0.0000e+00 - val_loss: 1.2126e-04 - val_acc: 0.0000e+00
Epoch 193/300
13702/13702 [==============================] - 1s - loss: 6.6038e-05 - acc: 0.0000e+00 - val_loss: 2.3818e-04 - val_acc: 0.0000e+00
Epoch 194/300
13702/13702 [==============================] - 1s - loss: 5.8044e-05 - acc: 0.0000e+00 - val_loss: 1.3099e-04 - val_acc: 0.0000e+00
Epoch 195/300
13702/13702 [==============================] - 1s - loss: 6.3732e-05 - acc: 0.0000e+00 - val_loss: 1.1213e-04 - val_acc: 0.0000e+00
Epoch 196/300
13702/13702 [==============================] - 1s - loss: 6.3969e-05 - acc: 0.0000e+00 - val_loss: 1.3187e-04 - val_acc: 0.0000e+00
Epoch 197/300
13702/13702 [==============================] - 1s - loss: 6.2717e-05 - acc: 0.0000e+00 - val_loss: 1.0646e-04 - val_acc: 0.0000e+00
Epoch 198/300
13702/13702 [==============================] - 1s - loss: 5.7569e-05 - acc: 0.0000e+00 - val_loss: 1.1197e-04 - val_acc: 0.0000e+00
Epoch 199/300
13702/13702 [==============================] - 1s - loss: 5.7443e-05 - acc: 0.0000e+00 - val_loss: 1.0446e-04 - val_acc: 0.0000e+00
Epoch 200/300
13702/13702 [==============================] - 1s - loss: 6.2139e-05 - acc: 0.0000e+00 - val_loss: 1.0632e-04 - val_acc: 0.0000e+00
Epoch 201/300
13702/13702 [==============================] - 1s - loss: 5.6544e-05 - acc: 0.0000e+00 - val_loss: 1.2540e-04 - val_acc: 0.0000e+00
Epoch 202/300
13702/13702 [==============================] - 1s - loss: 5.9468e-05 - acc: 0.0000e+00 - val_loss: 1.9867e-04 - val_acc: 0.0000e+00
Epoch 203/300
13702/13702 [==============================] - 1s - loss: 6.1620e-05 - acc: 0.0000e+00 - val_loss: 1.0763e-04 - val_acc: 0.0000e+00
Epoch 204/300
13702/13702 [==============================] - 1s - loss: 5.6003e-05 - acc: 0.0000e+00 - val_loss: 1.0648e-04 - val_acc: 0.0000e+00
Epoch 205/300
13702/13702 [==============================] - 1s - loss: 6.2303e-05 - acc: 0.0000e+00 - val_loss: 1.0303e-04 - val_acc: 0.0000e+00
Epoch 206/300
13702/13702 [==============================] - 1s - loss: 5.9844e-05 - acc: 0.0000e+00 - val_loss: 1.0600e-04 - val_acc: 0.0000e+00
Epoch 207/300
13702/13702 [==============================] - 1s - loss: 6.2151e-05 - acc: 0.0000e+00 - val_loss: 1.4827e-04 - val_acc: 0.0000e+00
Epoch 208/300
13702/13702 [==============================] - 1s - loss: 7.4383e-05 - acc: 0.0000e+00 - val_loss: 1.1854e-04 - val_acc: 0.0000e+00
Epoch 209/300
13702/13702 [==============================] - 1s - loss: 5.9670e-05 - acc: 0.0000e+00 - val_loss: 1.0738e-04 - val_acc: 0.0000e+00
Epoch 210/300
13702/13702 [==============================] - 1s - loss: 5.8342e-05 - acc: 0.0000e+00 - val_loss: 1.0530e-04 - val_acc: 0.0000e+00
Epoch 211/300
13702/13702 [==============================] - 1s - loss: 5.3738e-05 - acc: 0.0000e+00 - val_loss: 1.0798e-04 - val_acc: 0.0000e+00
Epoch 212/300
13702/13702 [==============================] - 1s - loss: 6.0454e-05 - acc: 0.0000e+00 - val_loss: 1.2263e-04 - val_acc: 0.0000e+00
Epoch 213/300
13702/13702 [==============================] - 1s - loss: 6.1032e-05 - acc: 0.0000e+00 - val_loss: 1.3996e-04 - val_acc: 0.0000e+00
Epoch 214/300
13702/13702 [==============================] - 1s - loss: 5.9959e-05 - acc: 0.0000e+00 - val_loss: 1.1025e-04 - val_acc: 0.0000e+00
Epoch 215/300
13702/13702 [==============================] - 1s - loss: 5.7925e-05 - acc: 0.0000e+00 - val_loss: 1.5510e-04 - val_acc: 0.0000e+00
Epoch 216/300
13702/13702 [==============================] - 1s - loss: 6.1216e-05 - acc: 0.0000e+00 - val_loss: 1.9629e-04 - val_acc: 0.0000e+00
Epoch 217/300
13702/13702 [==============================] - 1s - loss: 5.7475e-05 - acc: 0.0000e+00 - val_loss: 1.0115e-04 - val_acc: 0.0000e+00
Epoch 218/300
13702/13702 [==============================] - 1s - loss: 5.1581e-05 - acc: 0.0000e+00 - val_loss: 1.1191e-04 - val_acc: 0.0000e+00
Epoch 219/300
13702/13702 [==============================] - 1s - loss: 5.4284e-05 - acc: 0.0000e+00 - val_loss: 1.0124e-04 - val_acc: 0.0000e+00
Epoch 220/300
13702/13702 [==============================] - 1s - loss: 5.7498e-05 - acc: 0.0000e+00 - val_loss: 1.1479e-04 - val_acc: 0.0000e+00
Epoch 221/300
13702/13702 [==============================] - 1s - loss: 5.8663e-05 - acc: 0.0000e+00 - val_loss: 1.7664e-04 - val_acc: 0.0000e+00
Epoch 222/300
13702/13702 [==============================] - 1s - loss: 5.7405e-05 - acc: 0.0000e+00 - val_loss: 1.0198e-04 - val_acc: 0.0000e+00
Epoch 223/300
13702/13702 [==============================] - 1s - loss: 5.3040e-05 - acc: 0.0000e+00 - val_loss: 1.2547e-04 - val_acc: 0.0000e+00
Epoch 224/300
13702/13702 [==============================] - 1s - loss: 5.7431e-05 - acc: 0.0000e+00 - val_loss: 1.9220e-04 - val_acc: 0.0000e+00
Epoch 225/300
13702/13702 [==============================] - 1s - loss: 5.5968e-05 - acc: 0.0000e+00 - val_loss: 1.5135e-04 - val_acc: 0.0000e+00
Epoch 226/300
13702/13702 [==============================] - 1s - loss: 6.2070e-05 - acc: 0.0000e+00 - val_loss: 1.4379e-04 - val_acc: 0.0000e+00
Epoch 227/300
13702/13702 [==============================] - 1s - loss: 5.4778e-05 - acc: 0.0000e+00 - val_loss: 1.2681e-04 - val_acc: 0.0000e+00
Epoch 228/300
13702/13702 [==============================] - 1s - loss: 5.3250e-05 - acc: 0.0000e+00 - val_loss: 9.7876e-05 - val_acc: 0.0000e+00
Epoch 229/300
13702/13702 [==============================] - 1s - loss: 6.0361e-05 - acc: 0.0000e+00 - val_loss: 1.3320e-04 - val_acc: 0.0000e+00
Epoch 230/300
13702/13702 [==============================] - 1s - loss: 5.8374e-05 - acc: 0.0000e+00 - val_loss: 1.3013e-04 - val_acc: 0.0000e+00
Epoch 231/300
13702/13702 [==============================] - 1s - loss: 5.2750e-05 - acc: 0.0000e+00 - val_loss: 1.0110e-04 - val_acc: 0.0000e+00
Epoch 232/300
13702/13702 [==============================] - 1s - loss: 5.3470e-05 - acc: 0.0000e+00 - val_loss: 1.1140e-04 - val_acc: 0.0000e+00
Epoch 233/300
13702/13702 [==============================] - 1s - loss: 5.5594e-05 - acc: 0.0000e+00 - val_loss: 1.5273e-04 - val_acc: 0.0000e+00
Epoch 234/300
13702/13702 [==============================] - 1s - loss: 5.2522e-05 - acc: 0.0000e+00 - val_loss: 1.1535e-04 - val_acc: 0.0000e+00
Epoch 235/300
13702/13702 [==============================] - 1s - loss: 5.6578e-05 - acc: 0.0000e+00 - val_loss: 1.5998e-04 - val_acc: 0.0000e+00
Epoch 236/300
13702/13702 [==============================] - 1s - loss: 5.4088e-05 - acc: 0.0000e+00 - val_loss: 1.5390e-04 - val_acc: 0.0000e+00
Epoch 237/300
13702/13702 [==============================] - 1s - loss: 5.6779e-05 - acc: 0.0000e+00 - val_loss: 1.4682e-04 - val_acc: 0.0000e+0000e+
Epoch 238/300
13702/13702 [==============================] - 1s - loss: 5.7922e-05 - acc: 0.0000e+00 - val_loss: 9.3174e-05 - val_acc: 0.0000e+00
Epoch 239/300
13702/13702 [==============================] - 1s - loss: 5.3265e-05 - acc: 0.0000e+00 - val_loss: 1.8856e-04 - val_acc: 0.0000e+00
Epoch 240/300
13702/13702 [==============================] - 1s - loss: 5.9233e-05 - acc: 0.0000e+00 - val_loss: 1.8091e-04 - val_acc: 0.0000e+00
Epoch 241/300
13702/13702 [==============================] - 1s - loss: 5.3853e-05 - acc: 0.0000e+00 - val_loss: 3.0095e-04 - val_acc: 0.0000e+00
Epoch 242/300
13702/13702 [==============================] - 1s - loss: 5.7781e-05 - acc: 0.0000e+00 - val_loss: 1.2316e-04 - val_acc: 0.0000e+00
Epoch 243/300
13702/13702 [==============================] - 1s - loss: 5.1021e-05 - acc: 0.0000e+00 - val_loss: 1.5146e-04 - val_acc: 0.0000e+00
Epoch 244/300
13702/13702 [==============================] - 1s - loss: 5.1456e-05 - acc: 0.0000e+00 - val_loss: 1.3466e-04 - val_acc: 0.0000e+00
Epoch 245/300
13702/13702 [==============================] - 1s - loss: 5.5871e-05 - acc: 0.0000e+00 - val_loss: 1.2314e-04 - val_acc: 0.0000e+00
Epoch 246/300
13702/13702 [==============================] - 1s - loss: 5.3193e-05 - acc: 0.0000e+00 - val_loss: 1.0981e-04 - val_acc: 0.0000e+00
Epoch 247/300
13702/13702 [==============================] - 1s - loss: 5.3116e-05 - acc: 0.0000e+00 - val_loss: 1.3150e-04 - val_acc: 0.0000e+00
Epoch 248/300
13702/13702 [==============================] - 1s - loss: 5.6954e-05 - acc: 0.0000e+00 - val_loss: 1.0201e-04 - val_acc: 0.0000e+00
Epoch 249/300
13702/13702 [==============================] - 1s - loss: 5.1027e-05 - acc: 0.0000e+00 - val_loss: 2.4730e-04 - val_acc: 0.0000e+00
Epoch 250/300
13702/13702 [==============================] - 1s - loss: 6.0447e-05 - acc: 0.0000e+00 - val_loss: 1.1942e-04 - val_acc: 0.0000e+00
Epoch 251/300
13702/13702 [==============================] - 1s - loss: 5.4079e-05 - acc: 0.0000e+00 - val_loss: 1.1747e-04 - val_acc: 0.0000e+00
Epoch 252/300
13702/13702 [==============================] - 1s - loss: 5.2458e-05 - acc: 0.0000e+00 - val_loss: 1.0363e-04 - val_acc: 0.0000e+00
Epoch 253/300
13702/13702 [==============================] - 1s - loss: 5.4177e-05 - acc: 0.0000e+00 - val_loss: 1.1266e-04 - val_acc: 0.0000e+00
Epoch 254/300
13702/13702 [==============================] - 1s - loss: 5.3721e-05 - acc: 0.0000e+00 - val_loss: 1.1055e-04 - val_acc: 0.0000e+00
Epoch 255/300
13702/13702 [==============================] - 1s - loss: 5.0623e-05 - acc: 0.0000e+00 - val_loss: 1.2630e-04 - val_acc: 0.0000e+00
Epoch 256/300
13702/13702 [==============================] - 1s - loss: 4.7740e-05 - acc: 0.0000e+00 - val_loss: 1.0913e-04 - val_acc: 0.0000e+00
Epoch 257/300
13702/13702 [==============================] - 1s - loss: 4.6993e-05 - acc: 0.0000e+00 - val_loss: 9.7524e-05 - val_acc: 0.0000e+00
Epoch 258/300
13702/13702 [==============================] - 1s - loss: 5.1242e-05 - acc: 0.0000e+00 - val_loss: 1.1515e-04 - val_acc: 0.0000e+00
Epoch 259/300
13702/13702 [==============================] - 1s - loss: 5.2337e-05 - acc: 0.0000e+00 - val_loss: 9.7952e-05 - val_acc: 0.0000e+00
Epoch 260/300
13702/13702 [==============================] - 1s - loss: 5.2440e-05 - acc: 0.0000e+00 - val_loss: 1.6855e-04 - val_acc: 0.0000e+00
Epoch 261/300
13702/13702 [==============================] - 1s - loss: 5.5380e-05 - acc: 0.0000e+00 - val_loss: 1.2676e-04 - val_acc: 0.0000e+00
Epoch 262/300
13702/13702 [==============================] - 1s - loss: 5.0743e-05 - acc: 0.0000e+00 - val_loss: 2.2898e-04 - val_acc: 0.0000e+00
Epoch 263/300
13702/13702 [==============================] - 1s - loss: 5.5650e-05 - acc: 0.0000e+00 - val_loss: 1.0924e-04 - val_acc: 0.0000e+00
Epoch 264/300
13702/13702 [==============================] - 1s - loss: 4.8488e-05 - acc: 0.0000e+00 - val_loss: 8.8419e-05 - val_acc: 0.0000e+00
Epoch 265/300
13702/13702 [==============================] - 1s - loss: 4.9993e-05 - acc: 0.0000e+00 - val_loss: 1.1606e-04 - val_acc: 0.0000e+00
Epoch 266/300
13702/13702 [==============================] - 1s - loss: 4.9953e-05 - acc: 0.0000e+00 - val_loss: 1.3346e-04 - val_acc: 0.0000e+00
Epoch 267/300
13702/13702 [==============================] - 1s - loss: 5.1597e-05 - acc: 0.0000e+00 - val_loss: 1.6634e-04 - val_acc: 0.0000e+00
Epoch 268/300
13702/13702 [==============================] - 1s - loss: 5.3323e-05 - acc: 0.0000e+00 - val_loss: 1.3890e-04 - val_acc: 0.0000e+00
Epoch 269/300
13702/13702 [==============================] - 1s - loss: 5.1693e-05 - acc: 0.0000e+00 - val_loss: 1.0279e-04 - val_acc: 0.0000e+00
Epoch 270/300
13702/13702 [==============================] - 1s - loss: 5.0918e-05 - acc: 0.0000e+00 - val_loss: 9.3707e-05 - val_acc: 0.0000e+00
Epoch 271/300
13702/13702 [==============================] - 1s - loss: 4.9399e-05 - acc: 0.0000e+00 - val_loss: 1.0380e-04 - val_acc: 0.0000e+00
Epoch 272/300
13702/13702 [==============================] - 1s - loss: 5.3103e-05 - acc: 0.0000e+00 - val_loss: 1.4977e-04 - val_acc: 0.0000e+00
Epoch 273/300
13702/13702 [==============================] - 1s - loss: 5.3189e-05 - acc: 0.0000e+00 - val_loss: 1.3721e-04 - val_acc: 0.0000e+00
Epoch 274/300
13702/13702 [==============================] - 1s - loss: 5.2630e-05 - acc: 0.0000e+00 - val_loss: 1.1542e-04 - val_acc: 0.0000e+00
Epoch 275/300
13702/13702 [==============================] - 1s - loss: 4.9756e-05 - acc: 0.0000e+00 - val_loss: 9.4683e-05 - val_acc: 0.0000e+00
Epoch 276/300
13702/13702 [==============================] - 1s - loss: 4.7233e-05 - acc: 0.0000e+00 - val_loss: 9.0343e-05 - val_acc: 0.0000e+00
Epoch 277/300
13702/13702 [==============================] - 1s - loss: 4.5547e-05 - acc: 0.0000e+00 - val_loss: 9.0549e-05 - val_acc: 0.0000e+00
Epoch 278/300
13702/13702 [==============================] - 1s - loss: 4.8327e-05 - acc: 0.0000e+00 - val_loss: 8.9974e-05 - val_acc: 0.0000e+00
Epoch 279/300
13702/13702 [==============================] - 1s - loss: 4.7631e-05 - acc: 0.0000e+00 - val_loss: 9.6555e-05 - val_acc: 0.0000e+00
Epoch 280/300
13702/13702 [==============================] - 1s - loss: 5.5944e-05 - acc: 0.0000e+00 - val_loss: 2.5733e-04 - val_acc: 0.0000e+00
Epoch 281/300
13702/13702 [==============================] - 1s - loss: 5.1099e-05 - acc: 0.0000e+00 - val_loss: 1.9478e-04 - val_acc: 0.0000e+00
Epoch 282/300
13702/13702 [==============================] - 1s - loss: 5.6470e-05 - acc: 0.0000e+00 - val_loss: 3.0808e-04 - val_acc: 0.0000e+00
Epoch 283/300
13702/13702 [==============================] - 1s - loss: 5.4080e-05 - acc: 0.0000e+00 - val_loss: 1.0495e-04 - val_acc: 0.0000e+00
Epoch 284/300
13702/13702 [==============================] - 1s - loss: 4.9222e-05 - acc: 0.0000e+00 - val_loss: 8.4861e-05 - val_acc: 0.0000e+00
Epoch 285/300
13702/13702 [==============================] - 1s - loss: 4.8117e-05 - acc: 0.0000e+00 - val_loss: 9.6464e-05 - val_acc: 0.0000e+00
Epoch 286/300
13702/13702 [==============================] - 1s - loss: 5.8539e-05 - acc: 0.0000e+00 - val_loss: 9.0727e-05 - val_acc: 0.0000e+00
Epoch 287/300
13702/13702 [==============================] - 1s - loss: 4.9883e-05 - acc: 0.0000e+00 - val_loss: 8.1974e-05 - val_acc: 0.0000e+00
Epoch 288/300
13702/13702 [==============================] - 1s - loss: 4.5807e-05 - acc: 0.0000e+00 - val_loss: 1.4715e-04 - val_acc: 0.0000e+00
Epoch 289/300
13702/13702 [==============================] - 1s - loss: 4.4835e-05 - acc: 0.0000e+00 - val_loss: 9.2197e-05 - val_acc: 0.0000e+00
Epoch 290/300
13702/13702 [==============================] - 1s - loss: 4.5945e-05 - acc: 0.0000e+00 - val_loss: 9.4772e-05 - val_acc: 0.0000e+00
Epoch 291/300
13702/13702 [==============================] - 1s - loss: 4.9566e-05 - acc: 0.0000e+00 - val_loss: 1.9376e-04 - val_acc: 0.0000e+00
Epoch 292/300
13702/13702 [==============================] - 1s - loss: 5.0818e-05 - acc: 0.0000e+00 - val_loss: 1.0003e-04 - val_acc: 0.0000e+00
Epoch 293/300
13702/13702 [==============================] - 1s - loss: 4.9343e-05 - acc: 0.0000e+00 - val_loss: 9.5092e-05 - val_acc: 0.0000e+00
Epoch 294/300
13702/13702 [==============================] - 1s - loss: 4.9634e-05 - acc: 0.0000e+00 - val_loss: 1.1669e-04 - val_acc: 0.0000e+00
Epoch 295/300
13702/13702 [==============================] - 1s - loss: 4.4528e-05 - acc: 0.0000e+00 - val_loss: 1.3761e-04 - val_acc: 0.0000e+00
Epoch 296/300
13702/13702 [==============================] - 1s - loss: 4.6013e-05 - acc: 0.0000e+00 - val_loss: 1.0861e-04 - val_acc: 0.0000e+00
Epoch 297/300
13702/13702 [==============================] - 1s - loss: 4.6978e-05 - acc: 0.0000e+00 - val_loss: 2.0963e-04 - val_acc: 0.0000e+00
Epoch 298/300
13702/13702 [==============================] - 1s - loss: 4.6692e-05 - acc: 0.0000e+00 - val_loss: 2.4573e-04 - val_acc: 0.0000e+00
Epoch 299/300
13702/13702 [==============================] - 1s - loss: 6.7129e-05 - acc: 0.0000e+00 - val_loss: 2.3804e-04 - val_acc: 0.0000e+00
Epoch 300/300
13702/13702 [==============================] - 1s - loss: 4.9061e-05 - acc: 0.0000e+00 - val_loss: 9.7557e-05 - val_acc: 0.0000e+00
Train Score: 0.00003 MSE (0.01 RMSE)
Test Score: 0.01020 MSE (0.10 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_23 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_23 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_24 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_24 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_23 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_24 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/300
13702/13702 [==============================] - 3s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00
Epoch 2/300
13702/13702 [==============================] - 1s - loss: 8.7047e-04 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 3/300
13702/13702 [==============================] - 1s - loss: 5.2440e-04 - acc: 0.0000e+00 - val_loss: 6.6885e-04 - val_acc: 0.0000e+00
Epoch 4/300
13702/13702 [==============================] - 1s - loss: 4.1595e-04 - acc: 0.0000e+00 - val_loss: 4.3015e-04 - val_acc: 0.0000e+00
Epoch 5/300
13702/13702 [==============================] - 1s - loss: 3.9376e-04 - acc: 0.0000e+00 - val_loss: 4.0145e-04 - val_acc: 0.0000e+00
Epoch 6/300
13702/13702 [==============================] - 1s - loss: 3.6609e-04 - acc: 0.0000e+00 - val_loss: 4.6610e-04 - val_acc: 0.0000e+00
Epoch 7/300
13702/13702 [==============================] - 1s - loss: 3.2988e-04 - acc: 0.0000e+00 - val_loss: 3.1850e-04 - val_acc: 0.0000e+00
Epoch 8/300
13702/13702 [==============================] - 1s - loss: 3.4119e-04 - acc: 0.0000e+00 - val_loss: 3.8677e-04 - val_acc: 0.0000e+00
Epoch 9/300
13702/13702 [==============================] - 1s - loss: 3.1360e-04 - acc: 0.0000e+00 - val_loss: 3.4713e-04 - val_acc: 0.0000e+00
Epoch 10/300
13702/13702 [==============================] - 1s - loss: 3.2563e-04 - acc: 0.0000e+00 - val_loss: 6.7438e-04 - val_acc: 0.0000e+00
Epoch 11/300
13702/13702 [==============================] - 1s - loss: 3.4692e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 12/300
13702/13702 [==============================] - 1s - loss: 3.3828e-04 - acc: 0.0000e+00 - val_loss: 2.6580e-04 - val_acc: 0.0000e+00
Epoch 13/300
13702/13702 [==============================] - 1s - loss: 2.8596e-04 - acc: 0.0000e+00 - val_loss: 5.3410e-04 - val_acc: 0.0000e+00
Epoch 14/300
13702/13702 [==============================] - 1s - loss: 3.3137e-04 - acc: 0.0000e+00 - val_loss: 8.8481e-04 - val_acc: 0.0000e+00
Epoch 15/300
13702/13702 [==============================] - 1s - loss: 3.0767e-04 - acc: 0.0000e+00 - val_loss: 4.8390e-04 - val_acc: 0.0000e+00
Epoch 16/300
13702/13702 [==============================] - 1s - loss: 3.1022e-04 - acc: 0.0000e+00 - val_loss: 6.7400e-04 - val_acc: 0.0000e+00
Epoch 17/300
13702/13702 [==============================] - 1s - loss: 2.8798e-04 - acc: 0.0000e+00 - val_loss: 9.7486e-04 - val_acc: 0.0000e+00
Epoch 18/300
13702/13702 [==============================] - 1s - loss: 2.7987e-04 - acc: 0.0000e+00 - val_loss: 6.8535e-04 - val_acc: 0.0000e+00
Epoch 19/300
13702/13702 [==============================] - 1s - loss: 2.8777e-04 - acc: 0.0000e+00 - val_loss: 5.7020e-04 - val_acc: 0.0000e+00
Epoch 20/300
13702/13702 [==============================] - 1s - loss: 2.7076e-04 - acc: 0.0000e+00 - val_loss: 2.5843e-04 - val_acc: 0.0000e+00
Epoch 21/300
13702/13702 [==============================] - 1s - loss: 3.2128e-04 - acc: 0.0000e+00 - val_loss: 2.6402e-04 - val_acc: 0.0000e+00
Epoch 22/300
13702/13702 [==============================] - 1s - loss: 2.7462e-04 - acc: 0.0000e+00 - val_loss: 2.2810e-04 - val_acc: 0.0000e+00
Epoch 23/300
13702/13702 [==============================] - 1s - loss: 2.6210e-04 - acc: 0.0000e+00 - val_loss: 3.5313e-04 - val_acc: 0.0000e+00
Epoch 24/300
13702/13702 [==============================] - 1s - loss: 2.6585e-04 - acc: 0.0000e+00 - val_loss: 2.4631e-04 - val_acc: 0.0000e+00
Epoch 25/300
13702/13702 [==============================] - 1s - loss: 3.1488e-04 - acc: 0.0000e+00 - val_loss: 3.3157e-04 - val_acc: 0.0000e+00
Epoch 26/300
13702/13702 [==============================] - 1s - loss: 2.6708e-04 - acc: 0.0000e+00 - val_loss: 3.7416e-04 - val_acc: 0.0000e+00
Epoch 27/300
13702/13702 [==============================] - 1s - loss: 2.5684e-04 - acc: 0.0000e+00 - val_loss: 7.9296e-04 - val_acc: 0.0000e+00
Epoch 28/300
13702/13702 [==============================] - 1s - loss: 2.4138e-04 - acc: 0.0000e+00 - val_loss: 2.4181e-04 - val_acc: 0.0000e+00
Epoch 29/300
13702/13702 [==============================] - 1s - loss: 2.3191e-04 - acc: 0.0000e+00 - val_loss: 2.1845e-04 - val_acc: 0.0000e+00
Epoch 30/300
13702/13702 [==============================] - 1s - loss: 2.1511e-04 - acc: 0.0000e+00 - val_loss: 2.2446e-04 - val_acc: 0.0000e+00
Epoch 31/300
13702/13702 [==============================] - 1s - loss: 2.2186e-04 - acc: 0.0000e+00 - val_loss: 2.0910e-04 - val_acc: 0.0000e+00
Epoch 32/300
13702/13702 [==============================] - 1s - loss: 2.2725e-04 - acc: 0.0000e+00 - val_loss: 2.3561e-04 - val_acc: 0.0000e+00
Epoch 33/300
13702/13702 [==============================] - 1s - loss: 2.2940e-04 - acc: 0.0000e+00 - val_loss: 3.1238e-04 - val_acc: 0.0000e+00
Epoch 34/300
13702/13702 [==============================] - 1s - loss: 2.3394e-04 - acc: 0.0000e+00 - val_loss: 2.6932e-04 - val_acc: 0.0000e+00
Epoch 35/300
13702/13702 [==============================] - 1s - loss: 2.1165e-04 - acc: 0.0000e+00 - val_loss: 2.1621e-04 - val_acc: 0.0000e+00
Epoch 36/300
13702/13702 [==============================] - 1s - loss: 2.1584e-04 - acc: 0.0000e+00 - val_loss: 5.2774e-04 - val_acc: 0.0000e+00
Epoch 37/300
13702/13702 [==============================] - 1s - loss: 2.5276e-04 - acc: 0.0000e+00 - val_loss: 8.2725e-04 - val_acc: 0.0000e+00
Epoch 38/300
13702/13702 [==============================] - 1s - loss: 2.3478e-04 - acc: 0.0000e+00 - val_loss: 3.4009e-04 - val_acc: 0.0000e+00
Epoch 39/300
13702/13702 [==============================] - 1s - loss: 2.3294e-04 - acc: 0.0000e+00 - val_loss: 3.9240e-04 - val_acc: 0.0000e+00
Epoch 40/300
13702/13702 [==============================] - 1s - loss: 2.0828e-04 - acc: 0.0000e+00 - val_loss: 2.3799e-04 - val_acc: 0.0000e+00
Epoch 41/300
13702/13702 [==============================] - 1s - loss: 1.9982e-04 - acc: 0.0000e+00 - val_loss: 2.0431e-04 - val_acc: 0.0000e+00
Epoch 42/300
13702/13702 [==============================] - 1s - loss: 1.9265e-04 - acc: 0.0000e+00 - val_loss: 4.9360e-04 - val_acc: 0.0000e+00
Epoch 43/300
13702/13702 [==============================] - 1s - loss: 2.0135e-04 - acc: 0.0000e+00 - val_loss: 4.0689e-04 - val_acc: 0.0000e+00
Epoch 44/300
13702/13702 [==============================] - 1s - loss: 1.9627e-04 - acc: 0.0000e+00 - val_loss: 1.8716e-04 - val_acc: 0.0000e+00
Epoch 45/300
13702/13702 [==============================] - 1s - loss: 1.7764e-04 - acc: 0.0000e+00 - val_loss: 1.8269e-04 - val_acc: 0.0000e+00
Epoch 46/300
13702/13702 [==============================] - 1s - loss: 1.7685e-04 - acc: 0.0000e+00 - val_loss: 2.4523e-04 - val_acc: 0.0000e+00
Epoch 47/300
13702/13702 [==============================] - 1s - loss: 1.8795e-04 - acc: 0.0000e+00 - val_loss: 1.9029e-04 - val_acc: 0.0000e+00
Epoch 48/300
13702/13702 [==============================] - 1s - loss: 1.6938e-04 - acc: 0.0000e+00 - val_loss: 2.9745e-04 - val_acc: 0.0000e+00
Epoch 49/300
13702/13702 [==============================] - 1s - loss: 1.7403e-04 - acc: 0.0000e+00 - val_loss: 3.7796e-04 - val_acc: 0.0000e+00
Epoch 50/300
13702/13702 [==============================] - 1s - loss: 2.1103e-04 - acc: 0.0000e+00 - val_loss: 1.9371e-04 - val_acc: 0.0000e+00
Epoch 51/300
13702/13702 [==============================] - 1s - loss: 1.8192e-04 - acc: 0.0000e+00 - val_loss: 3.0683e-04 - val_acc: 0.0000e+00
Epoch 52/300
13702/13702 [==============================] - 1s - loss: 1.5982e-04 - acc: 0.0000e+00 - val_loss: 2.6178e-04 - val_acc: 0.0000e+00
Epoch 53/300
13702/13702 [==============================] - 1s - loss: 1.5126e-04 - acc: 0.0000e+00 - val_loss: 1.9760e-04 - val_acc: 0.0000e+00
Epoch 54/300
13702/13702 [==============================] - 1s - loss: 1.5014e-04 - acc: 0.0000e+00 - val_loss: 1.8921e-04 - val_acc: 0.0000e+00
Epoch 55/300
13702/13702 [==============================] - 1s - loss: 1.4582e-04 - acc: 0.0000e+00 - val_loss: 1.8051e-04 - val_acc: 0.0000e+00
Epoch 56/300
13702/13702 [==============================] - 1s - loss: 1.5007e-04 - acc: 0.0000e+00 - val_loss: 2.3593e-04 - val_acc: 0.0000e+00
Epoch 57/300
13702/13702 [==============================] - 1s - loss: 1.5949e-04 - acc: 0.0000e+00 - val_loss: 1.8523e-04 - val_acc: 0.0000e+0000e
Epoch 58/300
13702/13702 [==============================] - 1s - loss: 1.4790e-04 - acc: 0.0000e+00 - val_loss: 1.7967e-04 - val_acc: 0.0000e+00
Epoch 59/300
13702/13702 [==============================] - 1s - loss: 1.4080e-04 - acc: 0.0000e+00 - val_loss: 1.9178e-04 - val_acc: 0.0000e+00
Epoch 60/300
13702/13702 [==============================] - 1s - loss: 1.4572e-04 - acc: 0.0000e+00 - val_loss: 3.2454e-04 - val_acc: 0.0000e+00
Epoch 61/300
13702/13702 [==============================] - 1s - loss: 1.4715e-04 - acc: 0.0000e+00 - val_loss: 2.3090e-04 - val_acc: 0.0000e+00
Epoch 62/300
13702/13702 [==============================] - 1s - loss: 1.4608e-04 - acc: 0.0000e+00 - val_loss: 6.2801e-04 - val_acc: 0.0000e+00
Epoch 63/300
13702/13702 [==============================] - 1s - loss: 1.4033e-04 - acc: 0.0000e+00 - val_loss: 1.8970e-04 - val_acc: 0.0000e+00
Epoch 64/300
13702/13702 [==============================] - 1s - loss: 1.3022e-04 - acc: 0.0000e+00 - val_loss: 2.6361e-04 - val_acc: 0.0000e+00
Epoch 65/300
13702/13702 [==============================] - 1s - loss: 1.3015e-04 - acc: 0.0000e+00 - val_loss: 2.3959e-04 - val_acc: 0.0000e+00
Epoch 66/300
13702/13702 [==============================] - 1s - loss: 1.4517e-04 - acc: 0.0000e+00 - val_loss: 4.3565e-04 - val_acc: 0.0000e+00
Epoch 67/300
13702/13702 [==============================] - 1s - loss: 1.4071e-04 - acc: 0.0000e+00 - val_loss: 1.8595e-04 - val_acc: 0.0000e+00
Epoch 68/300
13702/13702 [==============================] - 1s - loss: 1.3564e-04 - acc: 0.0000e+00 - val_loss: 1.8283e-04 - val_acc: 0.0000e+00
Epoch 69/300
13702/13702 [==============================] - 1s - loss: 1.3175e-04 - acc: 0.0000e+00 - val_loss: 1.8106e-04 - val_acc: 0.0000e+00
Epoch 70/300
13702/13702 [==============================] - 1s - loss: 1.3212e-04 - acc: 0.0000e+00 - val_loss: 7.9079e-04 - val_acc: 0.0000e+00
Epoch 71/300
13702/13702 [==============================] - 1s - loss: 1.4142e-04 - acc: 0.0000e+00 - val_loss: 1.8438e-04 - val_acc: 0.0000e+00
Epoch 72/300
13702/13702 [==============================] - 1s - loss: 1.2241e-04 - acc: 0.0000e+00 - val_loss: 1.9637e-04 - val_acc: 0.0000e+00
Epoch 73/300
13702/13702 [==============================] - 1s - loss: 1.2134e-04 - acc: 0.0000e+00 - val_loss: 1.9795e-04 - val_acc: 0.0000e+00
Epoch 74/300
13702/13702 [==============================] - 1s - loss: 1.2303e-04 - acc: 0.0000e+00 - val_loss: 1.8200e-04 - val_acc: 0.0000e+00
Epoch 75/300
13702/13702 [==============================] - 1s - loss: 1.1591e-04 - acc: 0.0000e+00 - val_loss: 1.8617e-04 - val_acc: 0.0000e+00
Epoch 76/300
13702/13702 [==============================] - 1s - loss: 1.2509e-04 - acc: 0.0000e+00 - val_loss: 1.7907e-04 - val_acc: 0.0000e+00
Epoch 77/300
13702/13702 [==============================] - 1s - loss: 1.1656e-04 - acc: 0.0000e+00 - val_loss: 4.6717e-04 - val_acc: 0.0000e+00
Epoch 78/300
13702/13702 [==============================] - 1s - loss: 1.1367e-04 - acc: 0.0000e+00 - val_loss: 1.9309e-04 - val_acc: 0.0000e+00
Epoch 79/300
13702/13702 [==============================] - 1s - loss: 1.1463e-04 - acc: 0.0000e+00 - val_loss: 3.0223e-04 - val_acc: 0.0000e+00
Epoch 80/300
13702/13702 [==============================] - 1s - loss: 1.4130e-04 - acc: 0.0000e+00 - val_loss: 4.0616e-04 - val_acc: 0.0000e+00
Epoch 81/300
13702/13702 [==============================] - 1s - loss: 1.2613e-04 - acc: 0.0000e+00 - val_loss: 1.9107e-04 - val_acc: 0.0000e+00
Epoch 82/300
13702/13702 [==============================] - 1s - loss: 1.0481e-04 - acc: 0.0000e+00 - val_loss: 2.7407e-04 - val_acc: 0.0000e+00
Epoch 83/300
13702/13702 [==============================] - 1s - loss: 1.1483e-04 - acc: 0.0000e+00 - val_loss: 1.7773e-04 - val_acc: 0.0000e+00
Epoch 84/300
13702/13702 [==============================] - 1s - loss: 1.0660e-04 - acc: 0.0000e+00 - val_loss: 1.7335e-04 - val_acc: 0.0000e+00
Epoch 85/300
13702/13702 [==============================] - 1s - loss: 1.0524e-04 - acc: 0.0000e+00 - val_loss: 2.8505e-04 - val_acc: 0.0000e+00
Epoch 86/300
13702/13702 [==============================] - 1s - loss: 1.2553e-04 - acc: 0.0000e+00 - val_loss: 3.5930e-04 - val_acc: 0.0000e+00
Epoch 87/300
13702/13702 [==============================] - 1s - loss: 1.1682e-04 - acc: 0.0000e+00 - val_loss: 2.3035e-04 - val_acc: 0.0000e+00
Epoch 88/300
13702/13702 [==============================] - 1s - loss: 1.0375e-04 - acc: 0.0000e+00 - val_loss: 2.3473e-04 - val_acc: 0.0000e+00
Epoch 89/300
13702/13702 [==============================] - 1s - loss: 1.1400e-04 - acc: 0.0000e+00 - val_loss: 3.7711e-04 - val_acc: 0.0000e+00
Epoch 90/300
13702/13702 [==============================] - 1s - loss: 1.1315e-04 - acc: 0.0000e+00 - val_loss: 4.4780e-04 - val_acc: 0.0000e+00
Epoch 91/300
13702/13702 [==============================] - 1s - loss: 1.0942e-04 - acc: 0.0000e+00 - val_loss: 1.8529e-04 - val_acc: 0.0000e+00
Epoch 92/300
13702/13702 [==============================] - 1s - loss: 1.1823e-04 - acc: 0.0000e+00 - val_loss: 2.4751e-04 - val_acc: 0.0000e+00
Epoch 93/300
13702/13702 [==============================] - 1s - loss: 1.0368e-04 - acc: 0.0000e+00 - val_loss: 1.8130e-04 - val_acc: 0.0000e+00
Epoch 94/300
13702/13702 [==============================] - 1s - loss: 1.1755e-04 - acc: 0.0000e+00 - val_loss: 3.8385e-04 - val_acc: 0.0000e+00
Epoch 95/300
13702/13702 [==============================] - 1s - loss: 1.1459e-04 - acc: 0.0000e+00 - val_loss: 1.7053e-04 - val_acc: 0.0000e+00
Epoch 96/300
13702/13702 [==============================] - 1s - loss: 1.1151e-04 - acc: 0.0000e+00 - val_loss: 1.6789e-04 - val_acc: 0.0000e+00
Epoch 97/300
13702/13702 [==============================] - 1s - loss: 1.0477e-04 - acc: 0.0000e+00 - val_loss: 2.1350e-04 - val_acc: 0.0000e+00
Epoch 98/300
13702/13702 [==============================] - 1s - loss: 9.4519e-05 - acc: 0.0000e+00 - val_loss: 2.1274e-04 - val_acc: 0.0000e+00
Epoch 99/300
13702/13702 [==============================] - 1s - loss: 9.8510e-05 - acc: 0.0000e+00 - val_loss: 2.3352e-04 - val_acc: 0.0000e+00
Epoch 100/300
13702/13702 [==============================] - 1s - loss: 1.0034e-04 - acc: 0.0000e+00 - val_loss: 2.5685e-04 - val_acc: 0.0000e+00
Epoch 101/300
13702/13702 [==============================] - 1s - loss: 9.3274e-05 - acc: 0.0000e+00 - val_loss: 2.0058e-04 - val_acc: 0.0000e+00
Epoch 102/300
13702/13702 [==============================] - 1s - loss: 9.9318e-05 - acc: 0.0000e+00 - val_loss: 1.5693e-04 - val_acc: 0.0000e+00
Epoch 103/300
13702/13702 [==============================] - 1s - loss: 9.7682e-05 - acc: 0.0000e+00 - val_loss: 1.6589e-04 - val_acc: 0.0000e+00
Epoch 104/300
13702/13702 [==============================] - 1s - loss: 9.4833e-05 - acc: 0.0000e+00 - val_loss: 1.7036e-04 - val_acc: 0.0000e+00
Epoch 105/300
13702/13702 [==============================] - 1s - loss: 9.0899e-05 - acc: 0.0000e+00 - val_loss: 1.6080e-04 - val_acc: 0.0000e+00
Epoch 106/300
13702/13702 [==============================] - 1s - loss: 9.7552e-05 - acc: 0.0000e+00 - val_loss: 1.7142e-04 - val_acc: 0.0000e+00
Epoch 107/300
13702/13702 [==============================] - 1s - loss: 9.7904e-05 - acc: 0.0000e+00 - val_loss: 2.0942e-04 - val_acc: 0.0000e+00
Epoch 108/300
13702/13702 [==============================] - 1s - loss: 9.9562e-05 - acc: 0.0000e+00 - val_loss: 1.8925e-04 - val_acc: 0.0000e+00
Epoch 109/300
13702/13702 [==============================] - 1s - loss: 9.3547e-05 - acc: 0.0000e+00 - val_loss: 1.7102e-04 - val_acc: 0.0000e+00
Epoch 110/300
13702/13702 [==============================] - 1s - loss: 9.3139e-05 - acc: 0.0000e+00 - val_loss: 1.5895e-04 - val_acc: 0.0000e+00
Epoch 111/300
13702/13702 [==============================] - 1s - loss: 9.2996e-05 - acc: 0.0000e+00 - val_loss: 2.3519e-04 - val_acc: 0.0000e+00
Epoch 112/300
13702/13702 [==============================] - 1s - loss: 9.4501e-05 - acc: 0.0000e+00 - val_loss: 2.2187e-04 - val_acc: 0.0000e+00
Epoch 113/300
13702/13702 [==============================] - 1s - loss: 8.8456e-05 - acc: 0.0000e+00 - val_loss: 1.5875e-04 - val_acc: 0.0000e+00
Epoch 114/300
13702/13702 [==============================] - 1s - loss: 8.3571e-05 - acc: 0.0000e+00 - val_loss: 3.6666e-04 - val_acc: 0.0000e+00
Epoch 115/300
13702/13702 [==============================] - 1s - loss: 1.1100e-04 - acc: 0.0000e+00 - val_loss: 2.5856e-04 - val_acc: 0.0000e+00
Epoch 116/300
13702/13702 [==============================] - 1s - loss: 1.0639e-04 - acc: 0.0000e+00 - val_loss: 1.8973e-04 - val_acc: 0.0000e+00
Epoch 117/300
13702/13702 [==============================] - 1s - loss: 8.6261e-05 - acc: 0.0000e+00 - val_loss: 1.4700e-04 - val_acc: 0.0000e+00
Epoch 118/300
13702/13702 [==============================] - 1s - loss: 9.7523e-05 - acc: 0.0000e+00 - val_loss: 1.7699e-04 - val_acc: 0.0000e+00
Epoch 119/300
13702/13702 [==============================] - 1s - loss: 9.3360e-05 - acc: 0.0000e+00 - val_loss: 2.2095e-04 - val_acc: 0.0000e+00
Epoch 120/300
13702/13702 [==============================] - 1s - loss: 9.2376e-05 - acc: 0.0000e+00 - val_loss: 1.4597e-04 - val_acc: 0.0000e+00
Epoch 121/300
13702/13702 [==============================] - 1s - loss: 1.0325e-04 - acc: 0.0000e+00 - val_loss: 1.6012e-04 - val_acc: 0.0000e+00
Epoch 122/300
13702/13702 [==============================] - 1s - loss: 8.7587e-05 - acc: 0.0000e+00 - val_loss: 2.6337e-04 - val_acc: 0.0000e+00
Epoch 123/300
13702/13702 [==============================] - 1s - loss: 8.5263e-05 - acc: 0.0000e+00 - val_loss: 1.4663e-04 - val_acc: 0.0000e+00
Epoch 124/300
13702/13702 [==============================] - 1s - loss: 8.7693e-05 - acc: 0.0000e+00 - val_loss: 1.6658e-04 - val_acc: 0.0000e+00
Epoch 125/300
13702/13702 [==============================] - 1s - loss: 9.3065e-05 - acc: 0.0000e+00 - val_loss: 1.5384e-04 - val_acc: 0.0000e+00
Epoch 126/300
13702/13702 [==============================] - 1s - loss: 9.3290e-05 - acc: 0.0000e+00 - val_loss: 2.8128e-04 - val_acc: 0.0000e+00
Epoch 127/300
13702/13702 [==============================] - 1s - loss: 8.8766e-05 - acc: 0.0000e+00 - val_loss: 1.5554e-04 - val_acc: 0.0000e+00
Epoch 128/300
13702/13702 [==============================] - 1s - loss: 8.4625e-05 - acc: 0.0000e+00 - val_loss: 1.4093e-04 - val_acc: 0.0000e+00
Epoch 129/300
13702/13702 [==============================] - 1s - loss: 7.9797e-05 - acc: 0.0000e+00 - val_loss: 1.6108e-04 - val_acc: 0.0000e+00
Epoch 130/300
13702/13702 [==============================] - 1s - loss: 7.7305e-05 - acc: 0.0000e+00 - val_loss: 1.5108e-04 - val_acc: 0.0000e+00
Epoch 131/300
13702/13702 [==============================] - 1s - loss: 8.4592e-05 - acc: 0.0000e+00 - val_loss: 2.5204e-04 - val_acc: 0.0000e+00
Epoch 132/300
13702/13702 [==============================] - 1s - loss: 7.9475e-05 - acc: 0.0000e+00 - val_loss: 2.6891e-04 - val_acc: 0.0000e+00
Epoch 133/300
13702/13702 [==============================] - 1s - loss: 8.8622e-05 - acc: 0.0000e+00 - val_loss: 1.7379e-04 - val_acc: 0.0000e+00
Epoch 134/300
13702/13702 [==============================] - 1s - loss: 8.6863e-05 - acc: 0.0000e+00 - val_loss: 1.4904e-04 - val_acc: 0.0000e+00
Epoch 135/300
13702/13702 [==============================] - 1s - loss: 8.7840e-05 - acc: 0.0000e+00 - val_loss: 1.4870e-04 - val_acc: 0.0000e+00
Epoch 136/300
13702/13702 [==============================] - 1s - loss: 9.1085e-05 - acc: 0.0000e+00 - val_loss: 1.6426e-04 - val_acc: 0.0000e+00
Epoch 137/300
13702/13702 [==============================] - 1s - loss: 9.3865e-05 - acc: 0.0000e+00 - val_loss: 1.3573e-04 - val_acc: 0.0000e+00
Epoch 138/300
13702/13702 [==============================] - 1s - loss: 8.4224e-05 - acc: 0.0000e+00 - val_loss: 1.4545e-04 - val_acc: 0.0000e+00
Epoch 139/300
13702/13702 [==============================] - 1s - loss: 8.3954e-05 - acc: 0.0000e+00 - val_loss: 2.2154e-04 - val_acc: 0.0000e+00
Epoch 140/300
13702/13702 [==============================] - 1s - loss: 8.2639e-05 - acc: 0.0000e+00 - val_loss: 1.6129e-04 - val_acc: 0.0000e+00
Epoch 141/300
13702/13702 [==============================] - 1s - loss: 7.7703e-05 - acc: 0.0000e+00 - val_loss: 2.3872e-04 - val_acc: 0.0000e+00
Epoch 142/300
13702/13702 [==============================] - 1s - loss: 7.5087e-05 - acc: 0.0000e+00 - val_loss: 1.7779e-04 - val_acc: 0.0000e+00
Epoch 143/300
13702/13702 [==============================] - 1s - loss: 8.3217e-05 - acc: 0.0000e+00 - val_loss: 3.8392e-04 - val_acc: 0.0000e+00
Epoch 144/300
13702/13702 [==============================] - 1s - loss: 8.5180e-05 - acc: 0.0000e+00 - val_loss: 1.4202e-04 - val_acc: 0.0000e+00- ETA: 0s - loss: 9.2897e-05 - 
Epoch 145/300
13702/13702 [==============================] - 1s - loss: 8.1882e-05 - acc: 0.0000e+00 - val_loss: 4.0928e-04 - val_acc: 0.0000e+00
Epoch 146/300
13702/13702 [==============================] - 1s - loss: 8.8688e-05 - acc: 0.0000e+00 - val_loss: 1.6493e-04 - val_acc: 0.0000e+00
Epoch 147/300
13702/13702 [==============================] - 1s - loss: 8.4141e-05 - acc: 0.0000e+00 - val_loss: 1.4611e-04 - val_acc: 0.0000e+00
Epoch 148/300
13702/13702 [==============================] - 1s - loss: 8.3430e-05 - acc: 0.0000e+00 - val_loss: 1.4648e-04 - val_acc: 0.0000e+00
Epoch 149/300
13702/13702 [==============================] - 1s - loss: 7.7399e-05 - acc: 0.0000e+00 - val_loss: 2.6628e-04 - val_acc: 0.0000e+00
Epoch 150/300
13702/13702 [==============================] - 1s - loss: 9.9887e-05 - acc: 0.0000e+00 - val_loss: 3.6011e-04 - val_acc: 0.0000e+00
Epoch 151/300
13702/13702 [==============================] - 1s - loss: 7.9753e-05 - acc: 0.0000e+00 - val_loss: 1.6192e-04 - val_acc: 0.0000e+0000e+
Epoch 152/300
13702/13702 [==============================] - 1s - loss: 9.1634e-05 - acc: 0.0000e+00 - val_loss: 2.2466e-04 - val_acc: 0.0000e+00
Epoch 153/300
13702/13702 [==============================] - 1s - loss: 8.4826e-05 - acc: 0.0000e+00 - val_loss: 2.2539e-04 - val_acc: 0.0000e+00
Epoch 154/300
13702/13702 [==============================] - 1s - loss: 7.6112e-05 - acc: 0.0000e+00 - val_loss: 2.3590e-04 - val_acc: 0.0000e+00
Epoch 155/300
13702/13702 [==============================] - 1s - loss: 8.1982e-05 - acc: 0.0000e+00 - val_loss: 3.2697e-04 - val_acc: 0.0000e+00
Epoch 156/300
13702/13702 [==============================] - 1s - loss: 7.7359e-05 - acc: 0.0000e+00 - val_loss: 2.6198e-04 - val_acc: 0.0000e+00
Epoch 157/300
13702/13702 [==============================] - 1s - loss: 7.8296e-05 - acc: 0.0000e+00 - val_loss: 1.4091e-04 - val_acc: 0.0000e+00
Epoch 158/300
13702/13702 [==============================] - 1s - loss: 8.6299e-05 - acc: 0.0000e+00 - val_loss: 1.3684e-04 - val_acc: 0.0000e+00
Epoch 159/300
13702/13702 [==============================] - 1s - loss: 8.1581e-05 - acc: 0.0000e+00 - val_loss: 1.9708e-04 - val_acc: 0.0000e+00
Epoch 160/300
13702/13702 [==============================] - 1s - loss: 8.3794e-05 - acc: 0.0000e+00 - val_loss: 1.6217e-04 - val_acc: 0.0000e+00
Epoch 161/300
13702/13702 [==============================] - 1s - loss: 7.8962e-05 - acc: 0.0000e+00 - val_loss: 2.3966e-04 - val_acc: 0.0000e+00
Epoch 162/300
13702/13702 [==============================] - 1s - loss: 7.5534e-05 - acc: 0.0000e+00 - val_loss: 2.1543e-04 - val_acc: 0.0000e+00
Epoch 163/300
13702/13702 [==============================] - 1s - loss: 7.7931e-05 - acc: 0.0000e+00 - val_loss: 1.5492e-04 - val_acc: 0.0000e+00
Epoch 164/300
13702/13702 [==============================] - 1s - loss: 7.2376e-05 - acc: 0.0000e+00 - val_loss: 2.6767e-04 - val_acc: 0.0000e+00
Epoch 165/300
13702/13702 [==============================] - 1s - loss: 8.1835e-05 - acc: 0.0000e+00 - val_loss: 2.7275e-04 - val_acc: 0.0000e+00
Epoch 166/300
13702/13702 [==============================] - 1s - loss: 8.3535e-05 - acc: 0.0000e+00 - val_loss: 1.8911e-04 - val_acc: 0.0000e+00
Epoch 167/300
13702/13702 [==============================] - 1s - loss: 7.6043e-05 - acc: 0.0000e+00 - val_loss: 1.3252e-04 - val_acc: 0.0000e+00
Epoch 168/300
13702/13702 [==============================] - 1s - loss: 7.2925e-05 - acc: 0.0000e+00 - val_loss: 2.0640e-04 - val_acc: 0.0000e+00
Epoch 169/300
13702/13702 [==============================] - 1s - loss: 7.5092e-05 - acc: 0.0000e+00 - val_loss: 1.5972e-04 - val_acc: 0.0000e+00
Epoch 170/300
13702/13702 [==============================] - 1s - loss: 7.6507e-05 - acc: 0.0000e+00 - val_loss: 1.5043e-04 - val_acc: 0.0000e+00
Epoch 171/300
13702/13702 [==============================] - 1s - loss: 7.7465e-05 - acc: 0.0000e+00 - val_loss: 1.6337e-04 - val_acc: 0.0000e+00
Epoch 172/300
13702/13702 [==============================] - 1s - loss: 7.3987e-05 - acc: 0.0000e+00 - val_loss: 1.7387e-04 - val_acc: 0.0000e+00
Epoch 173/300
13702/13702 [==============================] - 1s - loss: 7.6153e-05 - acc: 0.0000e+00 - val_loss: 3.8340e-04 - val_acc: 0.0000e+00
Epoch 174/300
13702/13702 [==============================] - 1s - loss: 7.8529e-05 - acc: 0.0000e+00 - val_loss: 1.9490e-04 - val_acc: 0.0000e+00
Epoch 175/300
13702/13702 [==============================] - 1s - loss: 7.1338e-05 - acc: 0.0000e+00 - val_loss: 1.6970e-04 - val_acc: 0.0000e+00
Epoch 176/300
13702/13702 [==============================] - 1s - loss: 8.3363e-05 - acc: 0.0000e+00 - val_loss: 3.0670e-04 - val_acc: 0.0000e+00
Epoch 177/300
13702/13702 [==============================] - 1s - loss: 8.4150e-05 - acc: 0.0000e+00 - val_loss: 1.5438e-04 - val_acc: 0.0000e+00
Epoch 178/300
13702/13702 [==============================] - 1s - loss: 8.1480e-05 - acc: 0.0000e+00 - val_loss: 1.5380e-04 - val_acc: 0.0000e+00
Epoch 179/300
13702/13702 [==============================] - 1s - loss: 8.2509e-05 - acc: 0.0000e+00 - val_loss: 2.7066e-04 - val_acc: 0.0000e+00
Epoch 180/300
13702/13702 [==============================] - 1s - loss: 7.5510e-05 - acc: 0.0000e+00 - val_loss: 1.3650e-04 - val_acc: 0.0000e+00
Epoch 181/300
13702/13702 [==============================] - 1s - loss: 7.4630e-05 - acc: 0.0000e+00 - val_loss: 1.3353e-04 - val_acc: 0.0000e+00
Epoch 182/300
13702/13702 [==============================] - 1s - loss: 7.5565e-05 - acc: 0.0000e+00 - val_loss: 1.4595e-04 - val_acc: 0.0000e+00
Epoch 183/300
13702/13702 [==============================] - 1s - loss: 7.5393e-05 - acc: 0.0000e+00 - val_loss: 1.5411e-04 - val_acc: 0.0000e+00
Epoch 184/300
13702/13702 [==============================] - 1s - loss: 7.2473e-05 - acc: 0.0000e+00 - val_loss: 2.4780e-04 - val_acc: 0.0000e+00
Epoch 185/300
13702/13702 [==============================] - 1s - loss: 7.0011e-05 - acc: 0.0000e+00 - val_loss: 1.3751e-04 - val_acc: 0.0000e+00
Epoch 186/300
13702/13702 [==============================] - 1s - loss: 7.2437e-05 - acc: 0.0000e+00 - val_loss: 1.4915e-04 - val_acc: 0.0000e+00
Epoch 187/300
13702/13702 [==============================] - 1s - loss: 7.0253e-05 - acc: 0.0000e+00 - val_loss: 1.3225e-04 - val_acc: 0.0000e+00
Epoch 188/300
13702/13702 [==============================] - 1s - loss: 7.2093e-05 - acc: 0.0000e+00 - val_loss: 1.9258e-04 - val_acc: 0.0000e+00
Epoch 189/300
13702/13702 [==============================] - 1s - loss: 7.4188e-05 - acc: 0.0000e+00 - val_loss: 1.4524e-04 - val_acc: 0.0000e+00
Epoch 190/300
13702/13702 [==============================] - 1s - loss: 7.2855e-05 - acc: 0.0000e+00 - val_loss: 2.9472e-04 - val_acc: 0.0000e+00
Epoch 191/300
13702/13702 [==============================] - 1s - loss: 7.2461e-05 - acc: 0.0000e+00 - val_loss: 1.4657e-04 - val_acc: 0.0000e+00
Epoch 192/300
13702/13702 [==============================] - 1s - loss: 6.6744e-05 - acc: 0.0000e+00 - val_loss: 1.6275e-04 - val_acc: 0.0000e+00
Epoch 193/300
13702/13702 [==============================] - 1s - loss: 6.5481e-05 - acc: 0.0000e+00 - val_loss: 1.3530e-04 - val_acc: 0.0000e+00
Epoch 194/300
13702/13702 [==============================] - 1s - loss: 6.8632e-05 - acc: 0.0000e+00 - val_loss: 1.2967e-04 - val_acc: 0.0000e+00
Epoch 195/300
13702/13702 [==============================] - 1s - loss: 7.9056e-05 - acc: 0.0000e+00 - val_loss: 3.0873e-04 - val_acc: 0.0000e+00
Epoch 196/300
13702/13702 [==============================] - 1s - loss: 7.2150e-05 - acc: 0.0000e+00 - val_loss: 1.6989e-04 - val_acc: 0.0000e+00
Epoch 197/300
13702/13702 [==============================] - 1s - loss: 7.2870e-05 - acc: 0.0000e+00 - val_loss: 1.4103e-04 - val_acc: 0.0000e+00
Epoch 198/300
13702/13702 [==============================] - 1s - loss: 6.8973e-05 - acc: 0.0000e+00 - val_loss: 2.6675e-04 - val_acc: 0.0000e+00
Epoch 199/300
13702/13702 [==============================] - 1s - loss: 7.4724e-05 - acc: 0.0000e+00 - val_loss: 2.9920e-04 - val_acc: 0.0000e+00
Epoch 200/300
13702/13702 [==============================] - 1s - loss: 6.7994e-05 - acc: 0.0000e+00 - val_loss: 1.4166e-04 - val_acc: 0.0000e+00
Epoch 201/300
13702/13702 [==============================] - 1s - loss: 7.2931e-05 - acc: 0.0000e+00 - val_loss: 1.3818e-04 - val_acc: 0.0000e+00
Epoch 202/300
13702/13702 [==============================] - 1s - loss: 7.0781e-05 - acc: 0.0000e+00 - val_loss: 1.9384e-04 - val_acc: 0.0000e+00
Epoch 203/300
13702/13702 [==============================] - 1s - loss: 6.8066e-05 - acc: 0.0000e+00 - val_loss: 2.6163e-04 - val_acc: 0.0000e+00
Epoch 204/300
13702/13702 [==============================] - 1s - loss: 7.2765e-05 - acc: 0.0000e+00 - val_loss: 1.4740e-04 - val_acc: 0.0000e+00
Epoch 205/300
13702/13702 [==============================] - 1s - loss: 7.9020e-05 - acc: 0.0000e+00 - val_loss: 2.2461e-04 - val_acc: 0.0000e+00
Epoch 206/300
13702/13702 [==============================] - 1s - loss: 7.4533e-05 - acc: 0.0000e+00 - val_loss: 1.2598e-04 - val_acc: 0.0000e+00
Epoch 207/300
13702/13702 [==============================] - 1s - loss: 6.4241e-05 - acc: 0.0000e+00 - val_loss: 1.3926e-04 - val_acc: 0.0000e+00
Epoch 208/300
13702/13702 [==============================] - 1s - loss: 6.8445e-05 - acc: 0.0000e+00 - val_loss: 1.3943e-04 - val_acc: 0.0000e+00
Epoch 209/300
13702/13702 [==============================] - 1s - loss: 7.0971e-05 - acc: 0.0000e+00 - val_loss: 1.3019e-04 - val_acc: 0.0000e+00
Epoch 210/300
13702/13702 [==============================] - 1s - loss: 6.7642e-05 - acc: 0.0000e+00 - val_loss: 1.2290e-04 - val_acc: 0.0000e+00
Epoch 211/300
13702/13702 [==============================] - 1s - loss: 6.7466e-05 - acc: 0.0000e+00 - val_loss: 1.2665e-04 - val_acc: 0.0000e+00
Epoch 212/300
13702/13702 [==============================] - 1s - loss: 7.2971e-05 - acc: 0.0000e+00 - val_loss: 1.8148e-04 - val_acc: 0.0000e+00
Epoch 213/300
13702/13702 [==============================] - 1s - loss: 6.7039e-05 - acc: 0.0000e+00 - val_loss: 1.7107e-04 - val_acc: 0.0000e+00
Epoch 214/300
13702/13702 [==============================] - 1s - loss: 7.3798e-05 - acc: 0.0000e+00 - val_loss: 1.8323e-04 - val_acc: 0.0000e+00
Epoch 215/300
13702/13702 [==============================] - 1s - loss: 7.8293e-05 - acc: 0.0000e+00 - val_loss: 1.3636e-04 - val_acc: 0.0000e+00
Epoch 216/300
13702/13702 [==============================] - 1s - loss: 7.3974e-05 - acc: 0.0000e+00 - val_loss: 3.7898e-04 - val_acc: 0.0000e+00
Epoch 217/300
13702/13702 [==============================] - 1s - loss: 8.0591e-05 - acc: 0.0000e+00 - val_loss: 2.3778e-04 - val_acc: 0.0000e+00
Epoch 218/300
13702/13702 [==============================] - 1s - loss: 6.8926e-05 - acc: 0.0000e+00 - val_loss: 1.1722e-04 - val_acc: 0.0000e+00
Epoch 219/300
13702/13702 [==============================] - 1s - loss: 6.9471e-05 - acc: 0.0000e+00 - val_loss: 2.1587e-04 - val_acc: 0.0000e+00
Epoch 220/300
13702/13702 [==============================] - 1s - loss: 6.7361e-05 - acc: 0.0000e+00 - val_loss: 1.6339e-04 - val_acc: 0.0000e+00
Epoch 221/300
13702/13702 [==============================] - 1s - loss: 7.6394e-05 - acc: 0.0000e+00 - val_loss: 2.5797e-04 - val_acc: 0.0000e+00
Epoch 222/300
13702/13702 [==============================] - 1s - loss: 7.1036e-05 - acc: 0.0000e+00 - val_loss: 1.2630e-04 - val_acc: 0.0000e+00
Epoch 223/300
13702/13702 [==============================] - 1s - loss: 6.4041e-05 - acc: 0.0000e+00 - val_loss: 1.7537e-04 - val_acc: 0.0000e+00
Epoch 224/300
13702/13702 [==============================] - 1s - loss: 6.5730e-05 - acc: 0.0000e+00 - val_loss: 1.6541e-04 - val_acc: 0.0000e+00
Epoch 225/300
13702/13702 [==============================] - 1s - loss: 6.7732e-05 - acc: 0.0000e+00 - val_loss: 1.4778e-04 - val_acc: 0.0000e+00
Epoch 226/300
13702/13702 [==============================] - 1s - loss: 6.5896e-05 - acc: 0.0000e+00 - val_loss: 1.8634e-04 - val_acc: 0.0000e+00
Epoch 227/300
13702/13702 [==============================] - 1s - loss: 8.0838e-05 - acc: 0.0000e+00 - val_loss: 1.3258e-04 - val_acc: 0.0000e+00
Epoch 228/300
13702/13702 [==============================] - 1s - loss: 6.5321e-05 - acc: 0.0000e+00 - val_loss: 2.6912e-04 - val_acc: 0.0000e+00
Epoch 229/300
13702/13702 [==============================] - 1s - loss: 7.2472e-05 - acc: 0.0000e+00 - val_loss: 1.4378e-04 - val_acc: 0.0000e+00
Epoch 230/300
13702/13702 [==============================] - 1s - loss: 6.8706e-05 - acc: 0.0000e+00 - val_loss: 1.5918e-04 - val_acc: 0.0000e+00
Epoch 231/300
13702/13702 [==============================] - 1s - loss: 6.3799e-05 - acc: 0.0000e+00 - val_loss: 1.4681e-04 - val_acc: 0.0000e+00
Epoch 232/300
13702/13702 [==============================] - 1s - loss: 7.0157e-05 - acc: 0.0000e+00 - val_loss: 1.2865e-04 - val_acc: 0.0000e+00
Epoch 233/300
13702/13702 [==============================] - 1s - loss: 6.7393e-05 - acc: 0.0000e+00 - val_loss: 1.6412e-04 - val_acc: 0.0000e+00
Epoch 234/300
13702/13702 [==============================] - 1s - loss: 6.6449e-05 - acc: 0.0000e+00 - val_loss: 1.2397e-04 - val_acc: 0.0000e+00
Epoch 235/300
13702/13702 [==============================] - 1s - loss: 6.6478e-05 - acc: 0.0000e+00 - val_loss: 4.2292e-04 - val_acc: 0.0000e+00
Epoch 236/300
13702/13702 [==============================] - 1s - loss: 7.0995e-05 - acc: 0.0000e+00 - val_loss: 1.9373e-04 - val_acc: 0.0000e+00
Epoch 237/300
13702/13702 [==============================] - 1s - loss: 6.3382e-05 - acc: 0.0000e+00 - val_loss: 1.3537e-04 - val_acc: 0.0000e+00
Epoch 238/300
13702/13702 [==============================] - 1s - loss: 6.4566e-05 - acc: 0.0000e+00 - val_loss: 1.8424e-04 - val_acc: 0.0000e+00
Epoch 239/300
13702/13702 [==============================] - 1s - loss: 6.2515e-05 - acc: 0.0000e+00 - val_loss: 1.7937e-04 - val_acc: 0.0000e+00
Epoch 240/300
13702/13702 [==============================] - 1s - loss: 6.2237e-05 - acc: 0.0000e+00 - val_loss: 1.5285e-04 - val_acc: 0.0000e+00
Epoch 241/300
13702/13702 [==============================] - 1s - loss: 6.2276e-05 - acc: 0.0000e+00 - val_loss: 1.3065e-04 - val_acc: 0.0000e+00
Epoch 242/300
13702/13702 [==============================] - 1s - loss: 7.3451e-05 - acc: 0.0000e+00 - val_loss: 1.2664e-04 - val_acc: 0.0000e+00
Epoch 243/300
13702/13702 [==============================] - 1s - loss: 8.8749e-05 - acc: 0.0000e+00 - val_loss: 5.3545e-04 - val_acc: 0.0000e+00
Epoch 244/300
13702/13702 [==============================] - 1s - loss: 6.5261e-05 - acc: 0.0000e+00 - val_loss: 1.8167e-04 - val_acc: 0.0000e+00
Epoch 245/300
13702/13702 [==============================] - 1s - loss: 6.9969e-05 - acc: 0.0000e+00 - val_loss: 1.9029e-04 - val_acc: 0.0000e+00
Epoch 246/300
13702/13702 [==============================] - 1s - loss: 6.4475e-05 - acc: 0.0000e+00 - val_loss: 1.1217e-04 - val_acc: 0.0000e+00
Epoch 247/300
13702/13702 [==============================] - 1s - loss: 6.1917e-05 - acc: 0.0000e+00 - val_loss: 1.1719e-04 - val_acc: 0.0000e+00
Epoch 248/300
13702/13702 [==============================] - 1s - loss: 6.1874e-05 - acc: 0.0000e+00 - val_loss: 1.4093e-04 - val_acc: 0.0000e+00
Epoch 249/300
13702/13702 [==============================] - 1s - loss: 6.1113e-05 - acc: 0.0000e+00 - val_loss: 2.7620e-04 - val_acc: 0.0000e+00
Epoch 250/300
13702/13702 [==============================] - 1s - loss: 6.5746e-05 - acc: 0.0000e+00 - val_loss: 3.2060e-04 - val_acc: 0.0000e+00
Epoch 251/300
13702/13702 [==============================] - 1s - loss: 6.2537e-05 - acc: 0.0000e+00 - val_loss: 1.6536e-04 - val_acc: 0.0000e+00
Epoch 252/300
13702/13702 [==============================] - 1s - loss: 6.0386e-05 - acc: 0.0000e+00 - val_loss: 1.8480e-04 - val_acc: 0.0000e+00
Epoch 253/300
13702/13702 [==============================] - 1s - loss: 6.7452e-05 - acc: 0.0000e+00 - val_loss: 1.7845e-04 - val_acc: 0.0000e+00
Epoch 254/300
13702/13702 [==============================] - 1s - loss: 6.9035e-05 - acc: 0.0000e+00 - val_loss: 1.3608e-04 - val_acc: 0.0000e+00
Epoch 255/300
13702/13702 [==============================] - 1s - loss: 6.6192e-05 - acc: 0.0000e+00 - val_loss: 1.4072e-04 - val_acc: 0.0000e+00
Epoch 256/300
13702/13702 [==============================] - 1s - loss: 6.0930e-05 - acc: 0.0000e+00 - val_loss: 1.5739e-04 - val_acc: 0.0000e+00
Epoch 257/300
13702/13702 [==============================] - 1s - loss: 5.8575e-05 - acc: 0.0000e+00 - val_loss: 2.4562e-04 - val_acc: 0.0000e+00
Epoch 258/300
13702/13702 [==============================] - 1s - loss: 6.6940e-05 - acc: 0.0000e+00 - val_loss: 1.1496e-04 - val_acc: 0.0000e+00
Epoch 259/300
13702/13702 [==============================] - 1s - loss: 5.9611e-05 - acc: 0.0000e+00 - val_loss: 1.3642e-04 - val_acc: 0.0000e+00
Epoch 260/300
13702/13702 [==============================] - 1s - loss: 6.6243e-05 - acc: 0.0000e+00 - val_loss: 1.6713e-04 - val_acc: 0.0000e+00
Epoch 261/300
13702/13702 [==============================] - 1s - loss: 6.2638e-05 - acc: 0.0000e+00 - val_loss: 1.3231e-04 - val_acc: 0.0000e+00
Epoch 262/300
13702/13702 [==============================] - 1s - loss: 5.8092e-05 - acc: 0.0000e+00 - val_loss: 1.4920e-04 - val_acc: 0.0000e+00
Epoch 263/300
13702/13702 [==============================] - 1s - loss: 6.2144e-05 - acc: 0.0000e+00 - val_loss: 1.2118e-04 - val_acc: 0.0000e+00
Epoch 264/300
13702/13702 [==============================] - 1s - loss: 5.9098e-05 - acc: 0.0000e+00 - val_loss: 1.2568e-04 - val_acc: 0.0000e+00
Epoch 265/300
13702/13702 [==============================] - 1s - loss: 6.3898e-05 - acc: 0.0000e+00 - val_loss: 1.3974e-04 - val_acc: 0.0000e+00
Epoch 266/300
13702/13702 [==============================] - 1s - loss: 6.2485e-05 - acc: 0.0000e+00 - val_loss: 1.3433e-04 - val_acc: 0.0000e+00
Epoch 267/300
13702/13702 [==============================] - 1s - loss: 5.9859e-05 - acc: 0.0000e+00 - val_loss: 1.7175e-04 - val_acc: 0.0000e+00
Epoch 268/300
13702/13702 [==============================] - 1s - loss: 5.8895e-05 - acc: 0.0000e+00 - val_loss: 2.5076e-04 - val_acc: 0.0000e+00
Epoch 269/300
13702/13702 [==============================] - 1s - loss: 5.7728e-05 - acc: 0.0000e+00 - val_loss: 1.8304e-04 - val_acc: 0.0000e+00
Epoch 270/300
13702/13702 [==============================] - 1s - loss: 5.5977e-05 - acc: 0.0000e+00 - val_loss: 1.4892e-04 - val_acc: 0.0000e+00
Epoch 271/300
13702/13702 [==============================] - 1s - loss: 5.9828e-05 - acc: 0.0000e+00 - val_loss: 1.1163e-04 - val_acc: 0.0000e+00
Epoch 272/300
13702/13702 [==============================] - 1s - loss: 5.7622e-05 - acc: 0.0000e+00 - val_loss: 1.5074e-04 - val_acc: 0.0000e+00
Epoch 273/300
13702/13702 [==============================] - 1s - loss: 6.2082e-05 - acc: 0.0000e+00 - val_loss: 1.5785e-04 - val_acc: 0.0000e+00
Epoch 274/300
13702/13702 [==============================] - 1s - loss: 6.0645e-05 - acc: 0.0000e+00 - val_loss: 1.9091e-04 - val_acc: 0.0000e+00
Epoch 275/300
13702/13702 [==============================] - 1s - loss: 5.4244e-05 - acc: 0.0000e+00 - val_loss: 2.0193e-04 - val_acc: 0.0000e+00
Epoch 276/300
13702/13702 [==============================] - 1s - loss: 6.2321e-05 - acc: 0.0000e+00 - val_loss: 1.8155e-04 - val_acc: 0.0000e+00
Epoch 277/300
13702/13702 [==============================] - 1s - loss: 5.5340e-05 - acc: 0.0000e+00 - val_loss: 1.2953e-04 - val_acc: 0.0000e+00
Epoch 278/300
13702/13702 [==============================] - 1s - loss: 5.8348e-05 - acc: 0.0000e+00 - val_loss: 1.3199e-04 - val_acc: 0.0000e+00
Epoch 279/300
13702/13702 [==============================] - 1s - loss: 6.3089e-05 - acc: 0.0000e+00 - val_loss: 1.1697e-04 - val_acc: 0.0000e+00
Epoch 280/300
13702/13702 [==============================] - 1s - loss: 5.8283e-05 - acc: 0.0000e+00 - val_loss: 1.5540e-04 - val_acc: 0.0000e+00
Epoch 281/300
13702/13702 [==============================] - 1s - loss: 6.0832e-05 - acc: 0.0000e+00 - val_loss: 1.0603e-04 - val_acc: 0.0000e+00
Epoch 282/300
13702/13702 [==============================] - 1s - loss: 6.4353e-05 - acc: 0.0000e+00 - val_loss: 1.2747e-04 - val_acc: 0.0000e+00
Epoch 283/300
13702/13702 [==============================] - 1s - loss: 5.9952e-05 - acc: 0.0000e+00 - val_loss: 1.3812e-04 - val_acc: 0.0000e+00
Epoch 284/300
13702/13702 [==============================] - 1s - loss: 5.7292e-05 - acc: 0.0000e+00 - val_loss: 1.3522e-04 - val_acc: 0.0000e+00
Epoch 285/300
13702/13702 [==============================] - 1s - loss: 5.8882e-05 - acc: 0.0000e+00 - val_loss: 1.1946e-04 - val_acc: 0.0000e+00
Epoch 286/300
13702/13702 [==============================] - 1s - loss: 5.5171e-05 - acc: 0.0000e+00 - val_loss: 1.1019e-04 - val_acc: 0.0000e+00
Epoch 287/300
13702/13702 [==============================] - 1s - loss: 5.9218e-05 - acc: 0.0000e+00 - val_loss: 4.5908e-04 - val_acc: 0.0000e+00
Epoch 288/300
13702/13702 [==============================] - 1s - loss: 6.5222e-05 - acc: 0.0000e+00 - val_loss: 1.4138e-04 - val_acc: 0.0000e+00
Epoch 289/300
13702/13702 [==============================] - 1s - loss: 6.0875e-05 - acc: 0.0000e+00 - val_loss: 1.6488e-04 - val_acc: 0.0000e+00
Epoch 290/300
13702/13702 [==============================] - 1s - loss: 5.8259e-05 - acc: 0.0000e+00 - val_loss: 1.0965e-04 - val_acc: 0.0000e+00
Epoch 291/300
13702/13702 [==============================] - 1s - loss: 6.2715e-05 - acc: 0.0000e+00 - val_loss: 1.2354e-04 - val_acc: 0.0000e+00
Epoch 292/300
13702/13702 [==============================] - 1s - loss: 5.6251e-05 - acc: 0.0000e+00 - val_loss: 4.3120e-04 - val_acc: 0.0000e+00
Epoch 293/300
13702/13702 [==============================] - 1s - loss: 6.6403e-05 - acc: 0.0000e+00 - val_loss: 2.2879e-04 - val_acc: 0.0000e+00
Epoch 294/300
13702/13702 [==============================] - 1s - loss: 5.9023e-05 - acc: 0.0000e+00 - val_loss: 1.2297e-04 - val_acc: 0.0000e+00
Epoch 295/300
13702/13702 [==============================] - 1s - loss: 5.6180e-05 - acc: 0.0000e+00 - val_loss: 1.2277e-04 - val_acc: 0.0000e+00
Epoch 296/300
13702/13702 [==============================] - 1s - loss: 5.8272e-05 - acc: 0.0000e+00 - val_loss: 1.1051e-04 - val_acc: 0.0000e+00
Epoch 297/300
13702/13702 [==============================] - 1s - loss: 5.6324e-05 - acc: 0.0000e+00 - val_loss: 2.8671e-04 - val_acc: 0.0000e+00
Epoch 298/300
13702/13702 [==============================] - 1s - loss: 5.9531e-05 - acc: 0.0000e+00 - val_loss: 2.0190e-04 - val_acc: 0.0000e+00
Epoch 299/300
13702/13702 [==============================] - 1s - loss: 6.8640e-05 - acc: 0.0000e+00 - val_loss: 3.7049e-04 - val_acc: 0.0000e+00
Epoch 300/300
13702/13702 [==============================] - 1s - loss: 6.4655e-05 - acc: 0.0000e+00 - val_loss: 1.9034e-04 - val_acc: 0.0000e+00
Train Score: 0.00004 MSE (0.01 RMSE)
Test Score: 0.01902 MSE (0.14 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_25 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_25 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_26 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_26 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_25 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_26 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/300
13702/13702 [==============================] - 3s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00
Epoch 2/300
13702/13702 [==============================] - 1s - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 3/300
13702/13702 [==============================] - 1s - loss: 6.4386e-04 - acc: 0.0000e+00 - val_loss: 5.4717e-04 - val_acc: 0.0000e+00
Epoch 4/300
13702/13702 [==============================] - 1s - loss: 5.5911e-04 - acc: 0.0000e+00 - val_loss: 4.4798e-04 - val_acc: 0.0000e+00
Epoch 5/300
13702/13702 [==============================] - 1s - loss: 4.6878e-04 - acc: 0.0000e+00 - val_loss: 4.6753e-04 - val_acc: 0.0000e+00
Epoch 6/300
13702/13702 [==============================] - 1s - loss: 5.0826e-04 - acc: 0.0000e+00 - val_loss: 7.1541e-04 - val_acc: 0.0000e+00
Epoch 7/300
13702/13702 [==============================] - 1s - loss: 4.7876e-04 - acc: 0.0000e+00 - val_loss: 2.7994e-04 - val_acc: 0.0000e+00
Epoch 8/300
13702/13702 [==============================] - 1s - loss: 4.5514e-04 - acc: 0.0000e+00 - val_loss: 2.9044e-04 - val_acc: 0.0000e+00
Epoch 9/300
13702/13702 [==============================] - 1s - loss: 4.3154e-04 - acc: 0.0000e+00 - val_loss: 2.6270e-04 - val_acc: 0.0000e+00
Epoch 10/300
13702/13702 [==============================] - 1s - loss: 4.1989e-04 - acc: 0.0000e+00 - val_loss: 2.5304e-04 - val_acc: 0.0000e+00
Epoch 11/300
13702/13702 [==============================] - 1s - loss: 4.0541e-04 - acc: 0.0000e+00 - val_loss: 2.9865e-04 - val_acc: 0.0000e+0000e
Epoch 12/300
13702/13702 [==============================] - 1s - loss: 4.4254e-04 - acc: 0.0000e+00 - val_loss: 8.1913e-04 - val_acc: 0.0000e+00
Epoch 13/300
13702/13702 [==============================] - 1s - loss: 4.1472e-04 - acc: 0.0000e+00 - val_loss: 4.6979e-04 - val_acc: 0.0000e+00
Epoch 14/300
13702/13702 [==============================] - 1s - loss: 4.5242e-04 - acc: 0.0000e+00 - val_loss: 3.1256e-04 - val_acc: 0.0000e+00
Epoch 15/300
13702/13702 [==============================] - 1s - loss: 4.2801e-04 - acc: 0.0000e+00 - val_loss: 4.7598e-04 - val_acc: 0.0000e+00
Epoch 16/300
13702/13702 [==============================] - 1s - loss: 3.7100e-04 - acc: 0.0000e+00 - val_loss: 3.2847e-04 - val_acc: 0.0000e+00
Epoch 17/300
13702/13702 [==============================] - 1s - loss: 3.6308e-04 - acc: 0.0000e+00 - val_loss: 4.8486e-04 - val_acc: 0.0000e+00
Epoch 18/300
13702/13702 [==============================] - 1s - loss: 3.9300e-04 - acc: 0.0000e+00 - val_loss: 2.7858e-04 - val_acc: 0.0000e+00
Epoch 19/300
13702/13702 [==============================] - 1s - loss: 3.7778e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 20/300
13702/13702 [==============================] - 1s - loss: 3.5661e-04 - acc: 0.0000e+00 - val_loss: 4.5353e-04 - val_acc: 0.0000e+00
Epoch 21/300
13702/13702 [==============================] - 1s - loss: 3.3533e-04 - acc: 0.0000e+00 - val_loss: 3.3419e-04 - val_acc: 0.0000e+00
Epoch 22/300
13702/13702 [==============================] - 1s - loss: 3.4494e-04 - acc: 0.0000e+00 - val_loss: 2.2743e-04 - val_acc: 0.0000e+00
Epoch 23/300
13702/13702 [==============================] - 1s - loss: 3.0804e-04 - acc: 0.0000e+00 - val_loss: 2.8642e-04 - val_acc: 0.0000e+00
Epoch 24/300
13702/13702 [==============================] - 1s - loss: 3.3852e-04 - acc: 0.0000e+00 - val_loss: 8.6978e-04 - val_acc: 0.0000e+00
Epoch 25/300
13702/13702 [==============================] - 1s - loss: 3.1388e-04 - acc: 0.0000e+00 - val_loss: 3.2524e-04 - val_acc: 0.0000e+00
Epoch 26/300
13702/13702 [==============================] - 1s - loss: 3.1022e-04 - acc: 0.0000e+00 - val_loss: 2.3178e-04 - val_acc: 0.0000e+00
Epoch 27/300
13702/13702 [==============================] - 1s - loss: 2.8452e-04 - acc: 0.0000e+00 - val_loss: 2.4241e-04 - val_acc: 0.0000e+00
Epoch 28/300
13702/13702 [==============================] - 1s - loss: 2.9892e-04 - acc: 0.0000e+00 - val_loss: 4.2017e-04 - val_acc: 0.0000e+00
Epoch 29/300
13702/13702 [==============================] - 1s - loss: 2.9662e-04 - acc: 0.0000e+00 - val_loss: 2.4998e-04 - val_acc: 0.0000e+00
Epoch 30/300
13702/13702 [==============================] - 1s - loss: 2.7747e-04 - acc: 0.0000e+00 - val_loss: 2.0171e-04 - val_acc: 0.0000e+00
Epoch 31/300
13702/13702 [==============================] - 1s - loss: 2.6957e-04 - acc: 0.0000e+00 - val_loss: 3.4630e-04 - val_acc: 0.0000e+00
Epoch 32/300
13702/13702 [==============================] - 1s - loss: 2.6704e-04 - acc: 0.0000e+00 - val_loss: 2.0108e-04 - val_acc: 0.0000e+00
Epoch 33/300
13702/13702 [==============================] - 1s - loss: 2.5439e-04 - acc: 0.0000e+00 - val_loss: 2.1021e-04 - val_acc: 0.0000e+00
Epoch 34/300
13702/13702 [==============================] - 1s - loss: 2.5575e-04 - acc: 0.0000e+00 - val_loss: 2.0457e-04 - val_acc: 0.0000e+00
Epoch 35/300
13702/13702 [==============================] - 1s - loss: 2.4987e-04 - acc: 0.0000e+00 - val_loss: 1.9787e-04 - val_acc: 0.0000e+00
Epoch 36/300
13702/13702 [==============================] - 1s - loss: 2.5587e-04 - acc: 0.0000e+00 - val_loss: 3.0186e-04 - val_acc: 0.0000e+00
Epoch 37/300
13702/13702 [==============================] - 1s - loss: 2.3401e-04 - acc: 0.0000e+00 - val_loss: 3.8604e-04 - val_acc: 0.0000e+00
Epoch 38/300
13702/13702 [==============================] - 1s - loss: 2.3493e-04 - acc: 0.0000e+00 - val_loss: 1.9353e-04 - val_acc: 0.0000e+00
Epoch 39/300
13702/13702 [==============================] - 1s - loss: 2.3649e-04 - acc: 0.0000e+00 - val_loss: 3.4251e-04 - val_acc: 0.0000e+00
Epoch 40/300
13702/13702 [==============================] - 1s - loss: 2.1416e-04 - acc: 0.0000e+00 - val_loss: 4.9627e-04 - val_acc: 0.0000e+00
Epoch 41/300
13702/13702 [==============================] - 1s - loss: 2.1964e-04 - acc: 0.0000e+00 - val_loss: 4.9572e-04 - val_acc: 0.0000e+00
Epoch 42/300
13702/13702 [==============================] - 1s - loss: 1.9810e-04 - acc: 0.0000e+00 - val_loss: 2.1679e-04 - val_acc: 0.0000e+00
Epoch 43/300
13702/13702 [==============================] - 1s - loss: 2.2570e-04 - acc: 0.0000e+00 - val_loss: 2.0586e-04 - val_acc: 0.0000e+00
Epoch 44/300
13702/13702 [==============================] - 1s - loss: 2.0277e-04 - acc: 0.0000e+00 - val_loss: 3.8255e-04 - val_acc: 0.0000e+00
Epoch 45/300
13702/13702 [==============================] - 1s - loss: 1.9291e-04 - acc: 0.0000e+00 - val_loss: 3.8149e-04 - val_acc: 0.0000e+00
Epoch 46/300
13702/13702 [==============================] - 1s - loss: 1.9607e-04 - acc: 0.0000e+00 - val_loss: 9.4827e-04 - val_acc: 0.0000e+00
Epoch 47/300
13702/13702 [==============================] - 1s - loss: 2.0954e-04 - acc: 0.0000e+00 - val_loss: 2.3133e-04 - val_acc: 0.0000e+00
Epoch 48/300
13702/13702 [==============================] - 1s - loss: 1.8128e-04 - acc: 0.0000e+00 - val_loss: 1.9753e-04 - val_acc: 0.0000e+00
Epoch 49/300
13702/13702 [==============================] - 1s - loss: 1.8207e-04 - acc: 0.0000e+00 - val_loss: 4.3097e-04 - val_acc: 0.0000e+00
Epoch 50/300
13702/13702 [==============================] - 1s - loss: 1.7184e-04 - acc: 0.0000e+00 - val_loss: 2.4755e-04 - val_acc: 0.0000e+00
Epoch 51/300
13702/13702 [==============================] - 1s - loss: 1.7364e-04 - acc: 0.0000e+00 - val_loss: 2.9544e-04 - val_acc: 0.0000e+00
Epoch 52/300
13702/13702 [==============================] - 1s - loss: 1.7096e-04 - acc: 0.0000e+00 - val_loss: 5.7128e-04 - val_acc: 0.0000e+00
Epoch 53/300
13702/13702 [==============================] - 1s - loss: 1.6964e-04 - acc: 0.0000e+00 - val_loss: 2.1292e-04 - val_acc: 0.0000e+00
Epoch 54/300
13702/13702 [==============================] - 1s - loss: 1.7671e-04 - acc: 0.0000e+00 - val_loss: 3.9969e-04 - val_acc: 0.0000e+00
Epoch 55/300
13702/13702 [==============================] - 1s - loss: 1.5702e-04 - acc: 0.0000e+00 - val_loss: 5.4666e-04 - val_acc: 0.0000e+00
Epoch 56/300
13702/13702 [==============================] - 1s - loss: 1.6992e-04 - acc: 0.0000e+00 - val_loss: 2.9789e-04 - val_acc: 0.0000e+00
Epoch 57/300
13702/13702 [==============================] - 1s - loss: 1.6094e-04 - acc: 0.0000e+00 - val_loss: 4.2323e-04 - val_acc: 0.0000e+00
Epoch 58/300
13702/13702 [==============================] - 1s - loss: 1.7045e-04 - acc: 0.0000e+00 - val_loss: 2.1987e-04 - val_acc: 0.0000e+00
Epoch 59/300
13702/13702 [==============================] - 1s - loss: 1.5717e-04 - acc: 0.0000e+00 - val_loss: 5.0090e-04 - val_acc: 0.0000e+00
Epoch 60/300
13702/13702 [==============================] - 1s - loss: 1.5769e-04 - acc: 0.0000e+00 - val_loss: 2.3909e-04 - val_acc: 0.0000e+00
Epoch 61/300
13702/13702 [==============================] - 1s - loss: 1.5693e-04 - acc: 0.0000e+00 - val_loss: 2.4203e-04 - val_acc: 0.0000e+00
Epoch 62/300
13702/13702 [==============================] - 1s - loss: 1.5487e-04 - acc: 0.0000e+00 - val_loss: 2.1495e-04 - val_acc: 0.0000e+00
Epoch 63/300
13702/13702 [==============================] - 1s - loss: 1.6468e-04 - acc: 0.0000e+00 - val_loss: 3.7309e-04 - val_acc: 0.0000e+00
Epoch 64/300
13702/13702 [==============================] - 1s - loss: 1.5239e-04 - acc: 0.0000e+00 - val_loss: 4.1053e-04 - val_acc: 0.0000e+00
Epoch 65/300
13702/13702 [==============================] - 1s - loss: 1.4429e-04 - acc: 0.0000e+00 - val_loss: 2.8143e-04 - val_acc: 0.0000e+00
Epoch 66/300
13702/13702 [==============================] - 1s - loss: 1.5112e-04 - acc: 0.0000e+00 - val_loss: 2.2238e-04 - val_acc: 0.0000e+00
Epoch 67/300
13702/13702 [==============================] - 1s - loss: 1.5104e-04 - acc: 0.0000e+00 - val_loss: 6.9409e-04 - val_acc: 0.0000e+00
Epoch 68/300
13702/13702 [==============================] - 1s - loss: 1.6073e-04 - acc: 0.0000e+00 - val_loss: 2.6559e-04 - val_acc: 0.0000e+00
Epoch 69/300
13702/13702 [==============================] - 1s - loss: 1.4011e-04 - acc: 0.0000e+00 - val_loss: 1.9698e-04 - val_acc: 0.0000e+00
Epoch 70/300
13702/13702 [==============================] - 1s - loss: 1.4121e-04 - acc: 0.0000e+00 - val_loss: 7.9541e-04 - val_acc: 0.0000e+00
Epoch 71/300
13702/13702 [==============================] - 1s - loss: 1.3430e-04 - acc: 0.0000e+00 - val_loss: 1.9281e-04 - val_acc: 0.0000e+00
Epoch 72/300
13702/13702 [==============================] - 1s - loss: 1.3912e-04 - acc: 0.0000e+00 - val_loss: 2.1734e-04 - val_acc: 0.0000e+00
Epoch 73/300
13702/13702 [==============================] - 1s - loss: 1.2586e-04 - acc: 0.0000e+00 - val_loss: 2.0766e-04 - val_acc: 0.0000e+00
Epoch 74/300
13702/13702 [==============================] - 1s - loss: 1.3371e-04 - acc: 0.0000e+00 - val_loss: 2.6869e-04 - val_acc: 0.0000e+00
Epoch 75/300
13702/13702 [==============================] - 1s - loss: 1.3078e-04 - acc: 0.0000e+00 - val_loss: 2.6931e-04 - val_acc: 0.0000e+00
Epoch 76/300
13702/13702 [==============================] - 1s - loss: 1.3657e-04 - acc: 0.0000e+00 - val_loss: 3.4031e-04 - val_acc: 0.0000e+00
Epoch 77/300
13702/13702 [==============================] - 1s - loss: 1.3356e-04 - acc: 0.0000e+00 - val_loss: 2.9700e-04 - val_acc: 0.0000e+00
Epoch 78/300
13702/13702 [==============================] - 1s - loss: 1.2530e-04 - acc: 0.0000e+00 - val_loss: 6.4052e-04 - val_acc: 0.0000e+00
Epoch 79/300
13702/13702 [==============================] - 1s - loss: 1.3424e-04 - acc: 0.0000e+00 - val_loss: 2.3059e-04 - val_acc: 0.0000e+00
Epoch 80/300
13702/13702 [==============================] - 1s - loss: 1.3022e-04 - acc: 0.0000e+00 - val_loss: 1.8061e-04 - val_acc: 0.0000e+00
Epoch 81/300
13702/13702 [==============================] - 1s - loss: 1.2594e-04 - acc: 0.0000e+00 - val_loss: 2.8664e-04 - val_acc: 0.0000e+00
Epoch 82/300
13702/13702 [==============================] - 1s - loss: 1.1836e-04 - acc: 0.0000e+00 - val_loss: 2.7994e-04 - val_acc: 0.0000e+00
Epoch 83/300
13702/13702 [==============================] - 1s - loss: 1.2215e-04 - acc: 0.0000e+00 - val_loss: 4.1451e-04 - val_acc: 0.0000e+00
Epoch 84/300
13702/13702 [==============================] - 1s - loss: 1.2452e-04 - acc: 0.0000e+00 - val_loss: 4.5247e-04 - val_acc: 0.0000e+00
Epoch 85/300
13702/13702 [==============================] - 1s - loss: 1.1907e-04 - acc: 0.0000e+00 - val_loss: 3.5772e-04 - val_acc: 0.0000e+00
Epoch 86/300
13702/13702 [==============================] - 1s - loss: 1.2178e-04 - acc: 0.0000e+00 - val_loss: 2.4325e-04 - val_acc: 0.0000e+00
Epoch 87/300
13702/13702 [==============================] - 1s - loss: 1.2205e-04 - acc: 0.0000e+00 - val_loss: 1.7202e-04 - val_acc: 0.0000e+00
Epoch 88/300
13702/13702 [==============================] - 1s - loss: 1.3277e-04 - acc: 0.0000e+00 - val_loss: 2.9559e-04 - val_acc: 0.0000e+00
Epoch 89/300
13702/13702 [==============================] - 1s - loss: 1.1644e-04 - acc: 0.0000e+00 - val_loss: 1.7751e-04 - val_acc: 0.0000e+00
Epoch 90/300
13702/13702 [==============================] - 1s - loss: 1.1919e-04 - acc: 0.0000e+00 - val_loss: 2.4570e-04 - val_acc: 0.0000e+00
Epoch 91/300
13702/13702 [==============================] - 1s - loss: 1.1800e-04 - acc: 0.0000e+00 - val_loss: 2.8960e-04 - val_acc: 0.0000e+00
Epoch 92/300
13702/13702 [==============================] - 1s - loss: 1.1036e-04 - acc: 0.0000e+00 - val_loss: 2.1366e-04 - val_acc: 0.0000e+00
Epoch 93/300
13702/13702 [==============================] - 1s - loss: 1.2211e-04 - acc: 0.0000e+00 - val_loss: 2.0458e-04 - val_acc: 0.0000e+00
Epoch 94/300
13702/13702 [==============================] - 1s - loss: 1.1221e-04 - acc: 0.0000e+00 - val_loss: 2.2724e-04 - val_acc: 0.0000e+00
Epoch 95/300
13702/13702 [==============================] - 1s - loss: 1.0279e-04 - acc: 0.0000e+00 - val_loss: 1.8861e-04 - val_acc: 0.0000e+00
Epoch 96/300
13702/13702 [==============================] - 1s - loss: 1.0649e-04 - acc: 0.0000e+00 - val_loss: 3.6068e-04 - val_acc: 0.0000e+00
Epoch 97/300
13702/13702 [==============================] - 1s - loss: 1.1200e-04 - acc: 0.0000e+00 - val_loss: 2.7149e-04 - val_acc: 0.0000e+00
Epoch 98/300
13702/13702 [==============================] - 1s - loss: 1.0834e-04 - acc: 0.0000e+00 - val_loss: 4.6287e-04 - val_acc: 0.0000e+00
Epoch 99/300
13702/13702 [==============================] - 1s - loss: 1.3148e-04 - acc: 0.0000e+00 - val_loss: 2.2160e-04 - val_acc: 0.0000e+00
Epoch 100/300
13702/13702 [==============================] - 1s - loss: 1.1892e-04 - acc: 0.0000e+00 - val_loss: 1.8485e-04 - val_acc: 0.0000e+00
Epoch 101/300
13702/13702 [==============================] - 1s - loss: 1.0123e-04 - acc: 0.0000e+00 - val_loss: 4.0034e-04 - val_acc: 0.0000e+00
Epoch 102/300
13702/13702 [==============================] - 1s - loss: 1.0113e-04 - acc: 0.0000e+00 - val_loss: 2.7637e-04 - val_acc: 0.0000e+00
Epoch 103/300
13702/13702 [==============================] - 1s - loss: 1.0429e-04 - acc: 0.0000e+00 - val_loss: 1.7193e-04 - val_acc: 0.0000e+00
Epoch 104/300
13702/13702 [==============================] - 1s - loss: 1.1349e-04 - acc: 0.0000e+00 - val_loss: 2.3464e-04 - val_acc: 0.0000e+00
Epoch 105/300
13702/13702 [==============================] - 1s - loss: 1.1262e-04 - acc: 0.0000e+00 - val_loss: 1.6379e-04 - val_acc: 0.0000e+00
Epoch 106/300
13702/13702 [==============================] - 1s - loss: 1.1030e-04 - acc: 0.0000e+00 - val_loss: 1.4948e-04 - val_acc: 0.0000e+00
Epoch 107/300
13702/13702 [==============================] - 1s - loss: 1.0890e-04 - acc: 0.0000e+00 - val_loss: 1.7867e-04 - val_acc: 0.0000e+00
Epoch 108/300
13702/13702 [==============================] - 1s - loss: 1.0323e-04 - acc: 0.0000e+00 - val_loss: 2.7668e-04 - val_acc: 0.0000e+00 ETA: 1s - loss: 1.0399e-04 -
Epoch 109/300
13702/13702 [==============================] - 1s - loss: 1.1591e-04 - acc: 0.0000e+00 - val_loss: 2.7390e-04 - val_acc: 0.0000e+00
Epoch 110/300
13702/13702 [==============================] - 1s - loss: 1.0458e-04 - acc: 0.0000e+00 - val_loss: 1.7119e-04 - val_acc: 0.0000e+00
Epoch 111/300
13702/13702 [==============================] - 1s - loss: 1.0202e-04 - acc: 0.0000e+00 - val_loss: 2.5151e-04 - val_acc: 0.0000e+00
Epoch 112/300
13702/13702 [==============================] - 1s - loss: 1.1107e-04 - acc: 0.0000e+00 - val_loss: 2.6078e-04 - val_acc: 0.0000e+00
Epoch 113/300
13702/13702 [==============================] - 1s - loss: 1.2399e-04 - acc: 0.0000e+00 - val_loss: 3.3143e-04 - val_acc: 0.0000e+00
Epoch 114/300
13702/13702 [==============================] - 1s - loss: 1.1590e-04 - acc: 0.0000e+00 - val_loss: 1.8577e-04 - val_acc: 0.0000e+00
Epoch 115/300
13702/13702 [==============================] - 1s - loss: 1.0652e-04 - acc: 0.0000e+00 - val_loss: 4.1832e-04 - val_acc: 0.0000e+00
Epoch 116/300
13702/13702 [==============================] - 1s - loss: 1.0478e-04 - acc: 0.0000e+00 - val_loss: 1.7093e-04 - val_acc: 0.0000e+00
Epoch 117/300
13702/13702 [==============================] - 1s - loss: 1.0557e-04 - acc: 0.0000e+00 - val_loss: 1.6134e-04 - val_acc: 0.0000e+00
Epoch 118/300
13702/13702 [==============================] - 1s - loss: 1.0096e-04 - acc: 0.0000e+00 - val_loss: 3.1371e-04 - val_acc: 0.0000e+00
Epoch 119/300
13702/13702 [==============================] - 1s - loss: 1.0885e-04 - acc: 0.0000e+00 - val_loss: 2.5236e-04 - val_acc: 0.0000e+00
Epoch 120/300
13702/13702 [==============================] - 1s - loss: 1.0217e-04 - acc: 0.0000e+00 - val_loss: 2.6262e-04 - val_acc: 0.0000e+00
Epoch 121/300
13702/13702 [==============================] - 1s - loss: 1.0019e-04 - acc: 0.0000e+00 - val_loss: 2.0006e-04 - val_acc: 0.0000e+00
Epoch 122/300
13702/13702 [==============================] - 1s - loss: 9.7932e-05 - acc: 0.0000e+00 - val_loss: 3.8748e-04 - val_acc: 0.0000e+00
Epoch 123/300
13702/13702 [==============================] - 1s - loss: 1.0174e-04 - acc: 0.0000e+00 - val_loss: 1.8820e-04 - val_acc: 0.0000e+00
Epoch 124/300
13702/13702 [==============================] - 1s - loss: 9.4865e-05 - acc: 0.0000e+00 - val_loss: 2.5676e-04 - val_acc: 0.0000e+00
Epoch 125/300
13702/13702 [==============================] - 1s - loss: 9.3420e-05 - acc: 0.0000e+00 - val_loss: 1.6266e-04 - val_acc: 0.0000e+00
Epoch 126/300
13702/13702 [==============================] - 1s - loss: 1.0300e-04 - acc: 0.0000e+00 - val_loss: 1.7114e-04 - val_acc: 0.0000e+00
Epoch 127/300
13702/13702 [==============================] - 1s - loss: 1.0958e-04 - acc: 0.0000e+00 - val_loss: 2.9933e-04 - val_acc: 0.0000e+00
Epoch 128/300
13702/13702 [==============================] - 1s - loss: 1.0269e-04 - acc: 0.0000e+00 - val_loss: 1.4630e-04 - val_acc: 0.0000e+00
Epoch 129/300
13702/13702 [==============================] - 1s - loss: 1.0454e-04 - acc: 0.0000e+00 - val_loss: 3.6714e-04 - val_acc: 0.0000e+00
Epoch 130/300
13702/13702 [==============================] - 1s - loss: 1.0483e-04 - acc: 0.0000e+00 - val_loss: 4.5926e-04 - val_acc: 0.0000e+00
Epoch 131/300
13702/13702 [==============================] - 1s - loss: 9.4432e-05 - acc: 0.0000e+00 - val_loss: 1.5934e-04 - val_acc: 0.0000e+00
Epoch 132/300
13702/13702 [==============================] - 1s - loss: 9.2716e-05 - acc: 0.0000e+00 - val_loss: 1.7572e-04 - val_acc: 0.0000e+00
Epoch 133/300
13702/13702 [==============================] - 1s - loss: 9.4678e-05 - acc: 0.0000e+00 - val_loss: 2.2130e-04 - val_acc: 0.0000e+00
Epoch 134/300
13702/13702 [==============================] - 1s - loss: 9.6648e-05 - acc: 0.0000e+00 - val_loss: 2.2840e-04 - val_acc: 0.0000e+00
Epoch 135/300
13702/13702 [==============================] - 1s - loss: 1.0326e-04 - acc: 0.0000e+00 - val_loss: 1.5782e-04 - val_acc: 0.0000e+00
Epoch 136/300
13702/13702 [==============================] - 1s - loss: 9.3905e-05 - acc: 0.0000e+00 - val_loss: 1.7204e-04 - val_acc: 0.0000e+00
Epoch 137/300
13702/13702 [==============================] - 1s - loss: 9.4373e-05 - acc: 0.0000e+00 - val_loss: 1.7494e-04 - val_acc: 0.0000e+00
Epoch 138/300
13702/13702 [==============================] - 1s - loss: 9.6960e-05 - acc: 0.0000e+00 - val_loss: 1.5272e-04 - val_acc: 0.0000e+00
Epoch 139/300
13702/13702 [==============================] - 1s - loss: 9.8571e-05 - acc: 0.0000e+00 - val_loss: 4.9830e-04 - val_acc: 0.0000e+00
Epoch 140/300
13702/13702 [==============================] - 1s - loss: 9.0131e-05 - acc: 0.0000e+00 - val_loss: 2.0329e-04 - val_acc: 0.0000e+00
Epoch 141/300
13702/13702 [==============================] - 1s - loss: 9.1099e-05 - acc: 0.0000e+00 - val_loss: 4.8096e-04 - val_acc: 0.0000e+00
Epoch 142/300
13702/13702 [==============================] - 1s - loss: 1.0184e-04 - acc: 0.0000e+00 - val_loss: 2.2018e-04 - val_acc: 0.0000e+00
Epoch 143/300
13702/13702 [==============================] - 1s - loss: 8.9399e-05 - acc: 0.0000e+00 - val_loss: 1.6973e-04 - val_acc: 0.0000e+00
Epoch 144/300
13702/13702 [==============================] - 1s - loss: 9.8588e-05 - acc: 0.0000e+00 - val_loss: 4.8580e-04 - val_acc: 0.0000e+00
Epoch 145/300
13702/13702 [==============================] - 1s - loss: 9.2787e-05 - acc: 0.0000e+00 - val_loss: 1.9445e-04 - val_acc: 0.0000e+00
Epoch 146/300
13702/13702 [==============================] - 1s - loss: 8.9057e-05 - acc: 0.0000e+00 - val_loss: 1.9007e-04 - val_acc: 0.0000e+00
Epoch 147/300
13702/13702 [==============================] - 1s - loss: 9.3715e-05 - acc: 0.0000e+00 - val_loss: 5.7164e-04 - val_acc: 0.0000e+00
Epoch 148/300
13702/13702 [==============================] - 1s - loss: 9.7711e-05 - acc: 0.0000e+00 - val_loss: 2.9675e-04 - val_acc: 0.0000e+00
Epoch 149/300
13702/13702 [==============================] - 1s - loss: 8.9916e-05 - acc: 0.0000e+00 - val_loss: 1.4352e-04 - val_acc: 0.0000e+00
Epoch 150/300
13702/13702 [==============================] - 1s - loss: 9.2860e-05 - acc: 0.0000e+00 - val_loss: 1.8121e-04 - val_acc: 0.0000e+00
Epoch 151/300
13702/13702 [==============================] - 1s - loss: 9.2551e-05 - acc: 0.0000e+00 - val_loss: 5.3585e-04 - val_acc: 0.0000e+00
Epoch 152/300
13702/13702 [==============================] - 1s - loss: 1.0055e-04 - acc: 0.0000e+00 - val_loss: 2.9006e-04 - val_acc: 0.0000e+00
Epoch 153/300
13702/13702 [==============================] - 1s - loss: 1.0379e-04 - acc: 0.0000e+00 - val_loss: 4.6684e-04 - val_acc: 0.0000e+00
Epoch 154/300
13702/13702 [==============================] - 1s - loss: 9.1671e-05 - acc: 0.0000e+00 - val_loss: 1.7542e-04 - val_acc: 0.0000e+00
Epoch 155/300
13702/13702 [==============================] - 1s - loss: 9.0315e-05 - acc: 0.0000e+00 - val_loss: 1.7235e-04 - val_acc: 0.0000e+00
Epoch 156/300
13702/13702 [==============================] - 1s - loss: 9.0181e-05 - acc: 0.0000e+00 - val_loss: 1.9466e-04 - val_acc: 0.0000e+00
Epoch 157/300
13702/13702 [==============================] - 1s - loss: 9.7080e-05 - acc: 0.0000e+00 - val_loss: 1.6134e-04 - val_acc: 0.0000e+00
Epoch 158/300
13702/13702 [==============================] - 1s - loss: 9.8794e-05 - acc: 0.0000e+00 - val_loss: 1.9032e-04 - val_acc: 0.0000e+00
Epoch 159/300
13702/13702 [==============================] - 1s - loss: 8.6255e-05 - acc: 0.0000e+00 - val_loss: 1.8715e-04 - val_acc: 0.0000e+00
Epoch 160/300
13702/13702 [==============================] - 1s - loss: 8.5782e-05 - acc: 0.0000e+00 - val_loss: 1.8604e-04 - val_acc: 0.0000e+00
Epoch 161/300
13702/13702 [==============================] - 1s - loss: 9.1113e-05 - acc: 0.0000e+00 - val_loss: 2.5892e-04 - val_acc: 0.0000e+00
Epoch 162/300
13702/13702 [==============================] - 1s - loss: 8.7331e-05 - acc: 0.0000e+00 - val_loss: 2.1153e-04 - val_acc: 0.0000e+00
Epoch 163/300
13702/13702 [==============================] - 1s - loss: 8.6033e-05 - acc: 0.0000e+00 - val_loss: 2.3757e-04 - val_acc: 0.0000e+00
Epoch 164/300
13702/13702 [==============================] - 2s - loss: 8.4232e-05 - acc: 0.0000e+00 - val_loss: 1.7204e-04 - val_acc: 0.0000e+00
Epoch 165/300
13702/13702 [==============================] - 1s - loss: 1.0269e-04 - acc: 0.0000e+00 - val_loss: 1.7793e-04 - val_acc: 0.0000e+00
Epoch 166/300
13702/13702 [==============================] - 1s - loss: 8.7834e-05 - acc: 0.0000e+00 - val_loss: 3.6255e-04 - val_acc: 0.0000e+00
Epoch 167/300
13702/13702 [==============================] - 2s - loss: 8.4666e-05 - acc: 0.0000e+00 - val_loss: 2.3223e-04 - val_acc: 0.0000e+00
Epoch 168/300
13702/13702 [==============================] - 1s - loss: 8.2703e-05 - acc: 0.0000e+00 - val_loss: 1.6357e-04 - val_acc: 0.0000e+00
Epoch 169/300
13702/13702 [==============================] - 1s - loss: 8.7699e-05 - acc: 0.0000e+00 - val_loss: 2.1625e-04 - val_acc: 0.0000e+00
Epoch 170/300
13702/13702 [==============================] - 1s - loss: 9.1496e-05 - acc: 0.0000e+00 - val_loss: 2.6867e-04 - val_acc: 0.0000e+00
Epoch 171/300
13702/13702 [==============================] - 2s - loss: 8.7221e-05 - acc: 0.0000e+00 - val_loss: 2.0651e-04 - val_acc: 0.0000e+00
Epoch 172/300
13702/13702 [==============================] - 1s - loss: 9.6792e-05 - acc: 0.0000e+00 - val_loss: 4.4673e-04 - val_acc: 0.0000e+00
Epoch 173/300
13702/13702 [==============================] - 1s - loss: 9.2993e-05 - acc: 0.0000e+00 - val_loss: 1.7104e-04 - val_acc: 0.0000e+00
Epoch 174/300
13702/13702 [==============================] - 1s - loss: 8.6934e-05 - acc: 0.0000e+00 - val_loss: 3.8071e-04 - val_acc: 0.0000e+00
Epoch 175/300
13702/13702 [==============================] - 1s - loss: 9.3658e-05 - acc: 0.0000e+00 - val_loss: 1.7402e-04 - val_acc: 0.0000e+00
Epoch 176/300
13702/13702 [==============================] - 1s - loss: 9.1573e-05 - acc: 0.0000e+00 - val_loss: 4.2781e-04 - val_acc: 0.0000e+00
Epoch 177/300
13702/13702 [==============================] - 1s - loss: 8.8046e-05 - acc: 0.0000e+00 - val_loss: 4.0356e-04 - val_acc: 0.0000e+00
Epoch 178/300
13702/13702 [==============================] - 1s - loss: 8.3971e-05 - acc: 0.0000e+00 - val_loss: 1.5127e-04 - val_acc: 0.0000e+00
Epoch 179/300
13702/13702 [==============================] - 2s - loss: 8.3811e-05 - acc: 0.0000e+00 - val_loss: 2.4027e-04 - val_acc: 0.0000e+00
Epoch 180/300
13702/13702 [==============================] - 2s - loss: 7.8286e-05 - acc: 0.0000e+00 - val_loss: 1.4240e-04 - val_acc: 0.0000e+00
Epoch 181/300
13702/13702 [==============================] - 1s - loss: 8.3377e-05 - acc: 0.0000e+00 - val_loss: 2.4492e-04 - val_acc: 0.0000e+0000e+0
Epoch 182/300
13702/13702 [==============================] - 1s - loss: 9.0844e-05 - acc: 0.0000e+00 - val_loss: 2.9170e-04 - val_acc: 0.0000e+00
Epoch 183/300
13702/13702 [==============================] - 1s - loss: 9.0601e-05 - acc: 0.0000e+00 - val_loss: 1.8117e-04 - val_acc: 0.0000e+00
Epoch 184/300
13702/13702 [==============================] - 1s - loss: 8.1868e-05 - acc: 0.0000e+00 - val_loss: 2.0404e-04 - val_acc: 0.0000e+00
Epoch 185/300
13702/13702 [==============================] - 1s - loss: 7.9355e-05 - acc: 0.0000e+00 - val_loss: 4.4221e-04 - val_acc: 0.0000e+00
Epoch 186/300
13702/13702 [==============================] - 1s - loss: 8.2885e-05 - acc: 0.0000e+00 - val_loss: 2.8940e-04 - val_acc: 0.0000e+00
Epoch 187/300
13702/13702 [==============================] - 1s - loss: 8.3434e-05 - acc: 0.0000e+00 - val_loss: 2.6970e-04 - val_acc: 0.0000e+00
Epoch 188/300
13702/13702 [==============================] - 1s - loss: 7.7613e-05 - acc: 0.0000e+00 - val_loss: 1.3819e-04 - val_acc: 0.0000e+00
Epoch 189/300
13702/13702 [==============================] - 1s - loss: 8.6766e-05 - acc: 0.0000e+00 - val_loss: 1.5381e-04 - val_acc: 0.0000e+00
Epoch 190/300
13702/13702 [==============================] - 1s - loss: 8.5138e-05 - acc: 0.0000e+00 - val_loss: 3.1679e-04 - val_acc: 0.0000e+00 - ETA: 0s - loss: 8.8950e-05 - acc
Epoch 191/300
13702/13702 [==============================] - 1s - loss: 7.7643e-05 - acc: 0.0000e+00 - val_loss: 2.2191e-04 - val_acc: 0.0000e+00
Epoch 192/300
13702/13702 [==============================] - 1s - loss: 7.8531e-05 - acc: 0.0000e+00 - val_loss: 1.5878e-04 - val_acc: 0.0000e+000
Epoch 193/300
13702/13702 [==============================] - 1s - loss: 7.6755e-05 - acc: 0.0000e+00 - val_loss: 2.0950e-04 - val_acc: 0.0000e+00
Epoch 194/300
13702/13702 [==============================] - 1s - loss: 8.3127e-05 - acc: 0.0000e+00 - val_loss: 2.1628e-04 - val_acc: 0.0000e+00
Epoch 195/300
13702/13702 [==============================] - 1s - loss: 9.8357e-05 - acc: 0.0000e+00 - val_loss: 2.2033e-04 - val_acc: 0.0000e+00
Epoch 196/300
13702/13702 [==============================] - 1s - loss: 7.9893e-05 - acc: 0.0000e+00 - val_loss: 1.6198e-04 - val_acc: 0.0000e+00
Epoch 197/300
13702/13702 [==============================] - 1s - loss: 8.1980e-05 - acc: 0.0000e+00 - val_loss: 1.4816e-04 - val_acc: 0.0000e+00
Epoch 198/300
13702/13702 [==============================] - 1s - loss: 8.4150e-05 - acc: 0.0000e+00 - val_loss: 2.9659e-04 - val_acc: 0.0000e+00
Epoch 199/300
13702/13702 [==============================] - 1s - loss: 8.6184e-05 - acc: 0.0000e+00 - val_loss: 1.8203e-04 - val_acc: 0.0000e+00
Epoch 200/300
13702/13702 [==============================] - 1s - loss: 9.0429e-05 - acc: 0.0000e+00 - val_loss: 1.9676e-04 - val_acc: 0.0000e+00
Epoch 201/300
13702/13702 [==============================] - 1s - loss: 7.8432e-05 - acc: 0.0000e+00 - val_loss: 1.7621e-04 - val_acc: 0.0000e+00
Epoch 202/300
13702/13702 [==============================] - 1s - loss: 8.2099e-05 - acc: 0.0000e+00 - val_loss: 2.0047e-04 - val_acc: 0.0000e+00
Epoch 203/300
13702/13702 [==============================] - 1s - loss: 8.0038e-05 - acc: 0.0000e+00 - val_loss: 2.5995e-04 - val_acc: 0.0000e+00
Epoch 204/300
13702/13702 [==============================] - 1s - loss: 7.9615e-05 - acc: 0.0000e+00 - val_loss: 2.1900e-04 - val_acc: 0.0000e+00
Epoch 205/300
13702/13702 [==============================] - 1s - loss: 8.0319e-05 - acc: 0.0000e+00 - val_loss: 3.1494e-04 - val_acc: 0.0000e+00
Epoch 206/300
13702/13702 [==============================] - 1s - loss: 8.2696e-05 - acc: 0.0000e+00 - val_loss: 1.9053e-04 - val_acc: 0.0000e+00
Epoch 207/300
13702/13702 [==============================] - 1s - loss: 8.0010e-05 - acc: 0.0000e+00 - val_loss: 3.1766e-04 - val_acc: 0.0000e+00
Epoch 208/300
13702/13702 [==============================] - 1s - loss: 7.7783e-05 - acc: 0.0000e+00 - val_loss: 1.7325e-04 - val_acc: 0.0000e+00
Epoch 209/300
13702/13702 [==============================] - 1s - loss: 8.2373e-05 - acc: 0.0000e+00 - val_loss: 2.7009e-04 - val_acc: 0.0000e+00
Epoch 210/300
13702/13702 [==============================] - 1s - loss: 8.1575e-05 - acc: 0.0000e+00 - val_loss: 3.0607e-04 - val_acc: 0.0000e+00
Epoch 211/300
13702/13702 [==============================] - 1s - loss: 7.4362e-05 - acc: 0.0000e+00 - val_loss: 1.9781e-04 - val_acc: 0.0000e+00
Epoch 212/300
13702/13702 [==============================] - 1s - loss: 7.6505e-05 - acc: 0.0000e+00 - val_loss: 2.0563e-04 - val_acc: 0.0000e+00
Epoch 213/300
13702/13702 [==============================] - 1s - loss: 8.1572e-05 - acc: 0.0000e+00 - val_loss: 1.5519e-04 - val_acc: 0.0000e+00
Epoch 214/300
13702/13702 [==============================] - 1s - loss: 9.4095e-05 - acc: 0.0000e+00 - val_loss: 1.4218e-04 - val_acc: 0.0000e+00
Epoch 215/300
13702/13702 [==============================] - 1s - loss: 7.9674e-05 - acc: 0.0000e+00 - val_loss: 2.3033e-04 - val_acc: 0.0000e+00
Epoch 216/300
13702/13702 [==============================] - 1s - loss: 7.8890e-05 - acc: 0.0000e+00 - val_loss: 1.2473e-04 - val_acc: 0.0000e+00
Epoch 217/300
13702/13702 [==============================] - 1s - loss: 7.4772e-05 - acc: 0.0000e+00 - val_loss: 1.8648e-04 - val_acc: 0.0000e+00
Epoch 218/300
13702/13702 [==============================] - 1s - loss: 7.2462e-05 - acc: 0.0000e+00 - val_loss: 1.5368e-04 - val_acc: 0.0000e+00
Epoch 219/300
13702/13702 [==============================] - 1s - loss: 7.7712e-05 - acc: 0.0000e+00 - val_loss: 1.7582e-04 - val_acc: 0.0000e+00
Epoch 220/300
13702/13702 [==============================] - 1s - loss: 8.9960e-05 - acc: 0.0000e+00 - val_loss: 1.5407e-04 - val_acc: 0.0000e+00
Epoch 221/300
13702/13702 [==============================] - 2s - loss: 7.3900e-05 - acc: 0.0000e+00 - val_loss: 2.4410e-04 - val_acc: 0.0000e+00
Epoch 222/300
13702/13702 [==============================] - 1s - loss: 7.6905e-05 - acc: 0.0000e+00 - val_loss: 1.4138e-04 - val_acc: 0.0000e+00
Epoch 223/300
13702/13702 [==============================] - 1s - loss: 8.3777e-05 - acc: 0.0000e+00 - val_loss: 1.9402e-04 - val_acc: 0.0000e+00
Epoch 224/300
13702/13702 [==============================] - 1s - loss: 7.8529e-05 - acc: 0.0000e+00 - val_loss: 7.9249e-04 - val_acc: 0.0000e+00
Epoch 225/300
13702/13702 [==============================] - 1s - loss: 1.0925e-04 - acc: 0.0000e+00 - val_loss: 1.5116e-04 - val_acc: 0.0000e+00
Epoch 226/300
13702/13702 [==============================] - 1s - loss: 8.4016e-05 - acc: 0.0000e+00 - val_loss: 1.7025e-04 - val_acc: 0.0000e+00
Epoch 227/300
13702/13702 [==============================] - 1s - loss: 7.6481e-05 - acc: 0.0000e+00 - val_loss: 4.6258e-04 - val_acc: 0.0000e+00
Epoch 228/300
13702/13702 [==============================] - 1s - loss: 8.5338e-05 - acc: 0.0000e+00 - val_loss: 1.4189e-04 - val_acc: 0.0000e+00
Epoch 229/300
13702/13702 [==============================] - 1s - loss: 7.5524e-05 - acc: 0.0000e+00 - val_loss: 3.5414e-04 - val_acc: 0.0000e+00
Epoch 230/300
13702/13702 [==============================] - 1s - loss: 7.8513e-05 - acc: 0.0000e+00 - val_loss: 1.9358e-04 - val_acc: 0.0000e+00
Epoch 231/300
13702/13702 [==============================] - 1s - loss: 7.5735e-05 - acc: 0.0000e+00 - val_loss: 1.6353e-04 - val_acc: 0.0000e+00
Epoch 232/300
13702/13702 [==============================] - 1s - loss: 7.2876e-05 - acc: 0.0000e+00 - val_loss: 1.9740e-04 - val_acc: 0.0000e+0000
Epoch 233/300
13702/13702 [==============================] - 1s - loss: 7.4172e-05 - acc: 0.0000e+00 - val_loss: 2.1658e-04 - val_acc: 0.0000e+00
Epoch 234/300
13702/13702 [==============================] - 1s - loss: 7.9871e-05 - acc: 0.0000e+00 - val_loss: 1.4538e-04 - val_acc: 0.0000e+00
Epoch 235/300
13702/13702 [==============================] - 1s - loss: 7.4806e-05 - acc: 0.0000e+00 - val_loss: 3.2176e-04 - val_acc: 0.0000e+00
Epoch 236/300
13702/13702 [==============================] - 1s - loss: 7.1779e-05 - acc: 0.0000e+00 - val_loss: 2.0520e-04 - val_acc: 0.0000e+00
Epoch 237/300
13702/13702 [==============================] - 1s - loss: 7.3932e-05 - acc: 0.0000e+00 - val_loss: 1.8269e-04 - val_acc: 0.0000e+00
Epoch 238/300
13702/13702 [==============================] - 1s - loss: 7.0548e-05 - acc: 0.0000e+00 - val_loss: 1.4994e-04 - val_acc: 0.0000e+00
Epoch 239/300
13702/13702 [==============================] - 1s - loss: 7.6985e-05 - acc: 0.0000e+00 - val_loss: 2.8457e-04 - val_acc: 0.0000e+00
Epoch 240/300
13702/13702 [==============================] - 1s - loss: 8.2628e-05 - acc: 0.0000e+00 - val_loss: 2.2156e-04 - val_acc: 0.0000e+00
Epoch 241/300
13702/13702 [==============================] - 1s - loss: 8.1611e-05 - acc: 0.0000e+00 - val_loss: 1.6893e-04 - val_acc: 0.0000e+00
Epoch 242/300
13702/13702 [==============================] - 1s - loss: 7.4260e-05 - acc: 0.0000e+00 - val_loss: 2.5932e-04 - val_acc: 0.0000e+00
Epoch 243/300
13702/13702 [==============================] - 1s - loss: 7.7109e-05 - acc: 0.0000e+00 - val_loss: 3.8726e-04 - val_acc: 0.0000e+00
Epoch 244/300
13702/13702 [==============================] - 1s - loss: 7.1603e-05 - acc: 0.0000e+00 - val_loss: 1.8377e-04 - val_acc: 0.0000e+00
Epoch 245/300
13702/13702 [==============================] - 1s - loss: 7.4708e-05 - acc: 0.0000e+00 - val_loss: 1.4539e-04 - val_acc: 0.0000e+00
Epoch 246/300
13702/13702 [==============================] - 1s - loss: 7.3872e-05 - acc: 0.0000e+00 - val_loss: 1.5933e-04 - val_acc: 0.0000e+00
Epoch 247/300
13702/13702 [==============================] - 1s - loss: 7.2336e-05 - acc: 0.0000e+00 - val_loss: 3.3052e-04 - val_acc: 0.0000e+00
Epoch 248/300
13702/13702 [==============================] - 1s - loss: 7.9134e-05 - acc: 0.0000e+00 - val_loss: 1.7232e-04 - val_acc: 0.0000e+00
Epoch 249/300
13702/13702 [==============================] - 1s - loss: 7.8414e-05 - acc: 0.0000e+00 - val_loss: 5.2960e-04 - val_acc: 0.0000e+00
Epoch 250/300
13702/13702 [==============================] - 1s - loss: 7.3652e-05 - acc: 0.0000e+00 - val_loss: 7.1981e-04 - val_acc: 0.0000e+00
Epoch 251/300
13702/13702 [==============================] - 1s - loss: 7.8544e-05 - acc: 0.0000e+00 - val_loss: 1.9537e-04 - val_acc: 0.0000e+00
Epoch 252/300
13702/13702 [==============================] - 1s - loss: 7.2959e-05 - acc: 0.0000e+00 - val_loss: 3.6408e-04 - val_acc: 0.0000e+00
Epoch 253/300
13702/13702 [==============================] - 1s - loss: 6.6126e-05 - acc: 0.0000e+00 - val_loss: 2.6155e-04 - val_acc: 0.0000e+00
Epoch 254/300
13702/13702 [==============================] - 1s - loss: 6.7503e-05 - acc: 0.0000e+00 - val_loss: 1.8498e-04 - val_acc: 0.0000e+00
Epoch 255/300
13702/13702 [==============================] - 1s - loss: 7.1506e-05 - acc: 0.0000e+00 - val_loss: 1.9209e-04 - val_acc: 0.0000e+00
Epoch 256/300
13702/13702 [==============================] - 1s - loss: 6.8756e-05 - acc: 0.0000e+00 - val_loss: 2.7393e-04 - val_acc: 0.0000e+00
Epoch 257/300
13702/13702 [==============================] - 1s - loss: 8.1139e-05 - acc: 0.0000e+00 - val_loss: 4.4331e-04 - val_acc: 0.0000e+00
Epoch 258/300
13702/13702 [==============================] - 1s - loss: 7.7433e-05 - acc: 0.0000e+00 - val_loss: 1.9213e-04 - val_acc: 0.0000e+00
Epoch 259/300
13702/13702 [==============================] - 1s - loss: 7.3941e-05 - acc: 0.0000e+00 - val_loss: 1.5579e-04 - val_acc: 0.0000e+00
Epoch 260/300
13702/13702 [==============================] - 1s - loss: 7.4482e-05 - acc: 0.0000e+00 - val_loss: 4.4241e-04 - val_acc: 0.0000e+00
Epoch 261/300
13702/13702 [==============================] - 1s - loss: 8.0298e-05 - acc: 0.0000e+00 - val_loss: 2.7617e-04 - val_acc: 0.0000e+00
Epoch 262/300
13702/13702 [==============================] - 1s - loss: 7.6966e-05 - acc: 0.0000e+00 - val_loss: 2.7828e-04 - val_acc: 0.0000e+00
Epoch 263/300
13702/13702 [==============================] - 1s - loss: 7.5900e-05 - acc: 0.0000e+00 - val_loss: 1.7344e-04 - val_acc: 0.0000e+00
Epoch 264/300
13702/13702 [==============================] - 1s - loss: 7.3079e-05 - acc: 0.0000e+00 - val_loss: 4.6354e-04 - val_acc: 0.0000e+00
Epoch 265/300
13702/13702 [==============================] - 1s - loss: 7.1835e-05 - acc: 0.0000e+00 - val_loss: 2.8672e-04 - val_acc: 0.0000e+00
Epoch 266/300
13702/13702 [==============================] - 1s - loss: 7.4642e-05 - acc: 0.0000e+00 - val_loss: 3.2372e-04 - val_acc: 0.0000e+00
Epoch 267/300
13702/13702 [==============================] - 1s - loss: 7.4722e-05 - acc: 0.0000e+00 - val_loss: 6.5295e-04 - val_acc: 0.0000e+00
Epoch 268/300
13702/13702 [==============================] - 1s - loss: 7.5105e-05 - acc: 0.0000e+00 - val_loss: 1.8325e-04 - val_acc: 0.0000e+00
Epoch 269/300
13702/13702 [==============================] - 1s - loss: 7.1055e-05 - acc: 0.0000e+00 - val_loss: 2.6456e-04 - val_acc: 0.0000e+00
Epoch 270/300
13702/13702 [==============================] - 1s - loss: 6.7923e-05 - acc: 0.0000e+00 - val_loss: 2.3129e-04 - val_acc: 0.0000e+00
Epoch 271/300
13702/13702 [==============================] - 1s - loss: 6.9181e-05 - acc: 0.0000e+00 - val_loss: 2.7518e-04 - val_acc: 0.0000e+00
Epoch 272/300
13702/13702 [==============================] - 1s - loss: 7.3365e-05 - acc: 0.0000e+00 - val_loss: 3.3132e-04 - val_acc: 0.0000e+00
Epoch 273/300
13702/13702 [==============================] - 1s - loss: 6.9777e-05 - acc: 0.0000e+00 - val_loss: 1.6098e-04 - val_acc: 0.0000e+00
Epoch 274/300
13702/13702 [==============================] - 1s - loss: 7.0377e-05 - acc: 0.0000e+00 - val_loss: 2.4420e-04 - val_acc: 0.0000e+0000e+0
Epoch 275/300
13702/13702 [==============================] - 1s - loss: 7.4858e-05 - acc: 0.0000e+00 - val_loss: 1.4363e-04 - val_acc: 0.0000e+00
Epoch 276/300
13702/13702 [==============================] - 1s - loss: 8.0115e-05 - acc: 0.0000e+00 - val_loss: 2.0925e-04 - val_acc: 0.0000e+00
Epoch 277/300
13702/13702 [==============================] - 1s - loss: 7.1130e-05 - acc: 0.0000e+00 - val_loss: 1.2054e-04 - val_acc: 0.0000e+00
Epoch 278/300
13702/13702 [==============================] - 1s - loss: 6.5693e-05 - acc: 0.0000e+00 - val_loss: 2.3336e-04 - val_acc: 0.0000e+00
Epoch 279/300
13702/13702 [==============================] - 1s - loss: 6.7777e-05 - acc: 0.0000e+00 - val_loss: 1.7634e-04 - val_acc: 0.0000e+00
Epoch 280/300
13702/13702 [==============================] - 1s - loss: 7.8940e-05 - acc: 0.0000e+00 - val_loss: 2.9703e-04 - val_acc: 0.0000e+00
Epoch 281/300
13702/13702 [==============================] - 1s - loss: 7.2386e-05 - acc: 0.0000e+00 - val_loss: 1.3204e-04 - val_acc: 0.0000e+00
Epoch 282/300
13702/13702 [==============================] - 1s - loss: 6.9240e-05 - acc: 0.0000e+00 - val_loss: 3.1336e-04 - val_acc: 0.0000e+00
Epoch 283/300
13702/13702 [==============================] - 1s - loss: 7.0363e-05 - acc: 0.0000e+00 - val_loss: 2.3485e-04 - val_acc: 0.0000e+00
Epoch 284/300
13702/13702 [==============================] - 1s - loss: 6.8012e-05 - acc: 0.0000e+00 - val_loss: 3.1219e-04 - val_acc: 0.0000e+00
Epoch 285/300
13702/13702 [==============================] - 1s - loss: 6.6163e-05 - acc: 0.0000e+00 - val_loss: 3.6892e-04 - val_acc: 0.0000e+00
Epoch 286/300
13702/13702 [==============================] - 1s - loss: 6.3121e-05 - acc: 0.0000e+00 - val_loss: 4.4599e-04 - val_acc: 0.0000e+00
Epoch 287/300
13702/13702 [==============================] - 1s - loss: 7.0318e-05 - acc: 0.0000e+00 - val_loss: 2.9425e-04 - val_acc: 0.0000e+00
Epoch 288/300
13702/13702 [==============================] - 1s - loss: 7.0926e-05 - acc: 0.0000e+00 - val_loss: 7.8019e-04 - val_acc: 0.0000e+00
Epoch 289/300
13702/13702 [==============================] - 1s - loss: 7.0558e-05 - acc: 0.0000e+00 - val_loss: 2.0309e-04 - val_acc: 0.0000e+00
Epoch 290/300
13702/13702 [==============================] - 1s - loss: 6.9708e-05 - acc: 0.0000e+00 - val_loss: 2.5192e-04 - val_acc: 0.0000e+00
Epoch 291/300
13702/13702 [==============================] - 1s - loss: 6.5409e-05 - acc: 0.0000e+00 - val_loss: 3.6990e-04 - val_acc: 0.0000e+00
Epoch 292/300
13702/13702 [==============================] - 1s - loss: 6.3968e-05 - acc: 0.0000e+00 - val_loss: 1.5363e-04 - val_acc: 0.0000e+00
Epoch 293/300
13702/13702 [==============================] - 1s - loss: 6.8142e-05 - acc: 0.0000e+00 - val_loss: 1.5397e-04 - val_acc: 0.0000e+00
Epoch 294/300
13702/13702 [==============================] - 1s - loss: 7.3668e-05 - acc: 0.0000e+00 - val_loss: 1.4684e-04 - val_acc: 0.0000e+00
Epoch 295/300
13702/13702 [==============================] - 1s - loss: 7.0757e-05 - acc: 0.0000e+00 - val_loss: 2.9196e-04 - val_acc: 0.0000e+00
Epoch 296/300
13702/13702 [==============================] - 1s - loss: 6.2730e-05 - acc: 0.0000e+00 - val_loss: 2.6135e-04 - val_acc: 0.0000e+00
Epoch 297/300
13702/13702 [==============================] - 1s - loss: 6.6917e-05 - acc: 0.0000e+00 - val_loss: 6.9356e-04 - val_acc: 0.0000e+00
Epoch 298/300
13702/13702 [==============================] - 1s - loss: 7.6131e-05 - acc: 0.0000e+00 - val_loss: 3.0078e-04 - val_acc: 0.0000e+00
Epoch 299/300
13702/13702 [==============================] - 1s - loss: 6.6266e-05 - acc: 0.0000e+00 - val_loss: 3.5602e-04 - val_acc: 0.0000e+00
Epoch 300/300
13702/13702 [==============================] - 1s - loss: 6.3080e-05 - acc: 0.0000e+00 - val_loss: 4.2830e-04 - val_acc: 0.0000e+00
Train Score: 0.00008 MSE (0.01 RMSE)
Test Score: 0.03805 MSE (0.20 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_27 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_27 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_28 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_28 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_27 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_28 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/300
13702/13702 [==============================] - 3s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00
Epoch 2/300
13702/13702 [==============================] - 1s - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 3/300
13702/13702 [==============================] - 1s - loss: 8.8205e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 4/300
13702/13702 [==============================] - 1s - loss: 8.0464e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 5/300
13702/13702 [==============================] - 1s - loss: 7.0607e-04 - acc: 0.0000e+00 - val_loss: 3.5865e-04 - val_acc: 0.0000e+00
Epoch 6/300
13702/13702 [==============================] - 1s - loss: 7.4529e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 7/300
13702/13702 [==============================] - 1s - loss: 6.6446e-04 - acc: 0.0000e+00 - val_loss: 4.7940e-04 - val_acc: 0.0000e+00
Epoch 8/300
13702/13702 [==============================] - 1s - loss: 7.0687e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 9/300
13702/13702 [==============================] - 1s - loss: 6.8588e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 10/300
13702/13702 [==============================] - 1s - loss: 6.2296e-04 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00
Epoch 11/300
13702/13702 [==============================] - 1s - loss: 6.3806e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 12/300
13702/13702 [==============================] - 1s - loss: 6.0633e-04 - acc: 0.0000e+00 - val_loss: 2.8050e-04 - val_acc: 0.0000e+00
Epoch 13/300
13702/13702 [==============================] - 1s - loss: 5.5517e-04 - acc: 0.0000e+00 - val_loss: 2.7580e-04 - val_acc: 0.0000e+00
Epoch 14/300
13702/13702 [==============================] - 1s - loss: 5.9202e-04 - acc: 0.0000e+00 - val_loss: 3.9986e-04 - val_acc: 0.0000e+00
Epoch 15/300
13702/13702 [==============================] - 1s - loss: 5.6628e-04 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00
Epoch 16/300
13702/13702 [==============================] - 1s - loss: 6.9721e-04 - acc: 0.0000e+00 - val_loss: 0.0059 - val_acc: 0.0000e+00
Epoch 17/300
13702/13702 [==============================] - 1s - loss: 5.5008e-04 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00
Epoch 18/300
13702/13702 [==============================] - 1s - loss: 5.1979e-04 - acc: 0.0000e+00 - val_loss: 3.8112e-04 - val_acc: 0.0000e+00
Epoch 19/300
13702/13702 [==============================] - 1s - loss: 4.9234e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 20/300
13702/13702 [==============================] - 1s - loss: 4.7317e-04 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00
Epoch 21/300
13702/13702 [==============================] - 1s - loss: 4.6351e-04 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00
Epoch 22/300
13702/13702 [==============================] - 1s - loss: 4.8662e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 23/300
13702/13702 [==============================] - 1s - loss: 4.5858e-04 - acc: 0.0000e+00 - val_loss: 9.9145e-04 - val_acc: 0.0000e+00
Epoch 24/300
13702/13702 [==============================] - 1s - loss: 4.1628e-04 - acc: 0.0000e+00 - val_loss: 3.7363e-04 - val_acc: 0.0000e+00
Epoch 25/300
13702/13702 [==============================] - 1s - loss: 4.0881e-04 - acc: 0.0000e+00 - val_loss: 7.8377e-04 - val_acc: 0.0000e+00
Epoch 26/300
13702/13702 [==============================] - 1s - loss: 3.9096e-04 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00
Epoch 27/300
13702/13702 [==============================] - 1s - loss: 4.2150e-04 - acc: 0.0000e+00 - val_loss: 8.9764e-04 - val_acc: 0.0000e+00
Epoch 28/300
13702/13702 [==============================] - 1s - loss: 3.8239e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 29/300
13702/13702 [==============================] - 1s - loss: 3.7979e-04 - acc: 0.0000e+00 - val_loss: 3.0652e-04 - val_acc: 0.0000e+00
Epoch 30/300
13702/13702 [==============================] - 1s - loss: 3.4510e-04 - acc: 0.0000e+00 - val_loss: 2.9828e-04 - val_acc: 0.0000e+00
Epoch 31/300
13702/13702 [==============================] - 1s - loss: 3.5371e-04 - acc: 0.0000e+00 - val_loss: 2.3553e-04 - val_acc: 0.0000e+00
Epoch 32/300
13702/13702 [==============================] - 1s - loss: 3.2689e-04 - acc: 0.0000e+00 - val_loss: 2.5611e-04 - val_acc: 0.0000e+00
Epoch 33/300
13702/13702 [==============================] - 1s - loss: 3.3701e-04 - acc: 0.0000e+00 - val_loss: 2.6443e-04 - val_acc: 0.0000e+00
Epoch 34/300
13702/13702 [==============================] - 1s - loss: 3.0594e-04 - acc: 0.0000e+00 - val_loss: 7.7305e-04 - val_acc: 0.0000e+00
Epoch 35/300
13702/13702 [==============================] - 1s - loss: 2.9524e-04 - acc: 0.0000e+00 - val_loss: 4.0189e-04 - val_acc: 0.0000e+00
Epoch 36/300
13702/13702 [==============================] - 1s - loss: 3.1088e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 37/300
13702/13702 [==============================] - 1s - loss: 2.7307e-04 - acc: 0.0000e+00 - val_loss: 2.8868e-04 - val_acc: 0.0000e+0000e
Epoch 38/300
13702/13702 [==============================] - 1s - loss: 2.7804e-04 - acc: 0.0000e+00 - val_loss: 2.8125e-04 - val_acc: 0.0000e+00
Epoch 39/300
13702/13702 [==============================] - 1s - loss: 2.6431e-04 - acc: 0.0000e+00 - val_loss: 4.4726e-04 - val_acc: 0.0000e+00
Epoch 40/300
13702/13702 [==============================] - 1s - loss: 2.4018e-04 - acc: 0.0000e+00 - val_loss: 2.6722e-04 - val_acc: 0.0000e+00
Epoch 41/300
13702/13702 [==============================] - 1s - loss: 2.6175e-04 - acc: 0.0000e+00 - val_loss: 5.7507e-04 - val_acc: 0.0000e+00
Epoch 42/300
13702/13702 [==============================] - 1s - loss: 2.3272e-04 - acc: 0.0000e+00 - val_loss: 8.0673e-04 - val_acc: 0.0000e+00
Epoch 43/300
13702/13702 [==============================] - 1s - loss: 2.3516e-04 - acc: 0.0000e+00 - val_loss: 4.6058e-04 - val_acc: 0.0000e+00
Epoch 44/300
13702/13702 [==============================] - 1s - loss: 2.2931e-04 - acc: 0.0000e+00 - val_loss: 3.1342e-04 - val_acc: 0.0000e+00
Epoch 45/300
13702/13702 [==============================] - 1s - loss: 2.5078e-04 - acc: 0.0000e+00 - val_loss: 8.1869e-04 - val_acc: 0.0000e+00
Epoch 46/300
13702/13702 [==============================] - 1s - loss: 2.5140e-04 - acc: 0.0000e+00 - val_loss: 5.7557e-04 - val_acc: 0.0000e+00
Epoch 47/300
13702/13702 [==============================] - 1s - loss: 2.2200e-04 - acc: 0.0000e+00 - val_loss: 4.8446e-04 - val_acc: 0.0000e+00
Epoch 48/300
13702/13702 [==============================] - 1s - loss: 2.3785e-04 - acc: 0.0000e+00 - val_loss: 5.3462e-04 - val_acc: 0.0000e+00
Epoch 49/300
13702/13702 [==============================] - 1s - loss: 2.2596e-04 - acc: 0.0000e+00 - val_loss: 3.3617e-04 - val_acc: 0.0000e+00
Epoch 50/300
13702/13702 [==============================] - 1s - loss: 1.9799e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 51/300
13702/13702 [==============================] - 1s - loss: 1.9827e-04 - acc: 0.0000e+00 - val_loss: 5.7664e-04 - val_acc: 0.0000e+00
Epoch 52/300
13702/13702 [==============================] - 1s - loss: 1.8755e-04 - acc: 0.0000e+00 - val_loss: 3.6004e-04 - val_acc: 0.0000e+00
Epoch 53/300
13702/13702 [==============================] - 1s - loss: 1.9766e-04 - acc: 0.0000e+00 - val_loss: 2.9210e-04 - val_acc: 0.0000e+00
Epoch 54/300
13702/13702 [==============================] - 1s - loss: 1.9642e-04 - acc: 0.0000e+00 - val_loss: 3.9535e-04 - val_acc: 0.0000e+00
Epoch 55/300
13702/13702 [==============================] - 1s - loss: 2.1163e-04 - acc: 0.0000e+00 - val_loss: 4.8770e-04 - val_acc: 0.0000e+00
Epoch 56/300
13702/13702 [==============================] - 1s - loss: 1.8221e-04 - acc: 0.0000e+00 - val_loss: 3.6152e-04 - val_acc: 0.0000e+00
Epoch 57/300
13702/13702 [==============================] - 1s - loss: 1.7873e-04 - acc: 0.0000e+00 - val_loss: 3.9166e-04 - val_acc: 0.0000e+00
Epoch 58/300
13702/13702 [==============================] - 1s - loss: 1.7510e-04 - acc: 0.0000e+00 - val_loss: 3.8632e-04 - val_acc: 0.0000e+00
Epoch 59/300
13702/13702 [==============================] - 1s - loss: 1.7604e-04 - acc: 0.0000e+00 - val_loss: 4.3733e-04 - val_acc: 0.0000e+00
Epoch 60/300
13702/13702 [==============================] - 1s - loss: 1.8173e-04 - acc: 0.0000e+00 - val_loss: 6.8579e-04 - val_acc: 0.0000e+00
Epoch 61/300
13702/13702 [==============================] - 1s - loss: 1.8676e-04 - acc: 0.0000e+00 - val_loss: 8.9812e-04 - val_acc: 0.0000e+0000e+0
Epoch 62/300
13702/13702 [==============================] - 1s - loss: 1.8157e-04 - acc: 0.0000e+00 - val_loss: 3.0229e-04 - val_acc: 0.0000e+00
Epoch 63/300
13702/13702 [==============================] - 1s - loss: 1.7360e-04 - acc: 0.0000e+00 - val_loss: 7.4918e-04 - val_acc: 0.0000e+00
Epoch 64/300
13702/13702 [==============================] - 1s - loss: 1.6384e-04 - acc: 0.0000e+00 - val_loss: 7.8085e-04 - val_acc: 0.0000e+00
Epoch 65/300
13702/13702 [==============================] - 1s - loss: 1.6542e-04 - acc: 0.0000e+00 - val_loss: 3.3013e-04 - val_acc: 0.0000e+00
Epoch 66/300
13702/13702 [==============================] - 1s - loss: 1.7062e-04 - acc: 0.0000e+00 - val_loss: 4.4724e-04 - val_acc: 0.0000e+00
Epoch 67/300
13702/13702 [==============================] - 1s - loss: 1.6574e-04 - acc: 0.0000e+00 - val_loss: 5.9483e-04 - val_acc: 0.0000e+00
Epoch 68/300
13702/13702 [==============================] - 1s - loss: 1.5794e-04 - acc: 0.0000e+00 - val_loss: 6.8203e-04 - val_acc: 0.0000e+00
Epoch 69/300
13702/13702 [==============================] - 1s - loss: 1.5336e-04 - acc: 0.0000e+00 - val_loss: 6.0593e-04 - val_acc: 0.0000e+00
Epoch 70/300
13702/13702 [==============================] - 1s - loss: 1.5521e-04 - acc: 0.0000e+00 - val_loss: 5.1038e-04 - val_acc: 0.0000e+00
Epoch 71/300
13702/13702 [==============================] - 1s - loss: 1.5413e-04 - acc: 0.0000e+00 - val_loss: 6.2599e-04 - val_acc: 0.0000e+00
Epoch 72/300
13702/13702 [==============================] - 1s - loss: 1.6657e-04 - acc: 0.0000e+00 - val_loss: 3.1592e-04 - val_acc: 0.0000e+00
Epoch 73/300
13702/13702 [==============================] - 1s - loss: 1.6380e-04 - acc: 0.0000e+00 - val_loss: 2.7866e-04 - val_acc: 0.0000e+00
Epoch 74/300
13702/13702 [==============================] - 1s - loss: 1.5932e-04 - acc: 0.0000e+00 - val_loss: 3.1835e-04 - val_acc: 0.0000e+00
Epoch 75/300
13702/13702 [==============================] - 1s - loss: 1.5005e-04 - acc: 0.0000e+00 - val_loss: 4.3268e-04 - val_acc: 0.0000e+00
Epoch 76/300
13702/13702 [==============================] - 1s - loss: 1.4746e-04 - acc: 0.0000e+00 - val_loss: 7.5370e-04 - val_acc: 0.0000e+00
Epoch 77/300
13702/13702 [==============================] - 1s - loss: 1.5394e-04 - acc: 0.0000e+00 - val_loss: 3.2292e-04 - val_acc: 0.0000e+00
Epoch 78/300
13702/13702 [==============================] - 1s - loss: 1.5239e-04 - acc: 0.0000e+00 - val_loss: 5.2693e-04 - val_acc: 0.0000e+00
Epoch 79/300
13702/13702 [==============================] - 1s - loss: 1.5191e-04 - acc: 0.0000e+00 - val_loss: 9.4634e-04 - val_acc: 0.0000e+00
Epoch 80/300
13702/13702 [==============================] - 1s - loss: 1.5957e-04 - acc: 0.0000e+00 - val_loss: 6.2523e-04 - val_acc: 0.0000e+00
Epoch 81/300
13702/13702 [==============================] - 1s - loss: 1.4833e-04 - acc: 0.0000e+00 - val_loss: 8.2942e-04 - val_acc: 0.0000e+00
Epoch 82/300
13702/13702 [==============================] - 1s - loss: 1.4711e-04 - acc: 0.0000e+00 - val_loss: 9.1043e-04 - val_acc: 0.0000e+00
Epoch 83/300
13702/13702 [==============================] - 1s - loss: 1.4169e-04 - acc: 0.0000e+00 - val_loss: 5.8362e-04 - val_acc: 0.0000e+00
Epoch 84/300
13702/13702 [==============================] - 1s - loss: 1.4260e-04 - acc: 0.0000e+00 - val_loss: 9.9053e-04 - val_acc: 0.0000e+00
Epoch 85/300
13702/13702 [==============================] - 1s - loss: 1.3548e-04 - acc: 0.0000e+00 - val_loss: 8.2100e-04 - val_acc: 0.0000e+00
Epoch 86/300
13702/13702 [==============================] - 1s - loss: 1.3962e-04 - acc: 0.0000e+00 - val_loss: 3.3916e-04 - val_acc: 0.0000e+00
Epoch 87/300
13702/13702 [==============================] - 1s - loss: 1.4152e-04 - acc: 0.0000e+00 - val_loss: 4.3366e-04 - val_acc: 0.0000e+00
Epoch 88/300
13702/13702 [==============================] - 1s - loss: 1.3337e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 89/300
13702/13702 [==============================] - 1s - loss: 1.3334e-04 - acc: 0.0000e+00 - val_loss: 6.9874e-04 - val_acc: 0.0000e+00
Epoch 90/300
13702/13702 [==============================] - 1s - loss: 1.4250e-04 - acc: 0.0000e+00 - val_loss: 8.5213e-04 - val_acc: 0.0000e+00
Epoch 91/300
13702/13702 [==============================] - 1s - loss: 1.3242e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 92/300
13702/13702 [==============================] - 1s - loss: 1.2681e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 93/300
13702/13702 [==============================] - 1s - loss: 1.2801e-04 - acc: 0.0000e+00 - val_loss: 7.6735e-04 - val_acc: 0.0000e+00
Epoch 94/300
13702/13702 [==============================] - 1s - loss: 1.3938e-04 - acc: 0.0000e+00 - val_loss: 5.2901e-04 - val_acc: 0.0000e+00
Epoch 95/300
13702/13702 [==============================] - 1s - loss: 1.3808e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 96/300
13702/13702 [==============================] - 1s - loss: 1.4154e-04 - acc: 0.0000e+00 - val_loss: 9.8377e-04 - val_acc: 0.0000e+00
Epoch 97/300
13702/13702 [==============================] - 1s - loss: 1.3216e-04 - acc: 0.0000e+00 - val_loss: 9.2387e-04 - val_acc: 0.0000e+00
Epoch 98/300
13702/13702 [==============================] - 1s - loss: 1.3907e-04 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00
Epoch 99/300
13702/13702 [==============================] - 1s - loss: 1.5309e-04 - acc: 0.0000e+00 - val_loss: 8.1559e-04 - val_acc: 0.0000e+00
Epoch 100/300
13702/13702 [==============================] - 1s - loss: 1.2758e-04 - acc: 0.0000e+00 - val_loss: 5.3400e-04 - val_acc: 0.0000e+00
Epoch 101/300
13702/13702 [==============================] - 1s - loss: 1.3170e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 102/300
13702/13702 [==============================] - 1s - loss: 1.2112e-04 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00
Epoch 103/300
13702/13702 [==============================] - 1s - loss: 1.2947e-04 - acc: 0.0000e+00 - val_loss: 8.5040e-04 - val_acc: 0.0000e+00
Epoch 104/300
13702/13702 [==============================] - 1s - loss: 1.3201e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 105/300
13702/13702 [==============================] - 1s - loss: 1.2085e-04 - acc: 0.0000e+00 - val_loss: 9.3465e-04 - val_acc: 0.0000e+00
Epoch 106/300
13702/13702 [==============================] - 1s - loss: 1.2332e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 107/300
13702/13702 [==============================] - 1s - loss: 1.1724e-04 - acc: 0.0000e+00 - val_loss: 7.0489e-04 - val_acc: 0.0000e+00
Epoch 108/300
13702/13702 [==============================] - 1s - loss: 1.2396e-04 - acc: 0.0000e+00 - val_loss: 9.8016e-04 - val_acc: 0.0000e+00
Epoch 109/300
13702/13702 [==============================] - 1s - loss: 1.2234e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 110/300
13702/13702 [==============================] - 1s - loss: 1.1883e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+000.000
Epoch 111/300
13702/13702 [==============================] - 1s - loss: 1.2514e-04 - acc: 0.0000e+00 - val_loss: 7.8593e-04 - val_acc: 0.0000e+00
Epoch 112/300
13702/13702 [==============================] - 1s - loss: 1.2188e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 113/300
13702/13702 [==============================] - 2s - loss: 1.3652e-04 - acc: 0.0000e+00 - val_loss: 6.2520e-04 - val_acc: 0.0000e+00
Epoch 114/300
13702/13702 [==============================] - 1s - loss: 1.1921e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+000.
Epoch 115/300
13702/13702 [==============================] - 1s - loss: 1.1932e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 116/300
13702/13702 [==============================] - 1s - loss: 1.2542e-04 - acc: 0.0000e+00 - val_loss: 6.3308e-04 - val_acc: 0.0000e+00
Epoch 117/300
13702/13702 [==============================] - 2s - loss: 1.4133e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 118/300
13702/13702 [==============================] - 1s - loss: 1.3027e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 119/300
13702/13702 [==============================] - 1s - loss: 1.1767e-04 - acc: 0.0000e+00 - val_loss: 6.0836e-04 - val_acc: 0.0000e+00
Epoch 120/300
13702/13702 [==============================] - 1s - loss: 1.1301e-04 - acc: 0.0000e+00 - val_loss: 7.9080e-04 - val_acc: 0.0000e+00
Epoch 121/300
13702/13702 [==============================] - 2s - loss: 1.0753e-04 - acc: 0.0000e+00 - val_loss: 9.8405e-04 - val_acc: 0.0000e+00
Epoch 122/300
13702/13702 [==============================] - 1s - loss: 1.1788e-04 - acc: 0.0000e+00 - val_loss: 3.8826e-04 - val_acc: 0.0000e+00
Epoch 123/300
13702/13702 [==============================] - 1s - loss: 1.2423e-04 - acc: 0.0000e+00 - val_loss: 9.6733e-04 - val_acc: 0.0000e+00
Epoch 124/300
13702/13702 [==============================] - 1s - loss: 1.2056e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+000
Epoch 125/300
13702/13702 [==============================] - 1s - loss: 1.1005e-04 - acc: 0.0000e+00 - val_loss: 6.2626e-04 - val_acc: 0.0000e+00
Epoch 126/300
13702/13702 [==============================] - 1s - loss: 1.1821e-04 - acc: 0.0000e+00 - val_loss: 4.8729e-04 - val_acc: 0.0000e+00
Epoch 127/300
13702/13702 [==============================] - 1s - loss: 1.1375e-04 - acc: 0.0000e+00 - val_loss: 9.9913e-04 - val_acc: 0.0000e+00
Epoch 128/300
13702/13702 [==============================] - 1s - loss: 1.1242e-04 - acc: 0.0000e+00 - val_loss: 6.5294e-04 - val_acc: 0.0000e+00
Epoch 129/300
13702/13702 [==============================] - 1s - loss: 1.3210e-04 - acc: 0.0000e+00 - val_loss: 5.4471e-04 - val_acc: 0.0000e+00
Epoch 130/300
13702/13702 [==============================] - 1s - loss: 1.1742e-04 - acc: 0.0000e+00 - val_loss: 9.6405e-04 - val_acc: 0.0000e+00
Epoch 131/300
13702/13702 [==============================] - 1s - loss: 1.1917e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 132/300
13702/13702 [==============================] - 1s - loss: 1.1449e-04 - acc: 0.0000e+00 - val_loss: 9.2000e-04 - val_acc: 0.0000e+00
Epoch 133/300
13702/13702 [==============================] - 1s - loss: 1.0865e-04 - acc: 0.0000e+00 - val_loss: 8.0901e-04 - val_acc: 0.0000e+00
Epoch 134/300
13702/13702 [==============================] - 1s - loss: 1.0866e-04 - acc: 0.0000e+00 - val_loss: 9.9810e-04 - val_acc: 0.0000e+0000e - ETA: 1s - loss: 1.0834e-04 - - ETA: 0s - loss: 1.0946e-04 - acc: 0.0000e+0
Epoch 135/300
13702/13702 [==============================] - 1s - loss: 1.1446e-04 - acc: 0.0000e+00 - val_loss: 8.6315e-04 - val_acc: 0.0000e+00
Epoch 136/300
13702/13702 [==============================] - 1s - loss: 1.0892e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 137/300
13702/13702 [==============================] - 1s - loss: 1.1534e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 138/300
13702/13702 [==============================] - 1s - loss: 1.1257e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 139/300
13702/13702 [==============================] - 1s - loss: 1.1944e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 140/300
13702/13702 [==============================] - 1s - loss: 1.1707e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 141/300
13702/13702 [==============================] - 1s - loss: 1.1846e-04 - acc: 0.0000e+00 - val_loss: 5.2556e-04 - val_acc: 0.0000e+00
Epoch 142/300
13702/13702 [==============================] - 1s - loss: 1.1968e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 143/300
13702/13702 [==============================] - 1s - loss: 1.0702e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 144/300
13702/13702 [==============================] - 1s - loss: 1.0427e-04 - acc: 0.0000e+00 - val_loss: 9.3992e-04 - val_acc: 0.0000e+00
Epoch 145/300
13702/13702 [==============================] - 1s - loss: 1.0374e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 146/300
13702/13702 [==============================] - 1s - loss: 1.0725e-04 - acc: 0.0000e+00 - val_loss: 8.9807e-04 - val_acc: 0.0000e+00
Epoch 147/300
13702/13702 [==============================] - 1s - loss: 1.0906e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 148/300
13702/13702 [==============================] - 1s - loss: 1.1253e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 149/300
13702/13702 [==============================] - 1s - loss: 1.0869e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 150/300
13702/13702 [==============================] - 1s - loss: 1.1532e-04 - acc: 0.0000e+00 - val_loss: 8.7471e-04 - val_acc: 0.0000e+00
Epoch 151/300
13702/13702 [==============================] - 1s - loss: 1.1143e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 152/300
13702/13702 [==============================] - 1s - loss: 1.1022e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 153/300
13702/13702 [==============================] - 1s - loss: 1.0660e-04 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 154/300
13702/13702 [==============================] - 1s - loss: 1.2214e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 155/300
13702/13702 [==============================] - 1s - loss: 1.0754e-04 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 156/300
13702/13702 [==============================] - 1s - loss: 1.1264e-04 - acc: 0.0000e+00 - val_loss: 8.8631e-04 - val_acc: 0.0000e+00
Epoch 157/300
13702/13702 [==============================] - 1s - loss: 1.1225e-04 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00
Epoch 158/300
13702/13702 [==============================] - 1s - loss: 1.1725e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 159/300
13702/13702 [==============================] - 1s - loss: 1.0691e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 160/300
13702/13702 [==============================] - 1s - loss: 1.0652e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 161/300
13702/13702 [==============================] - 1s - loss: 1.0658e-04 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 162/300
13702/13702 [==============================] - 1s - loss: 1.0331e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 163/300
13702/13702 [==============================] - 1s - loss: 1.1288e-04 - acc: 0.0000e+00 - val_loss: 8.6782e-04 - val_acc: 0.0000e+00
Epoch 164/300
13702/13702 [==============================] - 1s - loss: 1.0991e-04 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 165/300
13702/13702 [==============================] - 1s - loss: 1.0435e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 166/300
13702/13702 [==============================] - 1s - loss: 1.0656e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 167/300
13702/13702 [==============================] - 1s - loss: 1.1167e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 168/300
13702/13702 [==============================] - 1s - loss: 1.0109e-04 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 169/300
13702/13702 [==============================] - 1s - loss: 1.0841e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 170/300
13702/13702 [==============================] - 1s - loss: 1.1717e-04 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00
Epoch 171/300
13702/13702 [==============================] - 1s - loss: 1.2765e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 172/300
13702/13702 [==============================] - 1s - loss: 1.0331e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 173/300
13702/13702 [==============================] - 1s - loss: 1.0610e-04 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 174/300
13702/13702 [==============================] - 1s - loss: 9.8750e-05 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+000.0000e+ - ETA: 0s - loss: 9.3632e-05 - a
Epoch 175/300
13702/13702 [==============================] - 1s - loss: 9.9783e-05 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 176/300
13702/13702 [==============================] - 1s - loss: 1.0282e-04 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00
Epoch 177/300
13702/13702 [==============================] - 1s - loss: 1.0446e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 178/300
13702/13702 [==============================] - 1s - loss: 1.0175e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 179/300
13702/13702 [==============================] - 1s - loss: 1.0271e-04 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 180/300
13702/13702 [==============================] - 1s - loss: 1.0110e-04 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 181/300
13702/13702 [==============================] - 1s - loss: 9.6696e-05 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00
Epoch 182/300
13702/13702 [==============================] - 1s - loss: 9.7067e-05 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00
Epoch 183/300
13702/13702 [==============================] - 1s - loss: 1.0208e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 184/300
13702/13702 [==============================] - 1s - loss: 1.0890e-04 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00
Epoch 185/300
13702/13702 [==============================] - 1s - loss: 1.0187e-04 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00
Epoch 186/300
13702/13702 [==============================] - 1s - loss: 1.1207e-04 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00
Epoch 187/300
13702/13702 [==============================] - 1s - loss: 1.1059e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 188/300
13702/13702 [==============================] - 1s - loss: 1.0803e-04 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00
Epoch 189/300
13702/13702 [==============================] - 1s - loss: 1.0134e-04 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+000.000
Epoch 190/300
13702/13702 [==============================] - 1s - loss: 1.0001e-04 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00
Epoch 191/300
13702/13702 [==============================] - 1s - loss: 9.8220e-05 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 192/300
13702/13702 [==============================] - 1s - loss: 9.8360e-05 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00
Epoch 193/300
13702/13702 [==============================] - 1s - loss: 1.0382e-04 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00
Epoch 194/300
13702/13702 [==============================] - 1s - loss: 9.5931e-05 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 195/300
13702/13702 [==============================] - 1s - loss: 1.0139e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 196/300
13702/13702 [==============================] - 1s - loss: 9.8990e-05 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 197/300
13702/13702 [==============================] - 1s - loss: 9.5385e-05 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 198/300
13702/13702 [==============================] - 1s - loss: 1.0146e-04 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00
Epoch 199/300
13702/13702 [==============================] - 1s - loss: 1.0006e-04 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 200/300
13702/13702 [==============================] - 1s - loss: 9.3053e-05 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 201/300
13702/13702 [==============================] - 1s - loss: 9.8905e-05 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 202/300
13702/13702 [==============================] - 1s - loss: 1.0692e-04 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 203/300
13702/13702 [==============================] - 1s - loss: 9.4885e-05 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 204/300
13702/13702 [==============================] - 1s - loss: 9.8841e-05 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 205/300
13702/13702 [==============================] - 1s - loss: 1.0503e-04 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00
Epoch 206/300
13702/13702 [==============================] - 1s - loss: 9.4537e-05 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00
Epoch 207/300
13702/13702 [==============================] - 1s - loss: 1.0402e-04 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00
Epoch 208/300
13702/13702 [==============================] - 1s - loss: 9.5872e-05 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00
Epoch 209/300
13702/13702 [==============================] - 1s - loss: 9.4978e-05 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00
Epoch 210/300
13702/13702 [==============================] - 1s - loss: 9.3475e-05 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00
Epoch 211/300
13702/13702 [==============================] - 1s - loss: 9.6428e-05 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 212/300
13702/13702 [==============================] - 1s - loss: 9.7085e-05 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00
Epoch 213/300
13702/13702 [==============================] - 1s - loss: 9.7078e-05 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00
Epoch 214/300
13702/13702 [==============================] - 1s - loss: 9.0278e-05 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00
Epoch 215/300
13702/13702 [==============================] - 1s - loss: 9.5531e-05 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 216/300
13702/13702 [==============================] - 1s - loss: 9.9501e-05 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00
Epoch 217/300
13702/13702 [==============================] - 1s - loss: 9.8626e-05 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 218/300
13702/13702 [==============================] - 1s - loss: 9.1751e-05 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00
Epoch 219/300
13702/13702 [==============================] - 1s - loss: 9.6242e-05 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 220/300
13702/13702 [==============================] - 1s - loss: 9.2380e-05 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00
Epoch 221/300
13702/13702 [==============================] - 1s - loss: 9.3692e-05 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00
Epoch 222/300
13702/13702 [==============================] - 1s - loss: 9.3870e-05 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 223/300
13702/13702 [==============================] - 1s - loss: 1.0095e-04 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00
Epoch 224/300
13702/13702 [==============================] - 1s - loss: 9.6934e-05 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 225/300
13702/13702 [==============================] - 1s - loss: 9.0423e-05 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00
Epoch 226/300
13702/13702 [==============================] - 1s - loss: 1.0342e-04 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00
Epoch 227/300
13702/13702 [==============================] - 1s - loss: 9.5432e-05 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00
Epoch 228/300
13702/13702 [==============================] - 1s - loss: 8.6888e-05 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00
Epoch 229/300
13702/13702 [==============================] - 1s - loss: 9.2719e-05 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00
Epoch 230/300
13702/13702 [==============================] - 1s - loss: 9.3481e-05 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00
Epoch 231/300
13702/13702 [==============================] - 1s - loss: 9.2623e-05 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00
Epoch 232/300
13702/13702 [==============================] - 1s - loss: 8.7488e-05 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+000.
Epoch 233/300
13702/13702 [==============================] - 1s - loss: 9.2512e-05 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+000.0000
Epoch 234/300
13702/13702 [==============================] - 1s - loss: 9.4039e-05 - acc: 0.0000e+00 - val_loss: 0.0035 - val_acc: 0.0000e+00
Epoch 235/300
13702/13702 [==============================] - 1s - loss: 8.6827e-05 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00
Epoch 236/300
13702/13702 [==============================] - 1s - loss: 9.0347e-05 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+000.0000
Epoch 237/300
13702/13702 [==============================] - 1s - loss: 8.8818e-05 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00
Epoch 238/300
13702/13702 [==============================] - 1s - loss: 9.0575e-05 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00
Epoch 239/300
13702/13702 [==============================] - 1s - loss: 8.8994e-05 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+000.0000e+
Epoch 240/300
13702/13702 [==============================] - 1s - loss: 8.4951e-05 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00
Epoch 241/300
13702/13702 [==============================] - 1s - loss: 9.1674e-05 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00
Epoch 242/300
13702/13702 [==============================] - 1s - loss: 8.6254e-05 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00
Epoch 243/300
13702/13702 [==============================] - 1s - loss: 8.8824e-05 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00
Epoch 244/300
13702/13702 [==============================] - 1s - loss: 9.2132e-05 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00
Epoch 245/300
13702/13702 [==============================] - 1s - loss: 9.5474e-05 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+000.000
Epoch 246/300
13702/13702 [==============================] - 1s - loss: 8.7269e-05 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00
Epoch 247/300
13702/13702 [==============================] - 1s - loss: 8.5431e-05 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00
Epoch 248/300
13702/13702 [==============================] - 1s - loss: 8.5194e-05 - acc: 0.0000e+00 - val_loss: 0.0035 - val_acc: 0.0000e+00
Epoch 249/300
13702/13702 [==============================] - 1s - loss: 8.4119e-05 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00
Epoch 250/300
13702/13702 [==============================] - 1s - loss: 8.4622e-05 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00
Epoch 251/300
13702/13702 [==============================] - 1s - loss: 8.7818e-05 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00
Epoch 252/300
13702/13702 [==============================] - 1s - loss: 9.4167e-05 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00
Epoch 253/300
13702/13702 [==============================] - 1s - loss: 9.1000e-05 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00
Epoch 254/300
13702/13702 [==============================] - 1s - loss: 9.3134e-05 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00
Epoch 255/300
13702/13702 [==============================] - 1s - loss: 9.3929e-05 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00
Epoch 256/300
13702/13702 [==============================] - 1s - loss: 8.7647e-05 - acc: 0.0000e+00 - val_loss: 0.0035 - val_acc: 0.0000e+00
Epoch 257/300
13702/13702 [==============================] - 1s - loss: 8.7625e-05 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+000
Epoch 258/300
13702/13702 [==============================] - 1s - loss: 8.3267e-05 - acc: 0.0000e+00 - val_loss: 0.0042 - val_acc: 0.0000e+00
Epoch 259/300
13702/13702 [==============================] - 1s - loss: 8.0942e-05 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00
Epoch 260/300
13702/13702 [==============================] - 1s - loss: 9.3551e-05 - acc: 0.0000e+00 - val_loss: 0.0049 - val_acc: 0.0000e+00
Epoch 261/300
13702/13702 [==============================] - 1s - loss: 9.0067e-05 - acc: 0.0000e+00 - val_loss: 0.0035 - val_acc: 0.0000e+00
Epoch 262/300
13702/13702 [==============================] - 1s - loss: 8.8477e-05 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00
Epoch 263/300
13702/13702 [==============================] - 1s - loss: 9.2520e-05 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00
Epoch 264/300
13702/13702 [==============================] - 1s - loss: 8.5144e-05 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00
Epoch 265/300
13702/13702 [==============================] - 1s - loss: 8.4355e-05 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00
Epoch 266/300
13702/13702 [==============================] - 1s - loss: 8.0969e-05 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00
Epoch 267/300
13702/13702 [==============================] - 1s - loss: 8.1810e-05 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00
Epoch 268/300
13702/13702 [==============================] - 1s - loss: 8.1823e-05 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00
Epoch 269/300
13702/13702 [==============================] - 1s - loss: 8.1051e-05 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00s - loss: 7.9135e-05 - acc: 0.000
Epoch 270/300
13702/13702 [==============================] - 1s - loss: 8.4315e-05 - acc: 0.0000e+00 - val_loss: 0.0041 - val_acc: 0.0000e+00
Epoch 271/300
13702/13702 [==============================] - 1s - loss: 8.6115e-05 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00
Epoch 272/300
13702/13702 [==============================] - 1s - loss: 8.7298e-05 - acc: 0.0000e+00 - val_loss: 0.0042 - val_acc: 0.0000e+00
Epoch 273/300
13702/13702 [==============================] - 1s - loss: 8.1670e-05 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00
Epoch 274/300
13702/13702 [==============================] - 1s - loss: 8.3702e-05 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00
Epoch 275/300
13702/13702 [==============================] - 1s - loss: 8.7613e-05 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+000.
Epoch 276/300
13702/13702 [==============================] - 1s - loss: 8.6305e-05 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00
Epoch 277/300
13702/13702 [==============================] - 1s - loss: 7.8780e-05 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00
Epoch 278/300
13702/13702 [==============================] - 1s - loss: 8.3515e-05 - acc: 0.0000e+00 - val_loss: 0.0049 - val_acc: 0.0000e+00
Epoch 279/300
13702/13702 [==============================] - 1s - loss: 7.5842e-05 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00
Epoch 280/300
13702/13702 [==============================] - 1s - loss: 7.8655e-05 - acc: 0.0000e+00 - val_loss: 0.0047 - val_acc: 0.0000e+00
Epoch 281/300
13702/13702 [==============================] - 1s - loss: 8.1377e-05 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+000.0000e+0
Epoch 282/300
13702/13702 [==============================] - 1s - loss: 7.9263e-05 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00
Epoch 283/300
13702/13702 [==============================] - 1s - loss: 8.3323e-05 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00
Epoch 284/300
13702/13702 [==============================] - 1s - loss: 7.8771e-05 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00
Epoch 285/300
13702/13702 [==============================] - 1s - loss: 8.4792e-05 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00
Epoch 286/300
13702/13702 [==============================] - 1s - loss: 8.0443e-05 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00
Epoch 287/300
13702/13702 [==============================] - 1s - loss: 8.4249e-05 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00
Epoch 288/300
13702/13702 [==============================] - 1s - loss: 8.2886e-05 - acc: 0.0000e+00 - val_loss: 0.0065 - val_acc: 0.0000e+00
Epoch 289/300
13702/13702 [==============================] - 1s - loss: 8.4691e-05 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00
Epoch 290/300
13702/13702 [==============================] - 1s - loss: 8.0465e-05 - acc: 0.0000e+00 - val_loss: 0.0058 - val_acc: 0.0000e+000.000
Epoch 291/300
13702/13702 [==============================] - 1s - loss: 7.8374e-05 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00
Epoch 292/300
13702/13702 [==============================] - 1s - loss: 8.0230e-05 - acc: 0.0000e+00 - val_loss: 0.0052 - val_acc: 0.0000e+00
Epoch 293/300
13702/13702 [==============================] - 1s - loss: 9.0778e-05 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+000.000
Epoch 294/300
13702/13702 [==============================] - 1s - loss: 7.8689e-05 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00
Epoch 295/300
13702/13702 [==============================] - 1s - loss: 7.4792e-05 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00
Epoch 296/300
13702/13702 [==============================] - 1s - loss: 8.2182e-05 - acc: 0.0000e+00 - val_loss: 0.0056 - val_acc: 0.0000e+00
Epoch 297/300
13702/13702 [==============================] - 1s - loss: 7.7594e-05 - acc: 0.0000e+00 - val_loss: 0.0061 - val_acc: 0.0000e+00
Epoch 298/300
13702/13702 [==============================] - 1s - loss: 7.9138e-05 - acc: 0.0000e+00 - val_loss: 0.0060 - val_acc: 0.0000e+00
Epoch 299/300
13702/13702 [==============================] - 1s - loss: 7.6621e-05 - acc: 0.0000e+00 - val_loss: 0.0066 - val_acc: 0.0000e+00
Epoch 300/300
13702/13702 [==============================] - 1s - loss: 7.7939e-05 - acc: 0.0000e+00 - val_loss: 0.0065 - val_acc: 0.0000e+00
Train Score: 0.00119 MSE (0.03 RMSE)
Test Score: 0.06796 MSE (0.26 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_29 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_29 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_30 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_30 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_29 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_30 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/300
13702/13702 [==============================] - 4s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 2/300
13702/13702 [==============================] - 1s - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00
Epoch 3/300
13702/13702 [==============================] - 1s - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 6.5230e-04 - val_acc: 0.0000e+00
Epoch 4/300
13702/13702 [==============================] - 1s - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 5/300
13702/13702 [==============================] - 1s - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 4.9021e-04 - val_acc: 0.0000e+00
Epoch 6/300
13702/13702 [==============================] - 1s - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 9.8289e-04 - val_acc: 0.0000e+00
Epoch 7/300
13702/13702 [==============================] - 1s - loss: 9.6046e-04 - acc: 0.0000e+00 - val_loss: 3.3653e-04 - val_acc: 0.0000e+00
Epoch 8/300
13702/13702 [==============================] - 1s - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00
Epoch 9/300
13702/13702 [==============================] - 1s - loss: 9.7068e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 10/300
13702/13702 [==============================] - 1s - loss: 9.3338e-04 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00
Epoch 11/300
13702/13702 [==============================] - 1s - loss: 8.5037e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 12/300
13702/13702 [==============================] - 1s - loss: 8.6161e-04 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00
Epoch 13/300
13702/13702 [==============================] - 1s - loss: 8.6060e-04 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+000.0000e+
Epoch 14/300
13702/13702 [==============================] - 1s - loss: 7.9179e-04 - acc: 0.0000e+00 - val_loss: 0.0051 - val_acc: 0.0000e+000.000
Epoch 15/300
13702/13702 [==============================] - 1s - loss: 7.6830e-04 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00
Epoch 16/300
13702/13702 [==============================] - 1s - loss: 7.4102e-04 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00
Epoch 17/300
13702/13702 [==============================] - 1s - loss: 7.6332e-04 - acc: 0.0000e+00 - val_loss: 0.0058 - val_acc: 0.0000e+00
Epoch 18/300
13702/13702 [==============================] - 1s - loss: 7.3765e-04 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00
Epoch 19/300
13702/13702 [==============================] - 1s - loss: 6.6167e-04 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00
Epoch 20/300
13702/13702 [==============================] - 1s - loss: 6.6200e-04 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00
Epoch 21/300
13702/13702 [==============================] - 1s - loss: 6.6812e-04 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00
Epoch 22/300
13702/13702 [==============================] - 1s - loss: 6.4723e-04 - acc: 0.0000e+00 - val_loss: 0.0065 - val_acc: 0.0000e+00
Epoch 23/300
13702/13702 [==============================] - 1s - loss: 5.9521e-04 - acc: 0.0000e+00 - val_loss: 0.0080 - val_acc: 0.0000e+00
Epoch 24/300
13702/13702 [==============================] - 1s - loss: 6.3330e-04 - acc: 0.0000e+00 - val_loss: 0.0059 - val_acc: 0.0000e+00
Epoch 25/300
13702/13702 [==============================] - 1s - loss: 5.9014e-04 - acc: 0.0000e+00 - val_loss: 0.0079 - val_acc: 0.0000e+00
Epoch 26/300
13702/13702 [==============================] - 1s - loss: 5.8288e-04 - acc: 0.0000e+00 - val_loss: 0.0048 - val_acc: 0.0000e+00
Epoch 27/300
13702/13702 [==============================] - 1s - loss: 5.8344e-04 - acc: 0.0000e+00 - val_loss: 0.0071 - val_acc: 0.0000e+00
Epoch 28/300
13702/13702 [==============================] - 1s - loss: 5.5036e-04 - acc: 0.0000e+00 - val_loss: 0.0051 - val_acc: 0.0000e+00
Epoch 29/300
13702/13702 [==============================] - 1s - loss: 5.1867e-04 - acc: 0.0000e+00 - val_loss: 0.0067 - val_acc: 0.0000e+000.0000
Epoch 30/300
13702/13702 [==============================] - 1s - loss: 4.9795e-04 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00
Epoch 31/300
13702/13702 [==============================] - 1s - loss: 4.9341e-04 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00
Epoch 32/300
13702/13702 [==============================] - 1s - loss: 4.8631e-04 - acc: 0.0000e+00 - val_loss: 0.0070 - val_acc: 0.0000e+00
Epoch 33/300
13702/13702 [==============================] - 1s - loss: 4.6503e-04 - acc: 0.0000e+00 - val_loss: 0.0078 - val_acc: 0.0000e+00
Epoch 34/300
13702/13702 [==============================] - 1s - loss: 4.6121e-04 - acc: 0.0000e+00 - val_loss: 0.0086 - val_acc: 0.0000e+00
Epoch 35/300
13702/13702 [==============================] - 1s - loss: 4.1851e-04 - acc: 0.0000e+00 - val_loss: 0.0109 - val_acc: 0.0000e+00
Epoch 36/300
13702/13702 [==============================] - 1s - loss: 4.6903e-04 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00
Epoch 37/300
13702/13702 [==============================] - 1s - loss: 3.8107e-04 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00
Epoch 38/300
13702/13702 [==============================] - 1s - loss: 3.9109e-04 - acc: 0.0000e+00 - val_loss: 0.0092 - val_acc: 0.0000e+00
Epoch 39/300
13702/13702 [==============================] - 1s - loss: 3.7466e-04 - acc: 0.0000e+00 - val_loss: 0.0061 - val_acc: 0.0000e+00
Epoch 40/300
13702/13702 [==============================] - 1s - loss: 3.5747e-04 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00
Epoch 41/300
13702/13702 [==============================] - 1s - loss: 3.3411e-04 - acc: 0.0000e+00 - val_loss: 0.0060 - val_acc: 0.0000e+00
Epoch 42/300
13702/13702 [==============================] - 1s - loss: 3.0964e-04 - acc: 0.0000e+00 - val_loss: 0.0074 - val_acc: 0.0000e+00
Epoch 43/300
13702/13702 [==============================] - 1s - loss: 3.3134e-04 - acc: 0.0000e+00 - val_loss: 0.0063 - val_acc: 0.0000e+00
Epoch 44/300
13702/13702 [==============================] - 1s - loss: 3.1705e-04 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00
Epoch 45/300
13702/13702 [==============================] - 1s - loss: 2.8323e-04 - acc: 0.0000e+00 - val_loss: 0.0074 - val_acc: 0.0000e+00
Epoch 46/300
13702/13702 [==============================] - 1s - loss: 2.8594e-04 - acc: 0.0000e+00 - val_loss: 0.0085 - val_acc: 0.0000e+00
Epoch 47/300
13702/13702 [==============================] - 1s - loss: 2.7427e-04 - acc: 0.0000e+00 - val_loss: 0.0074 - val_acc: 0.0000e+00
Epoch 48/300
13702/13702 [==============================] - 1s - loss: 2.8688e-04 - acc: 0.0000e+00 - val_loss: 0.0074 - val_acc: 0.0000e+000.000
Epoch 49/300
13702/13702 [==============================] - 1s - loss: 2.7037e-04 - acc: 0.0000e+00 - val_loss: 0.0081 - val_acc: 0.0000e+00
Epoch 50/300
13702/13702 [==============================] - 1s - loss: 2.9036e-04 - acc: 0.0000e+00 - val_loss: 0.0077 - val_acc: 0.0000e+00
Epoch 51/300
13702/13702 [==============================] - 1s - loss: 2.5137e-04 - acc: 0.0000e+00 - val_loss: 0.0081 - val_acc: 0.0000e+00
Epoch 52/300
13702/13702 [==============================] - 1s - loss: 2.2770e-04 - acc: 0.0000e+00 - val_loss: 0.0064 - val_acc: 0.0000e+00
Epoch 53/300
13702/13702 [==============================] - 1s - loss: 2.5440e-04 - acc: 0.0000e+00 - val_loss: 0.0057 - val_acc: 0.0000e+00
Epoch 54/300
13702/13702 [==============================] - 1s - loss: 2.3051e-04 - acc: 0.0000e+00 - val_loss: 0.0090 - val_acc: 0.0000e+00
Epoch 55/300
13702/13702 [==============================] - 1s - loss: 2.5118e-04 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00
Epoch 56/300
13702/13702 [==============================] - 1s - loss: 2.3575e-04 - acc: 0.0000e+00 - val_loss: 0.0063 - val_acc: 0.0000e+00
Epoch 57/300
13702/13702 [==============================] - 1s - loss: 2.2918e-04 - acc: 0.0000e+00 - val_loss: 0.0079 - val_acc: 0.0000e+00
Epoch 58/300
13702/13702 [==============================] - 1s - loss: 2.4577e-04 - acc: 0.0000e+00 - val_loss: 0.0073 - val_acc: 0.0000e+00
Epoch 59/300
13702/13702 [==============================] - 1s - loss: 2.2373e-04 - acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.0000e+00
Epoch 60/300
13702/13702 [==============================] - 1s - loss: 2.1266e-04 - acc: 0.0000e+00 - val_loss: 0.0092 - val_acc: 0.0000e+00
Epoch 61/300
13702/13702 [==============================] - 1s - loss: 2.1133e-04 - acc: 0.0000e+00 - val_loss: 0.0080 - val_acc: 0.0000e+00
Epoch 62/300
13702/13702 [==============================] - 1s - loss: 2.3447e-04 - acc: 0.0000e+00 - val_loss: 0.0120 - val_acc: 0.0000e+00
Epoch 63/300
13702/13702 [==============================] - 1s - loss: 2.2961e-04 - acc: 0.0000e+00 - val_loss: 0.0105 - val_acc: 0.0000e+00
Epoch 64/300
13702/13702 [==============================] - 1s - loss: 2.0946e-04 - acc: 0.0000e+00 - val_loss: 0.0084 - val_acc: 0.0000e+00
Epoch 65/300
13702/13702 [==============================] - 1s - loss: 2.1914e-04 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00
Epoch 66/300
13702/13702 [==============================] - 1s - loss: 2.1934e-04 - acc: 0.0000e+00 - val_loss: 0.0107 - val_acc: 0.0000e+00
Epoch 67/300
13702/13702 [==============================] - 1s - loss: 2.0701e-04 - acc: 0.0000e+00 - val_loss: 0.0101 - val_acc: 0.0000e+00
Epoch 68/300
13702/13702 [==============================] - 1s - loss: 1.9894e-04 - acc: 0.0000e+00 - val_loss: 0.0100 - val_acc: 0.0000e+00
Epoch 69/300
13702/13702 [==============================] - 1s - loss: 1.9843e-04 - acc: 0.0000e+00 - val_loss: 0.0114 - val_acc: 0.0000e+00
Epoch 70/300
13702/13702 [==============================] - 1s - loss: 2.0821e-04 - acc: 0.0000e+00 - val_loss: 0.0098 - val_acc: 0.0000e+00
Epoch 71/300
13702/13702 [==============================] - 1s - loss: 2.0083e-04 - acc: 0.0000e+00 - val_loss: 0.0091 - val_acc: 0.0000e+00
Epoch 72/300
13702/13702 [==============================] - 1s - loss: 1.9968e-04 - acc: 0.0000e+00 - val_loss: 0.0095 - val_acc: 0.0000e+00
Epoch 73/300
13702/13702 [==============================] - 1s - loss: 1.9959e-04 - acc: 0.0000e+00 - val_loss: 0.0099 - val_acc: 0.0000e+00
Epoch 74/300
13702/13702 [==============================] - 1s - loss: 1.9667e-04 - acc: 0.0000e+00 - val_loss: 0.0110 - val_acc: 0.0000e+000.0000e+
Epoch 75/300
13702/13702 [==============================] - 1s - loss: 2.2525e-04 - acc: 0.0000e+00 - val_loss: 0.0105 - val_acc: 0.0000e+00
Epoch 76/300
13702/13702 [==============================] - 1s - loss: 1.9806e-04 - acc: 0.0000e+00 - val_loss: 0.0115 - val_acc: 0.0000e+00
Epoch 77/300
13702/13702 [==============================] - 1s - loss: 1.9375e-04 - acc: 0.0000e+00 - val_loss: 0.0106 - val_acc: 0.0000e+00
Epoch 78/300
13702/13702 [==============================] - 1s - loss: 2.0027e-04 - acc: 0.0000e+00 - val_loss: 0.0114 - val_acc: 0.0000e+00
Epoch 79/300
13702/13702 [==============================] - 1s - loss: 1.9015e-04 - acc: 0.0000e+00 - val_loss: 0.0133 - val_acc: 0.0000e+00
Epoch 80/300
13702/13702 [==============================] - 1s - loss: 2.0844e-04 - acc: 0.0000e+00 - val_loss: 0.0134 - val_acc: 0.0000e+00
Epoch 81/300
13702/13702 [==============================] - 1s - loss: 1.8935e-04 - acc: 0.0000e+00 - val_loss: 0.0117 - val_acc: 0.0000e+00
Epoch 82/300
13702/13702 [==============================] - 1s - loss: 1.8755e-04 - acc: 0.0000e+00 - val_loss: 0.0143 - val_acc: 0.0000e+000.0000e+0
Epoch 83/300
13702/13702 [==============================] - 1s - loss: 2.2090e-04 - acc: 0.0000e+00 - val_loss: 0.0169 - val_acc: 0.0000e+00
Epoch 84/300
13702/13702 [==============================] - 1s - loss: 1.9312e-04 - acc: 0.0000e+00 - val_loss: 0.0129 - val_acc: 0.0000e+00
Epoch 85/300
13702/13702 [==============================] - 1s - loss: 1.7749e-04 - acc: 0.0000e+00 - val_loss: 0.0097 - val_acc: 0.0000e+00
Epoch 86/300
13702/13702 [==============================] - 1s - loss: 1.8486e-04 - acc: 0.0000e+00 - val_loss: 0.0147 - val_acc: 0.0000e+00
Epoch 87/300
13702/13702 [==============================] - 1s - loss: 1.8044e-04 - acc: 0.0000e+00 - val_loss: 0.0127 - val_acc: 0.0000e+00
Epoch 88/300
13702/13702 [==============================] - 1s - loss: 1.7809e-04 - acc: 0.0000e+00 - val_loss: 0.0128 - val_acc: 0.0000e+00
Epoch 89/300
13702/13702 [==============================] - 1s - loss: 1.7205e-04 - acc: 0.0000e+00 - val_loss: 0.0142 - val_acc: 0.0000e+00
Epoch 90/300
13702/13702 [==============================] - 1s - loss: 1.8813e-04 - acc: 0.0000e+00 - val_loss: 0.0150 - val_acc: 0.0000e+00
Epoch 91/300
13702/13702 [==============================] - 1s - loss: 1.7716e-04 - acc: 0.0000e+00 - val_loss: 0.0155 - val_acc: 0.0000e+00
Epoch 92/300
13702/13702 [==============================] - 1s - loss: 1.8283e-04 - acc: 0.0000e+00 - val_loss: 0.0148 - val_acc: 0.0000e+00
Epoch 93/300
13702/13702 [==============================] - 1s - loss: 1.7787e-04 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0000e+00
Epoch 94/300
13702/13702 [==============================] - 1s - loss: 1.7191e-04 - acc: 0.0000e+00 - val_loss: 0.0163 - val_acc: 0.0000e+00
Epoch 95/300
13702/13702 [==============================] - 1s - loss: 1.7666e-04 - acc: 0.0000e+00 - val_loss: 0.0143 - val_acc: 0.0000e+00
Epoch 96/300
13702/13702 [==============================] - 1s - loss: 1.9055e-04 - acc: 0.0000e+00 - val_loss: 0.0178 - val_acc: 0.0000e+00
Epoch 97/300
13702/13702 [==============================] - 1s - loss: 1.7386e-04 - acc: 0.0000e+00 - val_loss: 0.0179 - val_acc: 0.0000e+00
Epoch 98/300
13702/13702 [==============================] - 1s - loss: 1.7108e-04 - acc: 0.0000e+00 - val_loss: 0.0157 - val_acc: 0.0000e+00
Epoch 99/300
13702/13702 [==============================] - 1s - loss: 1.7312e-04 - acc: 0.0000e+00 - val_loss: 0.0162 - val_acc: 0.0000e+00
Epoch 100/300
13702/13702 [==============================] - 1s - loss: 1.7301e-04 - acc: 0.0000e+00 - val_loss: 0.0190 - val_acc: 0.0000e+00
Epoch 101/300
13702/13702 [==============================] - 1s - loss: 1.6941e-04 - acc: 0.0000e+00 - val_loss: 0.0179 - val_acc: 0.0000e+00
Epoch 102/300
13702/13702 [==============================] - 1s - loss: 1.7502e-04 - acc: 0.0000e+00 - val_loss: 0.0181 - val_acc: 0.0000e+00
Epoch 103/300
13702/13702 [==============================] - 1s - loss: 1.6959e-04 - acc: 0.0000e+00 - val_loss: 0.0185 - val_acc: 0.0000e+00
Epoch 104/300
13702/13702 [==============================] - 1s - loss: 1.6169e-04 - acc: 0.0000e+00 - val_loss: 0.0175 - val_acc: 0.0000e+00
Epoch 105/300
13702/13702 [==============================] - 1s - loss: 1.5937e-04 - acc: 0.0000e+00 - val_loss: 0.0191 - val_acc: 0.0000e+00
Epoch 106/300
13702/13702 [==============================] - 1s - loss: 1.6408e-04 - acc: 0.0000e+00 - val_loss: 0.0204 - val_acc: 0.0000e+00
Epoch 107/300
13702/13702 [==============================] - 1s - loss: 1.7346e-04 - acc: 0.0000e+00 - val_loss: 0.0188 - val_acc: 0.0000e+00
Epoch 108/300
13702/13702 [==============================] - 1s - loss: 1.7607e-04 - acc: 0.0000e+00 - val_loss: 0.0209 - val_acc: 0.0000e+00
Epoch 109/300
13702/13702 [==============================] - 1s - loss: 1.6748e-04 - acc: 0.0000e+00 - val_loss: 0.0222 - val_acc: 0.0000e+00
Epoch 110/300
13702/13702 [==============================] - 1s - loss: 1.6552e-04 - acc: 0.0000e+00 - val_loss: 0.0213 - val_acc: 0.0000e+00
Epoch 111/300
13702/13702 [==============================] - 1s - loss: 1.6837e-04 - acc: 0.0000e+00 - val_loss: 0.0215 - val_acc: 0.0000e+00
Epoch 112/300
13702/13702 [==============================] - 1s - loss: 1.6661e-04 - acc: 0.0000e+00 - val_loss: 0.0216 - val_acc: 0.0000e+00
Epoch 113/300
13702/13702 [==============================] - 1s - loss: 1.7291e-04 - acc: 0.0000e+00 - val_loss: 0.0204 - val_acc: 0.0000e+00
Epoch 114/300
13702/13702 [==============================] - 1s - loss: 1.5704e-04 - acc: 0.0000e+00 - val_loss: 0.0206 - val_acc: 0.0000e+00
Epoch 115/300
13702/13702 [==============================] - 1s - loss: 1.5654e-04 - acc: 0.0000e+00 - val_loss: 0.0218 - val_acc: 0.0000e+00
Epoch 116/300
13702/13702 [==============================] - 1s - loss: 1.5805e-04 - acc: 0.0000e+00 - val_loss: 0.0249 - val_acc: 0.0000e+00
Epoch 117/300
13702/13702 [==============================] - 1s - loss: 1.6865e-04 - acc: 0.0000e+00 - val_loss: 0.0212 - val_acc: 0.0000e+00
Epoch 118/300
13702/13702 [==============================] - 1s - loss: 1.6981e-04 - acc: 0.0000e+00 - val_loss: 0.0217 - val_acc: 0.0000e+00
Epoch 119/300
13702/13702 [==============================] - 1s - loss: 1.5490e-04 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00
Epoch 120/300
13702/13702 [==============================] - 1s - loss: 1.6080e-04 - acc: 0.0000e+00 - val_loss: 0.0234 - val_acc: 0.0000e+000.000
Epoch 121/300
13702/13702 [==============================] - 1s - loss: 1.5785e-04 - acc: 0.0000e+00 - val_loss: 0.0261 - val_acc: 0.0000e+00
Epoch 122/300
13702/13702 [==============================] - 1s - loss: 1.6486e-04 - acc: 0.0000e+00 - val_loss: 0.0243 - val_acc: 0.0000e+00
Epoch 123/300
13702/13702 [==============================] - 1s - loss: 1.4897e-04 - acc: 0.0000e+00 - val_loss: 0.0237 - val_acc: 0.0000e+00
Epoch 124/300
13702/13702 [==============================] - 1s - loss: 1.4837e-04 - acc: 0.0000e+00 - val_loss: 0.0236 - val_acc: 0.0000e+00
Epoch 125/300
13702/13702 [==============================] - 1s - loss: 1.6152e-04 - acc: 0.0000e+00 - val_loss: 0.0259 - val_acc: 0.0000e+00
Epoch 126/300
13702/13702 [==============================] - 1s - loss: 1.5313e-04 - acc: 0.0000e+00 - val_loss: 0.0247 - val_acc: 0.0000e+00
Epoch 127/300
13702/13702 [==============================] - 1s - loss: 1.5579e-04 - acc: 0.0000e+00 - val_loss: 0.0240 - val_acc: 0.0000e+00
Epoch 128/300
13702/13702 [==============================] - 1s - loss: 1.5161e-04 - acc: 0.0000e+00 - val_loss: 0.0253 - val_acc: 0.0000e+00
Epoch 129/300
13702/13702 [==============================] - 1s - loss: 1.4936e-04 - acc: 0.0000e+00 - val_loss: 0.0269 - val_acc: 0.0000e+00
Epoch 130/300
13702/13702 [==============================] - 1s - loss: 1.4931e-04 - acc: 0.0000e+00 - val_loss: 0.0261 - val_acc: 0.0000e+00
Epoch 131/300
13702/13702 [==============================] - 1s - loss: 1.5253e-04 - acc: 0.0000e+00 - val_loss: 0.0267 - val_acc: 0.0000e+00
Epoch 132/300
13702/13702 [==============================] - 1s - loss: 1.4954e-04 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0000e+00
Epoch 133/300
13702/13702 [==============================] - 1s - loss: 1.5850e-04 - acc: 0.0000e+00 - val_loss: 0.0278 - val_acc: 0.0000e+00
Epoch 134/300
13702/13702 [==============================] - 1s - loss: 1.6176e-04 - acc: 0.0000e+00 - val_loss: 0.0302 - val_acc: 0.0000e+00
Epoch 135/300
13702/13702 [==============================] - 1s - loss: 1.4731e-04 - acc: 0.0000e+00 - val_loss: 0.0300 - val_acc: 0.0000e+00
Epoch 136/300
13702/13702 [==============================] - 1s - loss: 1.4503e-04 - acc: 0.0000e+00 - val_loss: 0.0260 - val_acc: 0.0000e+000.000
Epoch 137/300
13702/13702 [==============================] - 1s - loss: 1.4453e-04 - acc: 0.0000e+00 - val_loss: 0.0310 - val_acc: 0.0000e+00
Epoch 138/300
13702/13702 [==============================] - 1s - loss: 1.5217e-04 - acc: 0.0000e+00 - val_loss: 0.0290 - val_acc: 0.0000e+00
Epoch 139/300
13702/13702 [==============================] - 1s - loss: 1.6020e-04 - acc: 0.0000e+00 - val_loss: 0.0281 - val_acc: 0.0000e+00
Epoch 140/300
13702/13702 [==============================] - 1s - loss: 1.5872e-04 - acc: 0.0000e+00 - val_loss: 0.0288 - val_acc: 0.0000e+00
Epoch 141/300
13702/13702 [==============================] - 1s - loss: 1.5016e-04 - acc: 0.0000e+00 - val_loss: 0.0278 - val_acc: 0.0000e+00
Epoch 142/300
13702/13702 [==============================] - 1s - loss: 1.5569e-04 - acc: 0.0000e+00 - val_loss: 0.0278 - val_acc: 0.0000e+00
Epoch 143/300
13702/13702 [==============================] - 1s - loss: 1.6434e-04 - acc: 0.0000e+00 - val_loss: 0.0265 - val_acc: 0.0000e+00
Epoch 144/300
13702/13702 [==============================] - 1s - loss: 1.6999e-04 - acc: 0.0000e+00 - val_loss: 0.0264 - val_acc: 0.0000e+00
Epoch 145/300
13702/13702 [==============================] - 1s - loss: 1.4926e-04 - acc: 0.0000e+00 - val_loss: 0.0298 - val_acc: 0.0000e+00
Epoch 146/300
13702/13702 [==============================] - 1s - loss: 1.5920e-04 - acc: 0.0000e+00 - val_loss: 0.0325 - val_acc: 0.0000e+00
Epoch 147/300
13702/13702 [==============================] - 1s - loss: 1.4636e-04 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0000e+00
Epoch 148/300
13702/13702 [==============================] - 1s - loss: 1.5600e-04 - acc: 0.0000e+00 - val_loss: 0.0291 - val_acc: 0.0000e+00
Epoch 149/300
13702/13702 [==============================] - 1s - loss: 1.6747e-04 - acc: 0.0000e+00 - val_loss: 0.0315 - val_acc: 0.0000e+00
Epoch 150/300
13702/13702 [==============================] - 1s - loss: 1.5907e-04 - acc: 0.0000e+00 - val_loss: 0.0297 - val_acc: 0.0000e+00
Epoch 151/300
13702/13702 [==============================] - 1s - loss: 1.4717e-04 - acc: 0.0000e+00 - val_loss: 0.0286 - val_acc: 0.0000e+00
Epoch 152/300
13702/13702 [==============================] - 1s - loss: 1.4868e-04 - acc: 0.0000e+00 - val_loss: 0.0306 - val_acc: 0.0000e+00
Epoch 153/300
13702/13702 [==============================] - 1s - loss: 1.4513e-04 - acc: 0.0000e+00 - val_loss: 0.0310 - val_acc: 0.0000e+00
Epoch 154/300
13702/13702 [==============================] - 1s - loss: 1.5364e-04 - acc: 0.0000e+00 - val_loss: 0.0283 - val_acc: 0.0000e+00
Epoch 155/300
13702/13702 [==============================] - 1s - loss: 1.4171e-04 - acc: 0.0000e+00 - val_loss: 0.0310 - val_acc: 0.0000e+00
Epoch 156/300
13702/13702 [==============================] - 1s - loss: 1.4190e-04 - acc: 0.0000e+00 - val_loss: 0.0313 - val_acc: 0.0000e+00
Epoch 157/300
13702/13702 [==============================] - 1s - loss: 1.5245e-04 - acc: 0.0000e+00 - val_loss: 0.0319 - val_acc: 0.0000e+00
Epoch 158/300
13702/13702 [==============================] - 1s - loss: 1.5031e-04 - acc: 0.0000e+00 - val_loss: 0.0347 - val_acc: 0.0000e+00
Epoch 159/300
13702/13702 [==============================] - 1s - loss: 1.3929e-04 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0000e+00
Epoch 160/300
13702/13702 [==============================] - 1s - loss: 1.4962e-04 - acc: 0.0000e+00 - val_loss: 0.0315 - val_acc: 0.0000e+00
Epoch 161/300
13702/13702 [==============================] - 1s - loss: 1.5208e-04 - acc: 0.0000e+00 - val_loss: 0.0313 - val_acc: 0.0000e+00
Epoch 162/300
13702/13702 [==============================] - 1s - loss: 1.4527e-04 - acc: 0.0000e+00 - val_loss: 0.0314 - val_acc: 0.0000e+00
Epoch 163/300
13702/13702 [==============================] - 1s - loss: 1.5024e-04 - acc: 0.0000e+00 - val_loss: 0.0335 - val_acc: 0.0000e+00
Epoch 164/300
13702/13702 [==============================] - 1s - loss: 1.4392e-04 - acc: 0.0000e+00 - val_loss: 0.0312 - val_acc: 0.0000e+00
Epoch 165/300
13702/13702 [==============================] - 1s - loss: 1.3748e-04 - acc: 0.0000e+00 - val_loss: 0.0305 - val_acc: 0.0000e+00
Epoch 166/300
13702/13702 [==============================] - 1s - loss: 1.4036e-04 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0000e+00
Epoch 167/300
13702/13702 [==============================] - 1s - loss: 1.4800e-04 - acc: 0.0000e+00 - val_loss: 0.0317 - val_acc: 0.0000e+00
Epoch 168/300
13702/13702 [==============================] - 1s - loss: 1.5110e-04 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00
Epoch 169/300
13702/13702 [==============================] - 1s - loss: 1.5115e-04 - acc: 0.0000e+00 - val_loss: 0.0310 - val_acc: 0.0000e+00
Epoch 170/300
13702/13702 [==============================] - 1s - loss: 1.4514e-04 - acc: 0.0000e+00 - val_loss: 0.0338 - val_acc: 0.0000e+00
Epoch 171/300
13702/13702 [==============================] - 1s - loss: 1.5196e-04 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0000e+00
Epoch 172/300
13702/13702 [==============================] - 1s - loss: 1.4729e-04 - acc: 0.0000e+00 - val_loss: 0.0335 - val_acc: 0.0000e+00
Epoch 173/300
13702/13702 [==============================] - 1s - loss: 1.4233e-04 - acc: 0.0000e+00 - val_loss: 0.0347 - val_acc: 0.0000e+00
Epoch 174/300
13702/13702 [==============================] - 1s - loss: 1.4182e-04 - acc: 0.0000e+00 - val_loss: 0.0354 - val_acc: 0.0000e+00
Epoch 175/300
13702/13702 [==============================] - 1s - loss: 1.6596e-04 - acc: 0.0000e+00 - val_loss: 0.0349 - val_acc: 0.0000e+00
Epoch 176/300
13702/13702 [==============================] - 1s - loss: 1.4592e-04 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00
Epoch 177/300
13702/13702 [==============================] - 1s - loss: 1.4454e-04 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00
Epoch 178/300
13702/13702 [==============================] - 1s - loss: 1.4870e-04 - acc: 0.0000e+00 - val_loss: 0.0335 - val_acc: 0.0000e+00
Epoch 179/300
13702/13702 [==============================] - 1s - loss: 1.3853e-04 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0000e+00
Epoch 180/300
13702/13702 [==============================] - 1s - loss: 1.4586e-04 - acc: 0.0000e+00 - val_loss: 0.0332 - val_acc: 0.0000e+00
Epoch 181/300
13702/13702 [==============================] - 1s - loss: 1.4767e-04 - acc: 0.0000e+00 - val_loss: 0.0350 - val_acc: 0.0000e+00
Epoch 182/300
13702/13702 [==============================] - 1s - loss: 1.3342e-04 - acc: 0.0000e+00 - val_loss: 0.0345 - val_acc: 0.0000e+00
Epoch 183/300
13702/13702 [==============================] - 1s - loss: 1.3567e-04 - acc: 0.0000e+00 - val_loss: 0.0332 - val_acc: 0.0000e+00
Epoch 184/300
13702/13702 [==============================] - 1s - loss: 1.3284e-04 - acc: 0.0000e+00 - val_loss: 0.0344 - val_acc: 0.0000e+00
Epoch 185/300
13702/13702 [==============================] - 1s - loss: 1.3521e-04 - acc: 0.0000e+00 - val_loss: 0.0341 - val_acc: 0.0000e+00
Epoch 186/300
13702/13702 [==============================] - 1s - loss: 1.4062e-04 - acc: 0.0000e+00 - val_loss: 0.0354 - val_acc: 0.0000e+00
Epoch 187/300
13702/13702 [==============================] - 1s - loss: 1.3751e-04 - acc: 0.0000e+00 - val_loss: 0.0333 - val_acc: 0.0000e+00
Epoch 188/300
13702/13702 [==============================] - 1s - loss: 1.4756e-04 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0000e+00
Epoch 189/300
13702/13702 [==============================] - 1s - loss: 1.4612e-04 - acc: 0.0000e+00 - val_loss: 0.0337 - val_acc: 0.0000e+00
Epoch 190/300
13702/13702 [==============================] - 1s - loss: 1.4534e-04 - acc: 0.0000e+00 - val_loss: 0.0341 - val_acc: 0.0000e+00
Epoch 191/300
13702/13702 [==============================] - 1s - loss: 1.3642e-04 - acc: 0.0000e+00 - val_loss: 0.0335 - val_acc: 0.0000e+00
Epoch 192/300
13702/13702 [==============================] - 1s - loss: 1.3641e-04 - acc: 0.0000e+00 - val_loss: 0.0337 - val_acc: 0.0000e+00
Epoch 193/300
13702/13702 [==============================] - 1s - loss: 1.4635e-04 - acc: 0.0000e+00 - val_loss: 0.0341 - val_acc: 0.0000e+00
Epoch 194/300
13702/13702 [==============================] - 1s - loss: 1.5511e-04 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00
Epoch 195/300
13702/13702 [==============================] - 1s - loss: 1.3058e-04 - acc: 0.0000e+00 - val_loss: 0.0332 - val_acc: 0.0000e+00
Epoch 196/300
13702/13702 [==============================] - 1s - loss: 1.3434e-04 - acc: 0.0000e+00 - val_loss: 0.0325 - val_acc: 0.0000e+00
Epoch 197/300
13702/13702 [==============================] - 1s - loss: 1.3567e-04 - acc: 0.0000e+00 - val_loss: 0.0341 - val_acc: 0.0000e+00
Epoch 198/300
13702/13702 [==============================] - 1s - loss: 1.3670e-04 - acc: 0.0000e+00 - val_loss: 0.0351 - val_acc: 0.0000e+00
Epoch 199/300
13702/13702 [==============================] - 1s - loss: 1.4268e-04 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0000e+00
Epoch 200/300
13702/13702 [==============================] - 1s - loss: 1.3357e-04 - acc: 0.0000e+00 - val_loss: 0.0340 - val_acc: 0.0000e+00
Epoch 201/300
13702/13702 [==============================] - 1s - loss: 1.2925e-04 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00
Epoch 202/300
13702/13702 [==============================] - 1s - loss: 1.3948e-04 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00
Epoch 203/300
13702/13702 [==============================] - 1s - loss: 1.3250e-04 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0000e+00
Epoch 204/300
13702/13702 [==============================] - 1s - loss: 1.2650e-04 - acc: 0.0000e+00 - val_loss: 0.0351 - val_acc: 0.0000e+00
Epoch 205/300
13702/13702 [==============================] - 1s - loss: 1.2902e-04 - acc: 0.0000e+00 - val_loss: 0.0355 - val_acc: 0.0000e+00
Epoch 206/300
13702/13702 [==============================] - 1s - loss: 1.3395e-04 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0000e+00
Epoch 207/300
13702/13702 [==============================] - 1s - loss: 1.3032e-04 - acc: 0.0000e+00 - val_loss: 0.0350 - val_acc: 0.0000e+00
Epoch 208/300
13702/13702 [==============================] - 1s - loss: 1.3161e-04 - acc: 0.0000e+00 - val_loss: 0.0361 - val_acc: 0.0000e+00
Epoch 209/300
13702/13702 [==============================] - 1s - loss: 1.3280e-04 - acc: 0.0000e+00 - val_loss: 0.0342 - val_acc: 0.0000e+00
Epoch 210/300
13702/13702 [==============================] - 1s - loss: 1.2652e-04 - acc: 0.0000e+00 - val_loss: 0.0345 - val_acc: 0.0000e+00
Epoch 211/300
13702/13702 [==============================] - 1s - loss: 1.2995e-04 - acc: 0.0000e+00 - val_loss: 0.0342 - val_acc: 0.0000e+00
Epoch 212/300
13702/13702 [==============================] - 1s - loss: 1.3373e-04 - acc: 0.0000e+00 - val_loss: 0.0342 - val_acc: 0.0000e+00
Epoch 213/300
13702/13702 [==============================] - 1s - loss: 1.2913e-04 - acc: 0.0000e+00 - val_loss: 0.0364 - val_acc: 0.0000e+00
Epoch 214/300
13702/13702 [==============================] - 1s - loss: 1.2724e-04 - acc: 0.0000e+00 - val_loss: 0.0356 - val_acc: 0.0000e+00
Epoch 215/300
13702/13702 [==============================] - 1s - loss: 1.3273e-04 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00
Epoch 216/300
13702/13702 [==============================] - 1s - loss: 1.2514e-04 - acc: 0.0000e+00 - val_loss: 0.0347 - val_acc: 0.0000e+00
Epoch 217/300
13702/13702 [==============================] - 1s - loss: 1.2304e-04 - acc: 0.0000e+00 - val_loss: 0.0332 - val_acc: 0.0000e+00
Epoch 218/300
13702/13702 [==============================] - 1s - loss: 1.2463e-04 - acc: 0.0000e+00 - val_loss: 0.0363 - val_acc: 0.0000e+00
Epoch 219/300
13702/13702 [==============================] - 1s - loss: 1.2767e-04 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00
Epoch 220/300
13702/13702 [==============================] - 1s - loss: 1.3333e-04 - acc: 0.0000e+00 - val_loss: 0.0350 - val_acc: 0.0000e+00
Epoch 221/300
13702/13702 [==============================] - 1s - loss: 1.2127e-04 - acc: 0.0000e+00 - val_loss: 0.0360 - val_acc: 0.0000e+00
Epoch 222/300
13702/13702 [==============================] - 1s - loss: 1.3149e-04 - acc: 0.0000e+00 - val_loss: 0.0349 - val_acc: 0.0000e+00
Epoch 223/300
13702/13702 [==============================] - 1s - loss: 1.3118e-04 - acc: 0.0000e+00 - val_loss: 0.0347 - val_acc: 0.0000e+00
Epoch 224/300
13702/13702 [==============================] - 1s - loss: 1.3272e-04 - acc: 0.0000e+00 - val_loss: 0.0339 - val_acc: 0.0000e+00
Epoch 225/300
13702/13702 [==============================] - 1s - loss: 1.2200e-04 - acc: 0.0000e+00 - val_loss: 0.0352 - val_acc: 0.0000e+00
Epoch 226/300
13702/13702 [==============================] - 1s - loss: 1.2153e-04 - acc: 0.0000e+00 - val_loss: 0.0355 - val_acc: 0.0000e+00
Epoch 227/300
13702/13702 [==============================] - 1s - loss: 1.3019e-04 - acc: 0.0000e+00 - val_loss: 0.0356 - val_acc: 0.0000e+00
Epoch 228/300
13702/13702 [==============================] - 1s - loss: 1.3147e-04 - acc: 0.0000e+00 - val_loss: 0.0341 - val_acc: 0.0000e+00
Epoch 229/300
13702/13702 [==============================] - 1s - loss: 1.2180e-04 - acc: 0.0000e+00 - val_loss: 0.0370 - val_acc: 0.0000e+00
Epoch 230/300
13702/13702 [==============================] - 1s - loss: 1.2825e-04 - acc: 0.0000e+00 - val_loss: 0.0374 - val_acc: 0.0000e+00
Epoch 231/300
13702/13702 [==============================] - 1s - loss: 1.2662e-04 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0000e+00
Epoch 232/300
13702/13702 [==============================] - 1s - loss: 1.1807e-04 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00
Epoch 233/300
13702/13702 [==============================] - 1s - loss: 1.3371e-04 - acc: 0.0000e+00 - val_loss: 0.0333 - val_acc: 0.0000e+00
Epoch 234/300
13702/13702 [==============================] - 1s - loss: 1.2195e-04 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00
Epoch 235/300
13702/13702 [==============================] - 1s - loss: 1.1892e-04 - acc: 0.0000e+00 - val_loss: 0.0329 - val_acc: 0.0000e+00
Epoch 236/300
13702/13702 [==============================] - 1s - loss: 1.4038e-04 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00
Epoch 237/300
13702/13702 [==============================] - 1s - loss: 1.3148e-04 - acc: 0.0000e+00 - val_loss: 0.0332 - val_acc: 0.0000e+00
Epoch 238/300
13702/13702 [==============================] - 1s - loss: 1.1812e-04 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00
Epoch 239/300
13702/13702 [==============================] - 1s - loss: 1.2764e-04 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0000e+00
Epoch 240/300
13702/13702 [==============================] - 1s - loss: 1.2171e-04 - acc: 0.0000e+00 - val_loss: 0.0344 - val_acc: 0.0000e+00
Epoch 241/300
13702/13702 [==============================] - 1s - loss: 1.2066e-04 - acc: 0.0000e+00 - val_loss: 0.0339 - val_acc: 0.0000e+00
Epoch 242/300
13702/13702 [==============================] - 1s - loss: 1.1932e-04 - acc: 0.0000e+00 - val_loss: 0.0333 - val_acc: 0.0000e+00
Epoch 243/300
13702/13702 [==============================] - 1s - loss: 1.3358e-04 - acc: 0.0000e+00 - val_loss: 0.0339 - val_acc: 0.0000e+00
Epoch 244/300
13702/13702 [==============================] - 1s - loss: 1.3335e-04 - acc: 0.0000e+00 - val_loss: 0.0356 - val_acc: 0.0000e+00
Epoch 245/300
13702/13702 [==============================] - 1s - loss: 1.2209e-04 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00
Epoch 246/300
13702/13702 [==============================] - 1s - loss: 1.2205e-04 - acc: 0.0000e+00 - val_loss: 0.0366 - val_acc: 0.0000e+00
Epoch 247/300
13702/13702 [==============================] - 1s - loss: 1.1827e-04 - acc: 0.0000e+00 - val_loss: 0.0358 - val_acc: 0.0000e+00
Epoch 248/300
13702/13702 [==============================] - 1s - loss: 1.2181e-04 - acc: 0.0000e+00 - val_loss: 0.0345 - val_acc: 0.0000e+00
Epoch 249/300
13702/13702 [==============================] - 1s - loss: 1.1912e-04 - acc: 0.0000e+00 - val_loss: 0.0348 - val_acc: 0.0000e+00
Epoch 250/300
13702/13702 [==============================] - 1s - loss: 1.2018e-04 - acc: 0.0000e+00 - val_loss: 0.0362 - val_acc: 0.0000e+00
Epoch 251/300
13702/13702 [==============================] - 1s - loss: 1.3371e-04 - acc: 0.0000e+00 - val_loss: 0.0336 - val_acc: 0.0000e+00
Epoch 252/300
13702/13702 [==============================] - 1s - loss: 1.2450e-04 - acc: 0.0000e+00 - val_loss: 0.0351 - val_acc: 0.0000e+00
Epoch 253/300
13702/13702 [==============================] - 1s - loss: 1.2947e-04 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0000e+00
Epoch 254/300
13702/13702 [==============================] - 1s - loss: 1.2153e-04 - acc: 0.0000e+00 - val_loss: 0.0329 - val_acc: 0.0000e+00
Epoch 255/300
13702/13702 [==============================] - 1s - loss: 1.2082e-04 - acc: 0.0000e+00 - val_loss: 0.0342 - val_acc: 0.0000e+00
Epoch 256/300
13702/13702 [==============================] - 1s - loss: 1.1632e-04 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00
Epoch 257/300
13702/13702 [==============================] - 1s - loss: 1.2359e-04 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0000e+00
Epoch 258/300
13702/13702 [==============================] - 1s - loss: 1.1809e-04 - acc: 0.0000e+00 - val_loss: 0.0355 - val_acc: 0.0000e+00
Epoch 259/300
13702/13702 [==============================] - 1s - loss: 1.1676e-04 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00
Epoch 260/300
13702/13702 [==============================] - 1s - loss: 1.1919e-04 - acc: 0.0000e+00 - val_loss: 0.0355 - val_acc: 0.0000e+00
Epoch 261/300
13702/13702 [==============================] - 1s - loss: 1.1845e-04 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00
Epoch 262/300
13702/13702 [==============================] - 1s - loss: 1.1971e-04 - acc: 0.0000e+00 - val_loss: 0.0361 - val_acc: 0.0000e+00
Epoch 263/300
13702/13702 [==============================] - 1s - loss: 1.2047e-04 - acc: 0.0000e+00 - val_loss: 0.0345 - val_acc: 0.0000e+00
Epoch 264/300
13702/13702 [==============================] - 1s - loss: 1.2095e-04 - acc: 0.0000e+00 - val_loss: 0.0348 - val_acc: 0.0000e+00
Epoch 265/300
13702/13702 [==============================] - 1s - loss: 1.2234e-04 - acc: 0.0000e+00 - val_loss: 0.0362 - val_acc: 0.0000e+00
Epoch 266/300
13702/13702 [==============================] - 1s - loss: 1.2183e-04 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00
Epoch 267/300
13702/13702 [==============================] - 1s - loss: 1.2994e-04 - acc: 0.0000e+00 - val_loss: 0.0375 - val_acc: 0.0000e+00
Epoch 268/300
13702/13702 [==============================] - 1s - loss: 1.1891e-04 - acc: 0.0000e+00 - val_loss: 0.0350 - val_acc: 0.0000e+00
Epoch 269/300
13702/13702 [==============================] - 1s - loss: 1.2397e-04 - acc: 0.0000e+00 - val_loss: 0.0361 - val_acc: 0.0000e+00
Epoch 270/300
13702/13702 [==============================] - 1s - loss: 1.2183e-04 - acc: 0.0000e+00 - val_loss: 0.0378 - val_acc: 0.0000e+00
Epoch 271/300
13702/13702 [==============================] - 1s - loss: 1.3585e-04 - acc: 0.0000e+00 - val_loss: 0.0369 - val_acc: 0.0000e+00
Epoch 272/300
13702/13702 [==============================] - 1s - loss: 1.2164e-04 - acc: 0.0000e+00 - val_loss: 0.0360 - val_acc: 0.0000e+00
Epoch 273/300
13702/13702 [==============================] - 1s - loss: 1.1416e-04 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0000e+00
Epoch 274/300
13702/13702 [==============================] - 1s - loss: 1.1925e-04 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00
Epoch 275/300
13702/13702 [==============================] - 1s - loss: 1.2074e-04 - acc: 0.0000e+00 - val_loss: 0.0364 - val_acc: 0.0000e+00
Epoch 276/300
13702/13702 [==============================] - 1s - loss: 1.1966e-04 - acc: 0.0000e+00 - val_loss: 0.0360 - val_acc: 0.0000e+00
Epoch 277/300
13702/13702 [==============================] - 1s - loss: 1.1957e-04 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00
Epoch 278/300
13702/13702 [==============================] - 1s - loss: 1.2117e-04 - acc: 0.0000e+00 - val_loss: 0.0358 - val_acc: 0.0000e+00
Epoch 279/300
13702/13702 [==============================] - 1s - loss: 1.1540e-04 - acc: 0.0000e+00 - val_loss: 0.0361 - val_acc: 0.0000e+00
Epoch 280/300
13702/13702 [==============================] - 1s - loss: 1.1603e-04 - acc: 0.0000e+00 - val_loss: 0.0372 - val_acc: 0.0000e+00
Epoch 281/300
13702/13702 [==============================] - 1s - loss: 1.1518e-04 - acc: 0.0000e+00 - val_loss: 0.0354 - val_acc: 0.0000e+00
Epoch 282/300
13702/13702 [==============================] - 1s - loss: 1.1142e-04 - acc: 0.0000e+00 - val_loss: 0.0377 - val_acc: 0.0000e+00
Epoch 283/300
13702/13702 [==============================] - 1s - loss: 1.3169e-04 - acc: 0.0000e+00 - val_loss: 0.0380 - val_acc: 0.0000e+00
Epoch 284/300
13702/13702 [==============================] - 1s - loss: 1.1358e-04 - acc: 0.0000e+00 - val_loss: 0.0364 - val_acc: 0.0000e+00
Epoch 285/300
13702/13702 [==============================] - 1s - loss: 1.1650e-04 - acc: 0.0000e+00 - val_loss: 0.0365 - val_acc: 0.0000e+00
Epoch 286/300
13702/13702 [==============================] - 1s - loss: 1.1636e-04 - acc: 0.0000e+00 - val_loss: 0.0363 - val_acc: 0.0000e+00
Epoch 287/300
13702/13702 [==============================] - 1s - loss: 1.2818e-04 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00
Epoch 288/300
13702/13702 [==============================] - 1s - loss: 1.1816e-04 - acc: 0.0000e+00 - val_loss: 0.0349 - val_acc: 0.0000e+00
Epoch 289/300
13702/13702 [==============================] - 1s - loss: 1.0846e-04 - acc: 0.0000e+00 - val_loss: 0.0374 - val_acc: 0.0000e+00
Epoch 290/300
13702/13702 [==============================] - 1s - loss: 1.1562e-04 - acc: 0.0000e+00 - val_loss: 0.0356 - val_acc: 0.0000e+00
Epoch 291/300
13702/13702 [==============================] - 1s - loss: 1.1277e-04 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0000e+00
Epoch 292/300
13702/13702 [==============================] - 1s - loss: 1.1469e-04 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00
Epoch 293/300
13702/13702 [==============================] - 1s - loss: 1.1035e-04 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00
Epoch 294/300
13702/13702 [==============================] - 1s - loss: 1.1818e-04 - acc: 0.0000e+00 - val_loss: 0.0352 - val_acc: 0.0000e+00
Epoch 295/300
13702/13702 [==============================] - 1s - loss: 1.1572e-04 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0000e+00
Epoch 296/300
13702/13702 [==============================] - 1s - loss: 1.1498e-04 - acc: 0.0000e+00 - val_loss: 0.0373 - val_acc: 0.0000e+00
Epoch 297/300
13702/13702 [==============================] - 1s - loss: 1.1335e-04 - acc: 0.0000e+00 - val_loss: 0.0334 - val_acc: 0.0000e+00
Epoch 298/300
13702/13702 [==============================] - 1s - loss: 1.0915e-04 - acc: 0.0000e+00 - val_loss: 0.0350 - val_acc: 0.0000e+00
Epoch 299/300
13702/13702 [==============================] - 1s - loss: 1.0312e-04 - acc: 0.0000e+00 - val_loss: 0.0351 - val_acc: 0.0000e+00
Epoch 300/300
13702/13702 [==============================] - 1s - loss: 1.1049e-04 - acc: 0.0000e+00 - val_loss: 0.0340 - val_acc: 0.0000e+00
Train Score: 0.00651 MSE (0.08 RMSE)
Test Score: 0.15820 MSE (0.40 RMSE)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">min_val</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">dropout_result</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
<span class="n">min_val_key</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dropout_result</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="n">min_val</span><span class="p">]</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">dropout_result</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">min_val_key</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>{0.2: 9.0012162654516434e-05, 0.3: 0.0022254438251234927, 0.8: 0.15819754752706974, 0.5: 0.01902325010867283, 0.7: 0.067955376763099332, 0.4: 0.01020233225110141, 0.6: 0.038046965636445065}
[0.2]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lists</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">dropout_result</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">lists</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Finding the best hyperparameter&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dropout&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Square Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>12.2 Optimial epochs value</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stock_name</span> <span class="o">=</span> <span class="s1">&#39;^GSPC&#39;</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># feature, window, output</span>
<span class="n">neurons</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">epochslist</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">90</span><span class="p">,</span><span class="mi">100</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epochs_result</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">epochs</span> <span class="ow">in</span> <span class="n">epochslist</span><span class="p">:</span>    
    <span class="n">trainScore</span><span class="p">,</span> <span class="n">testScore</span> <span class="o">=</span> <span class="n">quick_measure</span><span class="p">(</span><span class="n">stock_name</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">epochs</span><span class="p">)</span>
    <span class="n">epochs_result</span><span class="p">[</span><span class="n">epochs</span><span class="p">]</span> <span class="o">=</span> <span class="n">testScore</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_11 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_11 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_12 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_12 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_11 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_12 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/10
13702/13702 [==============================] - 3s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.0048 - val_acc: 0.0000e+00
Epoch 2/10
13702/13702 [==============================] - 1s - loss: 6.0977e-04 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00
Epoch 3/10
13702/13702 [==============================] - 1s - loss: 2.5514e-04 - acc: 0.0000e+00 - val_loss: 5.4461e-04 - val_acc: 0.0000e+00
Epoch 4/10
13702/13702 [==============================] - 1s - loss: 1.7068e-04 - acc: 0.0000e+00 - val_loss: 3.1617e-04 - val_acc: 0.0000e+00
Epoch 5/10
13702/13702 [==============================] - 1s - loss: 1.3632e-04 - acc: 0.0000e+00 - val_loss: 2.5412e-04 - val_acc: 0.0000e+00
Epoch 6/10
13702/13702 [==============================] - 1s - loss: 1.4826e-04 - acc: 0.0000e+00 - val_loss: 3.8467e-04 - val_acc: 0.0000e+00
Epoch 7/10
13702/13702 [==============================] - 1s - loss: 1.5060e-04 - acc: 0.0000e+00 - val_loss: 2.4584e-04 - val_acc: 0.0000e+00
Epoch 8/10
13702/13702 [==============================] - 1s - loss: 1.3162e-04 - acc: 0.0000e+00 - val_loss: 4.6549e-04 - val_acc: 0.0000e+00
Epoch 9/10
13702/13702 [==============================] - 1s - loss: 1.2821e-04 - acc: 0.0000e+00 - val_loss: 2.3023e-04 - val_acc: 0.0000e+00
Epoch 10/10
13702/13702 [==============================] - 1s - loss: 1.1644e-04 - acc: 0.0000e+00 - val_loss: 2.2494e-04 - val_acc: 0.0000e+00
Train Score: 0.00005 MSE (0.01 RMSE)
Test Score: 0.00688 MSE (0.08 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_13 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_13 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_14 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_14 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_13 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_14 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/20
13702/13702 [==============================] - 3s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00
Epoch 2/20
13702/13702 [==============================] - 1s - loss: 5.7648e-04 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00
Epoch 3/20
13702/13702 [==============================] - 1s - loss: 2.3047e-04 - acc: 0.0000e+00 - val_loss: 4.4981e-04 - val_acc: 0.0000e+000
Epoch 4/20
13702/13702 [==============================] - 1s - loss: 1.4836e-04 - acc: 0.0000e+00 - val_loss: 3.9520e-04 - val_acc: 0.0000e+00
Epoch 5/20
13702/13702 [==============================] - 1s - loss: 1.4473e-04 - acc: 0.0000e+00 - val_loss: 2.7514e-04 - val_acc: 0.0000e+00
Epoch 6/20
13702/13702 [==============================] - 1s - loss: 1.3865e-04 - acc: 0.0000e+00 - val_loss: 2.5957e-04 - val_acc: 0.0000e+00
Epoch 7/20
13702/13702 [==============================] - 1s - loss: 1.3782e-04 - acc: 0.0000e+00 - val_loss: 2.3727e-04 - val_acc: 0.0000e+00
Epoch 8/20
13702/13702 [==============================] - 1s - loss: 1.3275e-04 - acc: 0.0000e+00 - val_loss: 2.5933e-04 - val_acc: 0.0000e+00
Epoch 9/20
13702/13702 [==============================] - 1s - loss: 1.3449e-04 - acc: 0.0000e+00 - val_loss: 2.6785e-04 - val_acc: 0.0000e+00
Epoch 10/20
13702/13702 [==============================] - 1s - loss: 1.2307e-04 - acc: 0.0000e+00 - val_loss: 2.4295e-04 - val_acc: 0.0000e+00
Epoch 11/20
13702/13702 [==============================] - 1s - loss: 1.2362e-04 - acc: 0.0000e+00 - val_loss: 2.2618e-04 - val_acc: 0.0000e+00
Epoch 12/20
13702/13702 [==============================] - 1s - loss: 1.3013e-04 - acc: 0.0000e+00 - val_loss: 3.5044e-04 - val_acc: 0.0000e+00
Epoch 13/20
13702/13702 [==============================] - 1s - loss: 1.2809e-04 - acc: 0.0000e+00 - val_loss: 2.9878e-04 - val_acc: 0.0000e+00
Epoch 14/20
13702/13702 [==============================] - 1s - loss: 1.2076e-04 - acc: 0.0000e+00 - val_loss: 2.2057e-04 - val_acc: 0.0000e+00
Epoch 15/20
13702/13702 [==============================] - 1s - loss: 1.2301e-04 - acc: 0.0000e+00 - val_loss: 2.4244e-04 - val_acc: 0.0000e+00
Epoch 16/20
13702/13702 [==============================] - 1s - loss: 1.0470e-04 - acc: 0.0000e+00 - val_loss: 2.4535e-04 - val_acc: 0.0000e+00
Epoch 17/20
13702/13702 [==============================] - 1s - loss: 1.1185e-04 - acc: 0.0000e+00 - val_loss: 2.7684e-04 - val_acc: 0.0000e+00
Epoch 18/20
13702/13702 [==============================] - 1s - loss: 1.1273e-04 - acc: 0.0000e+00 - val_loss: 2.0549e-04 - val_acc: 0.0000e+00
Epoch 19/20
13702/13702 [==============================] - 1s - loss: 1.0107e-04 - acc: 0.0000e+00 - val_loss: 2.0827e-04 - val_acc: 0.0000e+00
Epoch 20/20
13702/13702 [==============================] - 1s - loss: 1.1576e-04 - acc: 0.0000e+00 - val_loss: 1.9721e-04 - val_acc: 0.0000e+00
Train Score: 0.00005 MSE (0.01 RMSE)
Test Score: 0.00399 MSE (0.06 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_15 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_15 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_16 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_16 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_15 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_16 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/30
13702/13702 [==============================] - 3s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 2/30
13702/13702 [==============================] - 1s - loss: 7.0148e-04 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 3/30
13702/13702 [==============================] - 1s - loss: 2.9509e-04 - acc: 0.0000e+00 - val_loss: 6.0482e-04 - val_acc: 0.0000e+00
Epoch 4/30
13702/13702 [==============================] - 1s - loss: 1.7697e-04 - acc: 0.0000e+00 - val_loss: 3.1067e-04 - val_acc: 0.0000e+00
Epoch 5/30
13702/13702 [==============================] - 1s - loss: 1.5215e-04 - acc: 0.0000e+00 - val_loss: 4.4219e-04 - val_acc: 0.0000e+00
Epoch 6/30
13702/13702 [==============================] - 1s - loss: 1.4073e-04 - acc: 0.0000e+00 - val_loss: 2.6874e-04 - val_acc: 0.0000e+00
Epoch 7/30
13702/13702 [==============================] - 1s - loss: 1.3135e-04 - acc: 0.0000e+00 - val_loss: 2.7715e-04 - val_acc: 0.0000e+00
Epoch 8/30
13702/13702 [==============================] - 1s - loss: 1.3380e-04 - acc: 0.0000e+00 - val_loss: 2.3787e-04 - val_acc: 0.0000e+00
Epoch 9/30
13702/13702 [==============================] - 1s - loss: 1.2737e-04 - acc: 0.0000e+00 - val_loss: 2.4054e-04 - val_acc: 0.0000e+00
Epoch 10/30
13702/13702 [==============================] - 1s - loss: 1.2328e-04 - acc: 0.0000e+00 - val_loss: 2.3832e-04 - val_acc: 0.0000e+00
Epoch 11/30
13702/13702 [==============================] - 1s - loss: 1.2693e-04 - acc: 0.0000e+00 - val_loss: 2.9406e-04 - val_acc: 0.0000e+00
Epoch 12/30
13702/13702 [==============================] - 1s - loss: 1.1905e-04 - acc: 0.0000e+00 - val_loss: 2.9851e-04 - val_acc: 0.0000e+00
Epoch 13/30
13702/13702 [==============================] - 1s - loss: 1.1706e-04 - acc: 0.0000e+00 - val_loss: 3.2124e-04 - val_acc: 0.0000e+00
Epoch 14/30
13702/13702 [==============================] - 1s - loss: 1.2192e-04 - acc: 0.0000e+00 - val_loss: 2.5944e-04 - val_acc: 0.0000e+00
Epoch 15/30
13702/13702 [==============================] - 1s - loss: 1.2931e-04 - acc: 0.0000e+00 - val_loss: 2.6223e-04 - val_acc: 0.0000e+00
Epoch 16/30
13702/13702 [==============================] - 1s - loss: 1.1747e-04 - acc: 0.0000e+00 - val_loss: 2.4936e-04 - val_acc: 0.0000e+00
Epoch 17/30
13702/13702 [==============================] - 1s - loss: 1.0972e-04 - acc: 0.0000e+00 - val_loss: 2.3647e-04 - val_acc: 0.0000e+00
Epoch 18/30
13702/13702 [==============================] - 1s - loss: 1.0306e-04 - acc: 0.0000e+00 - val_loss: 2.2543e-04 - val_acc: 0.0000e+00
Epoch 19/30
13702/13702 [==============================] - 1s - loss: 1.1887e-04 - acc: 0.0000e+00 - val_loss: 2.0989e-04 - val_acc: 0.0000e+00
Epoch 20/30
13702/13702 [==============================] - 1s - loss: 1.0859e-04 - acc: 0.0000e+00 - val_loss: 3.8667e-04 - val_acc: 0.0000e+00
Epoch 21/30
13702/13702 [==============================] - 1s - loss: 1.1471e-04 - acc: 0.0000e+00 - val_loss: 3.6375e-04 - val_acc: 0.0000e+00
Epoch 22/30
13702/13702 [==============================] - 1s - loss: 1.2573e-04 - acc: 0.0000e+00 - val_loss: 2.6742e-04 - val_acc: 0.0000e+00
Epoch 23/30
13702/13702 [==============================] - 1s - loss: 1.1244e-04 - acc: 0.0000e+00 - val_loss: 2.6535e-04 - val_acc: 0.0000e+00
Epoch 24/30
13702/13702 [==============================] - 1s - loss: 1.0619e-04 - acc: 0.0000e+00 - val_loss: 2.1584e-04 - val_acc: 0.0000e+00
Epoch 25/30
13702/13702 [==============================] - 1s - loss: 1.1700e-04 - acc: 0.0000e+00 - val_loss: 2.0474e-04 - val_acc: 0.0000e+00
Epoch 26/30
13702/13702 [==============================] - 1s - loss: 1.0436e-04 - acc: 0.0000e+00 - val_loss: 2.3404e-04 - val_acc: 0.0000e+00
Epoch 27/30
13702/13702 [==============================] - 1s - loss: 1.0469e-04 - acc: 0.0000e+00 - val_loss: 2.2938e-04 - val_acc: 0.0000e+00
Epoch 28/30
13702/13702 [==============================] - 1s - loss: 1.0402e-04 - acc: 0.0000e+00 - val_loss: 2.4717e-04 - val_acc: 0.0000e+00
Epoch 29/30
13702/13702 [==============================] - 1s - loss: 1.1419e-04 - acc: 0.0000e+00 - val_loss: 1.8406e-04 - val_acc: 0.0000e+00
Epoch 30/30
13702/13702 [==============================] - 1s - loss: 1.0101e-04 - acc: 0.0000e+00 - val_loss: 2.2434e-04 - val_acc: 0.0000e+00
Train Score: 0.00005 MSE (0.01 RMSE)
Test Score: 0.00248 MSE (0.05 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_17 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_17 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_18 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_18 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_17 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_18 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/40
13702/13702 [==============================] - 3s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00
Epoch 2/40
13702/13702 [==============================] - 1s - loss: 7.2688e-04 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 3/40
13702/13702 [==============================] - 1s - loss: 2.6004e-04 - acc: 0.0000e+00 - val_loss: 6.3789e-04 - val_acc: 0.0000e+00
Epoch 4/40
13702/13702 [==============================] - 1s - loss: 1.6260e-04 - acc: 0.0000e+00 - val_loss: 3.7565e-04 - val_acc: 0.0000e+00
Epoch 5/40
13702/13702 [==============================] - 1s - loss: 1.5108e-04 - acc: 0.0000e+00 - val_loss: 3.0041e-04 - val_acc: 0.0000e+00
Epoch 6/40
13702/13702 [==============================] - 1s - loss: 1.4047e-04 - acc: 0.0000e+00 - val_loss: 2.4641e-04 - val_acc: 0.0000e+00
Epoch 7/40
13702/13702 [==============================] - 1s - loss: 1.3508e-04 - acc: 0.0000e+00 - val_loss: 2.3550e-04 - val_acc: 0.0000e+00
Epoch 8/40
13702/13702 [==============================] - 1s - loss: 1.3071e-04 - acc: 0.0000e+00 - val_loss: 2.3340e-04 - val_acc: 0.0000e+00
Epoch 9/40
13702/13702 [==============================] - 1s - loss: 1.2566e-04 - acc: 0.0000e+00 - val_loss: 4.3926e-04 - val_acc: 0.0000e+00
Epoch 10/40
13702/13702 [==============================] - 1s - loss: 1.3430e-04 - acc: 0.0000e+00 - val_loss: 4.6782e-04 - val_acc: 0.0000e+00
Epoch 11/40
13702/13702 [==============================] - 1s - loss: 1.3022e-04 - acc: 0.0000e+00 - val_loss: 3.2948e-04 - val_acc: 0.0000e+00
Epoch 12/40
13702/13702 [==============================] - 1s - loss: 1.3279e-04 - acc: 0.0000e+00 - val_loss: 4.6956e-04 - val_acc: 0.0000e+00
Epoch 13/40
13702/13702 [==============================] - 1s - loss: 1.2543e-04 - acc: 0.0000e+00 - val_loss: 2.7856e-04 - val_acc: 0.0000e+00
Epoch 14/40
13702/13702 [==============================] - 1s - loss: 1.2903e-04 - acc: 0.0000e+00 - val_loss: 3.0534e-04 - val_acc: 0.0000e+00
Epoch 15/40
13702/13702 [==============================] - 1s - loss: 1.1479e-04 - acc: 0.0000e+00 - val_loss: 2.4125e-04 - val_acc: 0.0000e+00
Epoch 16/40
13702/13702 [==============================] - 1s - loss: 1.1614e-04 - acc: 0.0000e+00 - val_loss: 2.1554e-04 - val_acc: 0.0000e+00
Epoch 17/40
13702/13702 [==============================] - 1s - loss: 1.1369e-04 - acc: 0.0000e+00 - val_loss: 2.4213e-04 - val_acc: 0.0000e+00
Epoch 18/40
13702/13702 [==============================] - 1s - loss: 1.0457e-04 - acc: 0.0000e+00 - val_loss: 2.7168e-04 - val_acc: 0.0000e+00
Epoch 19/40
13702/13702 [==============================] - 1s - loss: 1.0646e-04 - acc: 0.0000e+00 - val_loss: 2.0539e-04 - val_acc: 0.0000e+00
Epoch 20/40
13702/13702 [==============================] - 1s - loss: 1.1510e-04 - acc: 0.0000e+00 - val_loss: 2.0423e-04 - val_acc: 0.0000e+00
Epoch 21/40
13702/13702 [==============================] - 1s - loss: 1.3198e-04 - acc: 0.0000e+00 - val_loss: 3.1054e-04 - val_acc: 0.0000e+00
Epoch 22/40
13702/13702 [==============================] - 1s - loss: 1.1354e-04 - acc: 0.0000e+00 - val_loss: 3.9297e-04 - val_acc: 0.0000e+00
Epoch 23/40
13702/13702 [==============================] - 1s - loss: 1.1534e-04 - acc: 0.0000e+00 - val_loss: 2.6460e-04 - val_acc: 0.0000e+00
Epoch 24/40
13702/13702 [==============================] - 1s - loss: 1.0624e-04 - acc: 0.0000e+00 - val_loss: 2.4153e-04 - val_acc: 0.0000e+00
Epoch 25/40
13702/13702 [==============================] - 1s - loss: 1.1315e-04 - acc: 0.0000e+00 - val_loss: 2.5060e-04 - val_acc: 0.0000e+00
Epoch 26/40
13702/13702 [==============================] - 1s - loss: 1.0642e-04 - acc: 0.0000e+00 - val_loss: 2.1543e-04 - val_acc: 0.0000e+00
Epoch 27/40
13702/13702 [==============================] - 1s - loss: 9.6967e-05 - acc: 0.0000e+00 - val_loss: 2.0206e-04 - val_acc: 0.0000e+00
Epoch 28/40
13702/13702 [==============================] - 1s - loss: 1.0736e-04 - acc: 0.0000e+00 - val_loss: 2.9245e-04 - val_acc: 0.0000e+00
Epoch 29/40
13702/13702 [==============================] - 1s - loss: 1.0128e-04 - acc: 0.0000e+00 - val_loss: 2.7116e-04 - val_acc: 0.0000e+00
Epoch 30/40
13702/13702 [==============================] - 1s - loss: 9.9848e-05 - acc: 0.0000e+00 - val_loss: 1.7949e-04 - val_acc: 0.0000e+00
Epoch 31/40
13702/13702 [==============================] - 1s - loss: 1.1545e-04 - acc: 0.0000e+00 - val_loss: 6.1536e-04 - val_acc: 0.0000e+00
Epoch 32/40
13702/13702 [==============================] - 1s - loss: 1.0434e-04 - acc: 0.0000e+00 - val_loss: 1.9542e-04 - val_acc: 0.0000e+00
Epoch 33/40
13702/13702 [==============================] - 1s - loss: 9.5638e-05 - acc: 0.0000e+00 - val_loss: 1.7668e-04 - val_acc: 0.0000e+00
Epoch 34/40
13702/13702 [==============================] - 1s - loss: 9.9248e-05 - acc: 0.0000e+00 - val_loss: 1.8277e-04 - val_acc: 0.0000e+00
Epoch 35/40
13702/13702 [==============================] - 1s - loss: 1.0626e-04 - acc: 0.0000e+00 - val_loss: 1.6658e-04 - val_acc: 0.0000e+00
Epoch 36/40
13702/13702 [==============================] - 1s - loss: 9.3601e-05 - acc: 0.0000e+00 - val_loss: 1.7512e-04 - val_acc: 0.0000e+00
Epoch 37/40
13702/13702 [==============================] - 1s - loss: 9.3409e-05 - acc: 0.0000e+00 - val_loss: 5.2558e-04 - val_acc: 0.0000e+00
Epoch 38/40
13702/13702 [==============================] - 1s - loss: 1.1318e-04 - acc: 0.0000e+00 - val_loss: 1.6685e-04 - val_acc: 0.0000e+00
Epoch 39/40
13702/13702 [==============================] - 1s - loss: 1.0324e-04 - acc: 0.0000e+00 - val_loss: 3.4065e-04 - val_acc: 0.0000e+00
Epoch 40/40
13702/13702 [==============================] - 1s - loss: 9.8323e-05 - acc: 0.0000e+00 - val_loss: 1.6378e-04 - val_acc: 0.0000e+00
Train Score: 0.00004 MSE (0.01 RMSE)
Test Score: 0.00154 MSE (0.04 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_19 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_19 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_20 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_20 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_19 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_20 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/50
13702/13702 [==============================] - 3s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00
Epoch 2/50
13702/13702 [==============================] - 1s - loss: 6.1155e-04 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00
Epoch 3/50
13702/13702 [==============================] - 1s - loss: 2.3406e-04 - acc: 0.0000e+00 - val_loss: 5.1013e-04 - val_acc: 0.0000e+00
Epoch 4/50
13702/13702 [==============================] - 1s - loss: 1.5794e-04 - acc: 0.0000e+00 - val_loss: 3.3826e-04 - val_acc: 0.0000e+00
Epoch 5/50
13702/13702 [==============================] - 1s - loss: 1.3746e-04 - acc: 0.0000e+00 - val_loss: 2.8517e-04 - val_acc: 0.0000e+00
Epoch 6/50
13702/13702 [==============================] - 1s - loss: 1.3370e-04 - acc: 0.0000e+00 - val_loss: 3.0560e-04 - val_acc: 0.0000e+00
Epoch 7/50
13702/13702 [==============================] - 1s - loss: 1.3598e-04 - acc: 0.0000e+00 - val_loss: 2.5555e-04 - val_acc: 0.0000e+00
Epoch 8/50
13702/13702 [==============================] - 1s - loss: 1.3494e-04 - acc: 0.0000e+00 - val_loss: 2.4354e-04 - val_acc: 0.0000e+00
Epoch 9/50
13702/13702 [==============================] - 1s - loss: 1.2616e-04 - acc: 0.0000e+00 - val_loss: 2.4152e-04 - val_acc: 0.0000e+00
Epoch 10/50
13702/13702 [==============================] - 1s - loss: 1.2931e-04 - acc: 0.0000e+00 - val_loss: 2.3151e-04 - val_acc: 0.0000e+00
Epoch 11/50
13702/13702 [==============================] - 1s - loss: 1.2589e-04 - acc: 0.0000e+00 - val_loss: 2.2615e-04 - val_acc: 0.0000e+00
Epoch 12/50
13702/13702 [==============================] - 1s - loss: 1.1997e-04 - acc: 0.0000e+00 - val_loss: 2.9215e-04 - val_acc: 0.0000e+00
Epoch 13/50
13702/13702 [==============================] - 1s - loss: 1.2614e-04 - acc: 0.0000e+00 - val_loss: 2.4829e-04 - val_acc: 0.0000e+00
Epoch 14/50
13702/13702 [==============================] - 1s - loss: 1.2350e-04 - acc: 0.0000e+00 - val_loss: 2.4059e-04 - val_acc: 0.0000e+00
Epoch 15/50
13702/13702 [==============================] - 1s - loss: 1.1747e-04 - acc: 0.0000e+00 - val_loss: 2.8092e-04 - val_acc: 0.0000e+00
Epoch 16/50
13702/13702 [==============================] - 1s - loss: 1.2386e-04 - acc: 0.0000e+00 - val_loss: 2.1389e-04 - val_acc: 0.0000e+00
Epoch 17/50
13702/13702 [==============================] - 2s - loss: 1.5281e-04 - acc: 0.0000e+00 - val_loss: 7.2467e-04 - val_acc: 0.0000e+00
Epoch 18/50
13702/13702 [==============================] - 1s - loss: 1.3556e-04 - acc: 0.0000e+00 - val_loss: 2.1918e-04 - val_acc: 0.0000e+00
Epoch 19/50
13702/13702 [==============================] - 1s - loss: 1.2646e-04 - acc: 0.0000e+00 - val_loss: 6.1137e-04 - val_acc: 0.0000e+00
Epoch 20/50
13702/13702 [==============================] - 1s - loss: 1.1138e-04 - acc: 0.0000e+00 - val_loss: 2.0882e-04 - val_acc: 0.0000e+00
Epoch 21/50
13702/13702 [==============================] - 1s - loss: 1.0793e-04 - acc: 0.0000e+00 - val_loss: 3.8489e-04 - val_acc: 0.0000e+00
Epoch 22/50
13702/13702 [==============================] - 1s - loss: 1.1824e-04 - acc: 0.0000e+00 - val_loss: 3.1272e-04 - val_acc: 0.0000e+00
Epoch 23/50
13702/13702 [==============================] - 1s - loss: 1.1111e-04 - acc: 0.0000e+00 - val_loss: 3.2462e-04 - val_acc: 0.0000e+00
Epoch 24/50
13702/13702 [==============================] - 1s - loss: 1.0455e-04 - acc: 0.0000e+00 - val_loss: 2.5115e-04 - val_acc: 0.0000e+00
Epoch 25/50
13702/13702 [==============================] - 1s - loss: 1.1037e-04 - acc: 0.0000e+00 - val_loss: 2.3322e-04 - val_acc: 0.0000e+00
Epoch 26/50
13702/13702 [==============================] - 1s - loss: 1.0611e-04 - acc: 0.0000e+00 - val_loss: 2.0152e-04 - val_acc: 0.0000e+00
Epoch 27/50
13702/13702 [==============================] - 1s - loss: 1.2070e-04 - acc: 0.0000e+00 - val_loss: 2.0877e-04 - val_acc: 0.0000e+00
Epoch 28/50
13702/13702 [==============================] - 1s - loss: 1.1532e-04 - acc: 0.0000e+00 - val_loss: 3.0907e-04 - val_acc: 0.0000e+00
Epoch 29/50
13702/13702 [==============================] - 1s - loss: 1.0028e-04 - acc: 0.0000e+00 - val_loss: 1.8533e-04 - val_acc: 0.0000e+00
Epoch 30/50
13702/13702 [==============================] - 1s - loss: 9.8638e-05 - acc: 0.0000e+00 - val_loss: 1.9049e-04 - val_acc: 0.0000e+00
Epoch 31/50
13702/13702 [==============================] - 1s - loss: 9.9254e-05 - acc: 0.0000e+00 - val_loss: 1.9360e-04 - val_acc: 0.0000e+00
Epoch 32/50
13702/13702 [==============================] - 1s - loss: 1.0794e-04 - acc: 0.0000e+00 - val_loss: 1.9874e-04 - val_acc: 0.0000e+00
Epoch 33/50
13702/13702 [==============================] - 1s - loss: 1.0507e-04 - acc: 0.0000e+00 - val_loss: 4.4674e-04 - val_acc: 0.0000e+00
Epoch 34/50
13702/13702 [==============================] - 1s - loss: 1.0573e-04 - acc: 0.0000e+00 - val_loss: 4.1609e-04 - val_acc: 0.0000e+00
Epoch 35/50
13702/13702 [==============================] - 1s - loss: 1.0865e-04 - acc: 0.0000e+00 - val_loss: 1.7750e-04 - val_acc: 0.0000e+00
Epoch 36/50
13702/13702 [==============================] - 1s - loss: 9.2070e-05 - acc: 0.0000e+00 - val_loss: 1.7007e-04 - val_acc: 0.0000e+00
Epoch 37/50
13702/13702 [==============================] - 1s - loss: 1.0111e-04 - acc: 0.0000e+00 - val_loss: 1.8545e-04 - val_acc: 0.0000e+00
Epoch 38/50
13702/13702 [==============================] - 1s - loss: 9.9522e-05 - acc: 0.0000e+00 - val_loss: 1.8062e-04 - val_acc: 0.0000e+0000e+
Epoch 39/50
13702/13702 [==============================] - 1s - loss: 9.3203e-05 - acc: 0.0000e+00 - val_loss: 1.7256e-04 - val_acc: 0.0000e+00
Epoch 40/50
13702/13702 [==============================] - 1s - loss: 1.0144e-04 - acc: 0.0000e+00 - val_loss: 4.2150e-04 - val_acc: 0.0000e+00
Epoch 41/50
13702/13702 [==============================] - 1s - loss: 9.6557e-05 - acc: 0.0000e+00 - val_loss: 1.9540e-04 - val_acc: 0.0000e+00
Epoch 42/50
13702/13702 [==============================] - 1s - loss: 9.6155e-05 - acc: 0.0000e+00 - val_loss: 1.6172e-04 - val_acc: 0.0000e+00
Epoch 43/50
13702/13702 [==============================] - 1s - loss: 1.0363e-04 - acc: 0.0000e+00 - val_loss: 5.1332e-04 - val_acc: 0.0000e+00
Epoch 44/50
13702/13702 [==============================] - 1s - loss: 1.1237e-04 - acc: 0.0000e+00 - val_loss: 1.6689e-04 - val_acc: 0.0000e+00
Epoch 45/50
13702/13702 [==============================] - 1s - loss: 9.0297e-05 - acc: 0.0000e+00 - val_loss: 2.2556e-04 - val_acc: 0.0000e+00
Epoch 46/50
13702/13702 [==============================] - 1s - loss: 9.7386e-05 - acc: 0.0000e+00 - val_loss: 2.3616e-04 - val_acc: 0.0000e+00
Epoch 47/50
13702/13702 [==============================] - 1s - loss: 8.8869e-05 - acc: 0.0000e+00 - val_loss: 4.3205e-04 - val_acc: 0.0000e+00
Epoch 48/50
13702/13702 [==============================] - 1s - loss: 9.0987e-05 - acc: 0.0000e+00 - val_loss: 2.0496e-04 - val_acc: 0.0000e+00
Epoch 49/50
13702/13702 [==============================] - 1s - loss: 8.6313e-05 - acc: 0.0000e+00 - val_loss: 2.4355e-04 - val_acc: 0.0000e+00
Epoch 50/50
13702/13702 [==============================] - 1s - loss: 1.0821e-04 - acc: 0.0000e+00 - val_loss: 1.5587e-04 - val_acc: 0.0000e+00
Train Score: 0.00004 MSE (0.01 RMSE)
Test Score: 0.00170 MSE (0.04 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_21 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_21 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_22 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_22 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_21 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_22 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/60
13702/13702 [==============================] - 4s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00
Epoch 2/60
13702/13702 [==============================] - 1s - loss: 6.2190e-04 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 3/60
13702/13702 [==============================] - 1s - loss: 2.6113e-04 - acc: 0.0000e+00 - val_loss: 7.6390e-04 - val_acc: 0.0000e+00
Epoch 4/60
13702/13702 [==============================] - 1s - loss: 1.7826e-04 - acc: 0.0000e+00 - val_loss: 4.4907e-04 - val_acc: 0.0000e+00
Epoch 5/60
13702/13702 [==============================] - 1s - loss: 1.5425e-04 - acc: 0.0000e+00 - val_loss: 2.5851e-04 - val_acc: 0.0000e+00
Epoch 6/60
13702/13702 [==============================] - 1s - loss: 1.3637e-04 - acc: 0.0000e+00 - val_loss: 2.5611e-04 - val_acc: 0.0000e+00
Epoch 7/60
13702/13702 [==============================] - 1s - loss: 1.2954e-04 - acc: 0.0000e+00 - val_loss: 4.1833e-04 - val_acc: 0.0000e+00
Epoch 8/60
13702/13702 [==============================] - 1s - loss: 1.2692e-04 - acc: 0.0000e+00 - val_loss: 3.0945e-04 - val_acc: 0.0000e+00
Epoch 9/60
13702/13702 [==============================] - 1s - loss: 1.2373e-04 - acc: 0.0000e+00 - val_loss: 2.2293e-04 - val_acc: 0.0000e+00
Epoch 10/60
13702/13702 [==============================] - 1s - loss: 1.1538e-04 - acc: 0.0000e+00 - val_loss: 2.7871e-04 - val_acc: 0.0000e+00
Epoch 11/60
13702/13702 [==============================] - 1s - loss: 1.2489e-04 - acc: 0.0000e+00 - val_loss: 3.6884e-04 - val_acc: 0.0000e+00
Epoch 12/60
13702/13702 [==============================] - 1s - loss: 1.1446e-04 - acc: 0.0000e+00 - val_loss: 2.9499e-04 - val_acc: 0.0000e+00
Epoch 13/60
13702/13702 [==============================] - 1s - loss: 1.1719e-04 - acc: 0.0000e+00 - val_loss: 2.2603e-04 - val_acc: 0.0000e+00
Epoch 14/60
13702/13702 [==============================] - 1s - loss: 1.0939e-04 - acc: 0.0000e+00 - val_loss: 2.8784e-04 - val_acc: 0.0000e+00
Epoch 15/60
13702/13702 [==============================] - 1s - loss: 1.1245e-04 - acc: 0.0000e+00 - val_loss: 2.1426e-04 - val_acc: 0.0000e+00
Epoch 16/60
13702/13702 [==============================] - 1s - loss: 1.1381e-04 - acc: 0.0000e+00 - val_loss: 2.2118e-04 - val_acc: 0.0000e+00
Epoch 17/60
13702/13702 [==============================] - 1s - loss: 1.0448e-04 - acc: 0.0000e+00 - val_loss: 6.2976e-04 - val_acc: 0.0000e+00
Epoch 18/60
13702/13702 [==============================] - 1s - loss: 1.2278e-04 - acc: 0.0000e+00 - val_loss: 2.6181e-04 - val_acc: 0.0000e+00
Epoch 19/60
13702/13702 [==============================] - 1s - loss: 1.0484e-04 - acc: 0.0000e+00 - val_loss: 2.0276e-04 - val_acc: 0.0000e+00
Epoch 20/60
13702/13702 [==============================] - 1s - loss: 1.0132e-04 - acc: 0.0000e+00 - val_loss: 2.2941e-04 - val_acc: 0.0000e+00
Epoch 21/60
13702/13702 [==============================] - 1s - loss: 1.0528e-04 - acc: 0.0000e+00 - val_loss: 2.3044e-04 - val_acc: 0.0000e+00
Epoch 22/60
13702/13702 [==============================] - 1s - loss: 1.0038e-04 - acc: 0.0000e+00 - val_loss: 1.9034e-04 - val_acc: 0.0000e+00
Epoch 23/60
13702/13702 [==============================] - 1s - loss: 9.8397e-05 - acc: 0.0000e+00 - val_loss: 4.0959e-04 - val_acc: 0.0000e+00
Epoch 24/60
13702/13702 [==============================] - 1s - loss: 1.1769e-04 - acc: 0.0000e+00 - val_loss: 2.9766e-04 - val_acc: 0.0000e+00
Epoch 25/60
13702/13702 [==============================] - 1s - loss: 1.0180e-04 - acc: 0.0000e+00 - val_loss: 4.3567e-04 - val_acc: 0.0000e+00
Epoch 26/60
13702/13702 [==============================] - 1s - loss: 1.0523e-04 - acc: 0.0000e+00 - val_loss: 3.1456e-04 - val_acc: 0.0000e+00
Epoch 27/60
13702/13702 [==============================] - 1s - loss: 1.1944e-04 - acc: 0.0000e+00 - val_loss: 1.8995e-04 - val_acc: 0.0000e+00
Epoch 28/60
13702/13702 [==============================] - 1s - loss: 9.7833e-05 - acc: 0.0000e+00 - val_loss: 5.1928e-04 - val_acc: 0.0000e+00
Epoch 29/60
13702/13702 [==============================] - 1s - loss: 1.0059e-04 - acc: 0.0000e+00 - val_loss: 2.1230e-04 - val_acc: 0.0000e+00
Epoch 30/60
13702/13702 [==============================] - 1s - loss: 9.8660e-05 - acc: 0.0000e+00 - val_loss: 1.8401e-04 - val_acc: 0.0000e+00
Epoch 31/60
13702/13702 [==============================] - 1s - loss: 9.4840e-05 - acc: 0.0000e+00 - val_loss: 1.7217e-04 - val_acc: 0.0000e+00
Epoch 32/60
13702/13702 [==============================] - 1s - loss: 1.0168e-04 - acc: 0.0000e+00 - val_loss: 6.7779e-04 - val_acc: 0.0000e+00
Epoch 33/60
13702/13702 [==============================] - 1s - loss: 1.0824e-04 - acc: 0.0000e+00 - val_loss: 2.2220e-04 - val_acc: 0.0000e+00
Epoch 34/60
13702/13702 [==============================] - 1s - loss: 1.0420e-04 - acc: 0.0000e+00 - val_loss: 2.2613e-04 - val_acc: 0.0000e+00
Epoch 35/60
13702/13702 [==============================] - 1s - loss: 9.5481e-05 - acc: 0.0000e+00 - val_loss: 1.6422e-04 - val_acc: 0.0000e+00
Epoch 36/60
13702/13702 [==============================] - 1s - loss: 8.8293e-05 - acc: 0.0000e+00 - val_loss: 1.6237e-04 - val_acc: 0.0000e+00
Epoch 37/60
13702/13702 [==============================] - 1s - loss: 9.4743e-05 - acc: 0.0000e+00 - val_loss: 1.8087e-04 - val_acc: 0.0000e+00
Epoch 38/60
13702/13702 [==============================] - 1s - loss: 1.0063e-04 - acc: 0.0000e+00 - val_loss: 3.6150e-04 - val_acc: 0.0000e+00
Epoch 39/60
13702/13702 [==============================] - 1s - loss: 9.1039e-05 - acc: 0.0000e+00 - val_loss: 1.6905e-04 - val_acc: 0.0000e+00
Epoch 40/60
13702/13702 [==============================] - 1s - loss: 1.0646e-04 - acc: 0.0000e+00 - val_loss: 1.7894e-04 - val_acc: 0.0000e+00
Epoch 41/60
13702/13702 [==============================] - 1s - loss: 1.0141e-04 - acc: 0.0000e+00 - val_loss: 2.6292e-04 - val_acc: 0.0000e+00
Epoch 42/60
13702/13702 [==============================] - 1s - loss: 1.0765e-04 - acc: 0.0000e+00 - val_loss: 1.5605e-04 - val_acc: 0.0000e+00
Epoch 43/60
13702/13702 [==============================] - 1s - loss: 9.4441e-05 - acc: 0.0000e+00 - val_loss: 1.7894e-04 - val_acc: 0.0000e+00
Epoch 44/60
13702/13702 [==============================] - 1s - loss: 9.1358e-05 - acc: 0.0000e+00 - val_loss: 2.0495e-04 - val_acc: 0.0000e+00
Epoch 45/60
13702/13702 [==============================] - 1s - loss: 9.7955e-05 - acc: 0.0000e+00 - val_loss: 2.2358e-04 - val_acc: 0.0000e+00
Epoch 46/60
13702/13702 [==============================] - 1s - loss: 1.0812e-04 - acc: 0.0000e+00 - val_loss: 1.5931e-04 - val_acc: 0.0000e+00
Epoch 47/60
13702/13702 [==============================] - 1s - loss: 8.4419e-05 - acc: 0.0000e+00 - val_loss: 2.1661e-04 - val_acc: 0.0000e+00
Epoch 48/60
13702/13702 [==============================] - 1s - loss: 9.6623e-05 - acc: 0.0000e+00 - val_loss: 3.0909e-04 - val_acc: 0.0000e+00
Epoch 49/60
13702/13702 [==============================] - 1s - loss: 9.6570e-05 - acc: 0.0000e+00 - val_loss: 1.7505e-04 - val_acc: 0.0000e+00
Epoch 50/60
13702/13702 [==============================] - 1s - loss: 9.9247e-05 - acc: 0.0000e+00 - val_loss: 3.6997e-04 - val_acc: 0.0000e+00
Epoch 51/60
13702/13702 [==============================] - 1s - loss: 9.2225e-05 - acc: 0.0000e+00 - val_loss: 1.4757e-04 - val_acc: 0.0000e+00
Epoch 52/60
13702/13702 [==============================] - 1s - loss: 8.3969e-05 - acc: 0.0000e+00 - val_loss: 2.3041e-04 - val_acc: 0.0000e+00
Epoch 53/60
13702/13702 [==============================] - 1s - loss: 8.8529e-05 - acc: 0.0000e+00 - val_loss: 1.5467e-04 - val_acc: 0.0000e+00
Epoch 54/60
13702/13702 [==============================] - 1s - loss: 8.9060e-05 - acc: 0.0000e+00 - val_loss: 1.4755e-04 - val_acc: 0.0000e+00
Epoch 55/60
13702/13702 [==============================] - 1s - loss: 8.3598e-05 - acc: 0.0000e+00 - val_loss: 1.3752e-04 - val_acc: 0.0000e+00
Epoch 56/60
13702/13702 [==============================] - 1s - loss: 8.8377e-05 - acc: 0.0000e+00 - val_loss: 2.4285e-04 - val_acc: 0.0000e+00
Epoch 57/60
13702/13702 [==============================] - 1s - loss: 9.5100e-05 - acc: 0.0000e+00 - val_loss: 1.7516e-04 - val_acc: 0.0000e+00
Epoch 58/60
13702/13702 [==============================] - 1s - loss: 8.7111e-05 - acc: 0.0000e+00 - val_loss: 2.2360e-04 - val_acc: 0.0000e+00
Epoch 59/60
13702/13702 [==============================] - 1s - loss: 1.1126e-04 - acc: 0.0000e+00 - val_loss: 4.3956e-04 - val_acc: 0.0000e+00
Epoch 60/60
13702/13702 [==============================] - 1s - loss: 9.5998e-05 - acc: 0.0000e+00 - val_loss: 2.6369e-04 - val_acc: 0.0000e+00
Train Score: 0.00006 MSE (0.01 RMSE)
Test Score: 0.00199 MSE (0.04 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_23 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_23 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_24 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_24 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_23 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_24 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/70
13702/13702 [==============================] - 3s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0034 - val_acc: 0.0000e+00
Epoch 2/70
13702/13702 [==============================] - 1s - loss: 5.3582e-04 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 3/70
13702/13702 [==============================] - 1s - loss: 2.2881e-04 - acc: 0.0000e+00 - val_loss: 5.0299e-04 - val_acc: 0.0000e+00
Epoch 4/70
13702/13702 [==============================] - 1s - loss: 1.6550e-04 - acc: 0.0000e+00 - val_loss: 5.0301e-04 - val_acc: 0.0000e+00
Epoch 5/70
13702/13702 [==============================] - 1s - loss: 1.4928e-04 - acc: 0.0000e+00 - val_loss: 3.9074e-04 - val_acc: 0.0000e+00
Epoch 6/70
13702/13702 [==============================] - 1s - loss: 1.4639e-04 - acc: 0.0000e+00 - val_loss: 2.5880e-04 - val_acc: 0.0000e+00
Epoch 7/70
13702/13702 [==============================] - 1s - loss: 1.3689e-04 - acc: 0.0000e+00 - val_loss: 2.6389e-04 - val_acc: 0.0000e+00
Epoch 8/70
13702/13702 [==============================] - 1s - loss: 1.2885e-04 - acc: 0.0000e+00 - val_loss: 2.6504e-04 - val_acc: 0.0000e+00
Epoch 9/70
13702/13702 [==============================] - 1s - loss: 1.3452e-04 - acc: 0.0000e+00 - val_loss: 2.2839e-04 - val_acc: 0.0000e+00
Epoch 10/70
13702/13702 [==============================] - 1s - loss: 1.1642e-04 - acc: 0.0000e+00 - val_loss: 3.1370e-04 - val_acc: 0.0000e+00
Epoch 11/70
13702/13702 [==============================] - 1s - loss: 1.1932e-04 - acc: 0.0000e+00 - val_loss: 2.2707e-04 - val_acc: 0.0000e+00
Epoch 12/70
13702/13702 [==============================] - 1s - loss: 1.1174e-04 - acc: 0.0000e+00 - val_loss: 3.0295e-04 - val_acc: 0.0000e+00
Epoch 13/70
13702/13702 [==============================] - 1s - loss: 1.2153e-04 - acc: 0.0000e+00 - val_loss: 2.5243e-04 - val_acc: 0.0000e+00
Epoch 14/70
13702/13702 [==============================] - 1s - loss: 1.1446e-04 - acc: 0.0000e+00 - val_loss: 2.4334e-04 - val_acc: 0.0000e+00
Epoch 15/70
13702/13702 [==============================] - 1s - loss: 1.0439e-04 - acc: 0.0000e+00 - val_loss: 2.2274e-04 - val_acc: 0.0000e+00
Epoch 16/70
13702/13702 [==============================] - 1s - loss: 1.0652e-04 - acc: 0.0000e+00 - val_loss: 2.2469e-04 - val_acc: 0.0000e+00
Epoch 17/70
13702/13702 [==============================] - 1s - loss: 1.1017e-04 - acc: 0.0000e+00 - val_loss: 2.8146e-04 - val_acc: 0.0000e+00
Epoch 18/70
13702/13702 [==============================] - 1s - loss: 1.3383e-04 - acc: 0.0000e+00 - val_loss: 6.2469e-04 - val_acc: 0.0000e+00
Epoch 19/70
13702/13702 [==============================] - 1s - loss: 1.2334e-04 - acc: 0.0000e+00 - val_loss: 2.2057e-04 - val_acc: 0.0000e+00
Epoch 20/70
13702/13702 [==============================] - 1s - loss: 1.0665e-04 - acc: 0.0000e+00 - val_loss: 1.9796e-04 - val_acc: 0.0000e+00
Epoch 21/70
13702/13702 [==============================] - 1s - loss: 1.0551e-04 - acc: 0.0000e+00 - val_loss: 2.3774e-04 - val_acc: 0.0000e+00
Epoch 22/70
13702/13702 [==============================] - 1s - loss: 1.0147e-04 - acc: 0.0000e+00 - val_loss: 3.9446e-04 - val_acc: 0.0000e+00
Epoch 23/70
13702/13702 [==============================] - 1s - loss: 1.0268e-04 - acc: 0.0000e+00 - val_loss: 2.1510e-04 - val_acc: 0.0000e+00
Epoch 24/70
13702/13702 [==============================] - 1s - loss: 1.1566e-04 - acc: 0.0000e+00 - val_loss: 3.7664e-04 - val_acc: 0.0000e+00
Epoch 25/70
13702/13702 [==============================] - 1s - loss: 1.0949e-04 - acc: 0.0000e+00 - val_loss: 1.9650e-04 - val_acc: 0.0000e+00
Epoch 26/70
13702/13702 [==============================] - 1s - loss: 1.0191e-04 - acc: 0.0000e+00 - val_loss: 1.8888e-04 - val_acc: 0.0000e+00
Epoch 27/70
13702/13702 [==============================] - 1s - loss: 1.0533e-04 - acc: 0.0000e+00 - val_loss: 1.8349e-04 - val_acc: 0.0000e+00
Epoch 28/70
13702/13702 [==============================] - 1s - loss: 9.6611e-05 - acc: 0.0000e+00 - val_loss: 1.7928e-04 - val_acc: 0.0000e+00
Epoch 29/70
13702/13702 [==============================] - 1s - loss: 1.0848e-04 - acc: 0.0000e+00 - val_loss: 3.2516e-04 - val_acc: 0.0000e+00
Epoch 30/70
13702/13702 [==============================] - 1s - loss: 1.0761e-04 - acc: 0.0000e+00 - val_loss: 2.0436e-04 - val_acc: 0.0000e+00
Epoch 31/70
13702/13702 [==============================] - 1s - loss: 9.5905e-05 - acc: 0.0000e+00 - val_loss: 5.1163e-04 - val_acc: 0.0000e+00
Epoch 32/70
13702/13702 [==============================] - 1s - loss: 1.0422e-04 - acc: 0.0000e+00 - val_loss: 3.6409e-04 - val_acc: 0.0000e+00
Epoch 33/70
13702/13702 [==============================] - 1s - loss: 1.0747e-04 - acc: 0.0000e+00 - val_loss: 3.8852e-04 - val_acc: 0.0000e+00
Epoch 34/70
13702/13702 [==============================] - 1s - loss: 9.8784e-05 - acc: 0.0000e+00 - val_loss: 1.7454e-04 - val_acc: 0.0000e+00
Epoch 35/70
13702/13702 [==============================] - 1s - loss: 9.3227e-05 - acc: 0.0000e+00 - val_loss: 2.0640e-04 - val_acc: 0.0000e+00
Epoch 36/70
13702/13702 [==============================] - 1s - loss: 8.9350e-05 - acc: 0.0000e+00 - val_loss: 1.7662e-04 - val_acc: 0.0000e+00
Epoch 37/70
13702/13702 [==============================] - 1s - loss: 9.1767e-05 - acc: 0.0000e+00 - val_loss: 1.8748e-04 - val_acc: 0.0000e+00
Epoch 38/70
13702/13702 [==============================] - 1s - loss: 9.5590e-05 - acc: 0.0000e+00 - val_loss: 1.7979e-04 - val_acc: 0.0000e+00
Epoch 39/70
13702/13702 [==============================] - 1s - loss: 9.2854e-05 - acc: 0.0000e+00 - val_loss: 1.6606e-04 - val_acc: 0.0000e+00
Epoch 40/70
13702/13702 [==============================] - 1s - loss: 9.2675e-05 - acc: 0.0000e+00 - val_loss: 2.9546e-04 - val_acc: 0.0000e+00
Epoch 41/70
13702/13702 [==============================] - 1s - loss: 9.6049e-05 - acc: 0.0000e+00 - val_loss: 3.1068e-04 - val_acc: 0.0000e+00
Epoch 42/70
13702/13702 [==============================] - 1s - loss: 9.5226e-05 - acc: 0.0000e+00 - val_loss: 1.7401e-04 - val_acc: 0.0000e+00
Epoch 43/70
13702/13702 [==============================] - 1s - loss: 8.7237e-05 - acc: 0.0000e+00 - val_loss: 5.6944e-04 - val_acc: 0.0000e+00
Epoch 44/70
13702/13702 [==============================] - 1s - loss: 9.8594e-05 - acc: 0.0000e+00 - val_loss: 2.1229e-04 - val_acc: 0.0000e+00
Epoch 45/70
13702/13702 [==============================] - 1s - loss: 8.6426e-05 - acc: 0.0000e+00 - val_loss: 1.6023e-04 - val_acc: 0.0000e+00
Epoch 46/70
13702/13702 [==============================] - 1s - loss: 9.6209e-05 - acc: 0.0000e+00 - val_loss: 1.8303e-04 - val_acc: 0.0000e+00
Epoch 47/70
13702/13702 [==============================] - 1s - loss: 9.9519e-05 - acc: 0.0000e+00 - val_loss: 2.6528e-04 - val_acc: 0.0000e+00
Epoch 48/70
13702/13702 [==============================] - 1s - loss: 9.9857e-05 - acc: 0.0000e+00 - val_loss: 1.9774e-04 - val_acc: 0.0000e+00
Epoch 49/70
13702/13702 [==============================] - 1s - loss: 8.9208e-05 - acc: 0.0000e+00 - val_loss: 1.4867e-04 - val_acc: 0.0000e+00
Epoch 50/70
13702/13702 [==============================] - 1s - loss: 8.8011e-05 - acc: 0.0000e+00 - val_loss: 3.8530e-04 - val_acc: 0.0000e+00
Epoch 51/70
13702/13702 [==============================] - 1s - loss: 9.6348e-05 - acc: 0.0000e+00 - val_loss: 2.3431e-04 - val_acc: 0.0000e+00
Epoch 52/70
13702/13702 [==============================] - 1s - loss: 8.5395e-05 - acc: 0.0000e+00 - val_loss: 3.0095e-04 - val_acc: 0.0000e+00
Epoch 53/70
13702/13702 [==============================] - 1s - loss: 9.2310e-05 - acc: 0.0000e+00 - val_loss: 4.1776e-04 - val_acc: 0.0000e+00
Epoch 54/70
13702/13702 [==============================] - 1s - loss: 1.1565e-04 - acc: 0.0000e+00 - val_loss: 4.8927e-04 - val_acc: 0.0000e+00
Epoch 55/70
13702/13702 [==============================] - 1s - loss: 9.4294e-05 - acc: 0.0000e+00 - val_loss: 2.3631e-04 - val_acc: 0.0000e+00
Epoch 56/70
13702/13702 [==============================] - 1s - loss: 9.2164e-05 - acc: 0.0000e+00 - val_loss: 4.0009e-04 - val_acc: 0.0000e+00
Epoch 57/70
13702/13702 [==============================] - 1s - loss: 1.0545e-04 - acc: 0.0000e+00 - val_loss: 5.7392e-04 - val_acc: 0.0000e+00
Epoch 58/70
13702/13702 [==============================] - 1s - loss: 8.5057e-05 - acc: 0.0000e+00 - val_loss: 1.4124e-04 - val_acc: 0.0000e+00
Epoch 59/70
13702/13702 [==============================] - 1s - loss: 8.9082e-05 - acc: 0.0000e+00 - val_loss: 3.3778e-04 - val_acc: 0.0000e+00
Epoch 60/70
13702/13702 [==============================] - 1s - loss: 8.0651e-05 - acc: 0.0000e+00 - val_loss: 1.5310e-04 - val_acc: 0.0000e+00
Epoch 61/70
13702/13702 [==============================] - 1s - loss: 8.5129e-05 - acc: 0.0000e+00 - val_loss: 1.3490e-04 - val_acc: 0.0000e+00
Epoch 62/70
13702/13702 [==============================] - 1s - loss: 7.9467e-05 - acc: 0.0000e+00 - val_loss: 1.9223e-04 - val_acc: 0.0000e+00
Epoch 63/70
13702/13702 [==============================] - 1s - loss: 8.2660e-05 - acc: 0.0000e+00 - val_loss: 1.6330e-04 - val_acc: 0.0000e+00
Epoch 64/70
13702/13702 [==============================] - 1s - loss: 8.1874e-05 - acc: 0.0000e+00 - val_loss: 1.2952e-04 - val_acc: 0.0000e+00
Epoch 65/70
13702/13702 [==============================] - 1s - loss: 7.7345e-05 - acc: 0.0000e+00 - val_loss: 1.3393e-04 - val_acc: 0.0000e+00
Epoch 66/70
13702/13702 [==============================] - 1s - loss: 8.8544e-05 - acc: 0.0000e+00 - val_loss: 1.6032e-04 - val_acc: 0.0000e+00
Epoch 67/70
13702/13702 [==============================] - 1s - loss: 8.1075e-05 - acc: 0.0000e+00 - val_loss: 1.5639e-04 - val_acc: 0.0000e+00
Epoch 68/70
13702/13702 [==============================] - 1s - loss: 8.7115e-05 - acc: 0.0000e+00 - val_loss: 2.3282e-04 - val_acc: 0.0000e+00
Epoch 69/70
13702/13702 [==============================] - 1s - loss: 1.0141e-04 - acc: 0.0000e+00 - val_loss: 1.8696e-04 - val_acc: 0.0000e+00
Epoch 70/70
13702/13702 [==============================] - 1s - loss: 7.5550e-05 - acc: 0.0000e+00 - val_loss: 1.2372e-04 - val_acc: 0.0000e+00
Train Score: 0.00003 MSE (0.01 RMSE)
Test Score: 0.00077 MSE (0.03 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_25 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_25 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_26 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_26 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_25 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_26 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/80
13702/13702 [==============================] - 3s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.0059 - val_acc: 0.0000e+00
Epoch 2/80
13702/13702 [==============================] - 1s - loss: 6.6267e-04 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 3/80
13702/13702 [==============================] - 1s - loss: 2.4498e-04 - acc: 0.0000e+00 - val_loss: 5.5771e-04 - val_acc: 0.0000e+00
Epoch 4/80
13702/13702 [==============================] - 1s - loss: 1.7450e-04 - acc: 0.0000e+00 - val_loss: 3.4238e-04 - val_acc: 0.0000e+00
Epoch 5/80
13702/13702 [==============================] - 1s - loss: 1.4284e-04 - acc: 0.0000e+00 - val_loss: 2.8612e-04 - val_acc: 0.0000e+00
Epoch 6/80
13702/13702 [==============================] - 1s - loss: 1.4325e-04 - acc: 0.0000e+00 - val_loss: 2.5068e-04 - val_acc: 0.0000e+00
Epoch 7/80
13702/13702 [==============================] - 1s - loss: 1.2786e-04 - acc: 0.0000e+00 - val_loss: 2.4169e-04 - val_acc: 0.0000e+00
Epoch 8/80
13702/13702 [==============================] - 1s - loss: 1.3242e-04 - acc: 0.0000e+00 - val_loss: 2.5684e-04 - val_acc: 0.0000e+00
Epoch 9/80
13702/13702 [==============================] - 1s - loss: 1.3923e-04 - acc: 0.0000e+00 - val_loss: 2.4193e-04 - val_acc: 0.0000e+00
Epoch 10/80
13702/13702 [==============================] - 1s - loss: 1.2400e-04 - acc: 0.0000e+00 - val_loss: 2.2988e-04 - val_acc: 0.0000e+00
Epoch 11/80
13702/13702 [==============================] - 1s - loss: 1.1814e-04 - acc: 0.0000e+00 - val_loss: 2.4150e-04 - val_acc: 0.0000e+0000e+0
Epoch 12/80
13702/13702 [==============================] - 1s - loss: 1.1486e-04 - acc: 0.0000e+00 - val_loss: 2.5128e-04 - val_acc: 0.0000e+00
Epoch 13/80
13702/13702 [==============================] - 1s - loss: 1.1179e-04 - acc: 0.0000e+00 - val_loss: 2.5135e-04 - val_acc: 0.0000e+00
Epoch 14/80
13702/13702 [==============================] - 1s - loss: 1.1877e-04 - acc: 0.0000e+00 - val_loss: 2.2166e-04 - val_acc: 0.0000e+00
Epoch 15/80
13702/13702 [==============================] - 1s - loss: 1.2411e-04 - acc: 0.0000e+00 - val_loss: 2.5036e-04 - val_acc: 0.0000e+00
Epoch 16/80
13702/13702 [==============================] - 1s - loss: 1.0942e-04 - acc: 0.0000e+00 - val_loss: 3.0699e-04 - val_acc: 0.0000e+00
Epoch 17/80
13702/13702 [==============================] - 1s - loss: 1.1899e-04 - acc: 0.0000e+00 - val_loss: 2.7711e-04 - val_acc: 0.0000e+00
Epoch 18/80
13702/13702 [==============================] - 1s - loss: 1.0949e-04 - acc: 0.0000e+00 - val_loss: 2.3521e-04 - val_acc: 0.0000e+00
Epoch 19/80
13702/13702 [==============================] - 1s - loss: 1.1111e-04 - acc: 0.0000e+00 - val_loss: 4.0093e-04 - val_acc: 0.0000e+00
Epoch 20/80
13702/13702 [==============================] - 1s - loss: 1.1555e-04 - acc: 0.0000e+00 - val_loss: 2.0657e-04 - val_acc: 0.0000e+00
Epoch 21/80
13702/13702 [==============================] - 1s - loss: 1.1336e-04 - acc: 0.0000e+00 - val_loss: 6.0995e-04 - val_acc: 0.0000e+00
Epoch 22/80
13702/13702 [==============================] - 1s - loss: 1.1706e-04 - acc: 0.0000e+00 - val_loss: 1.9505e-04 - val_acc: 0.0000e+00
Epoch 23/80
13702/13702 [==============================] - 1s - loss: 1.0549e-04 - acc: 0.0000e+00 - val_loss: 3.7993e-04 - val_acc: 0.0000e+0000
Epoch 24/80
13702/13702 [==============================] - 1s - loss: 1.0323e-04 - acc: 0.0000e+00 - val_loss: 4.6305e-04 - val_acc: 0.0000e+00
Epoch 25/80
13702/13702 [==============================] - 1s - loss: 1.0563e-04 - acc: 0.0000e+00 - val_loss: 1.9053e-04 - val_acc: 0.0000e+00
Epoch 26/80
13702/13702 [==============================] - 1s - loss: 1.0076e-04 - acc: 0.0000e+00 - val_loss: 1.9249e-04 - val_acc: 0.0000e+00
Epoch 27/80
13702/13702 [==============================] - 1s - loss: 1.0156e-04 - acc: 0.0000e+00 - val_loss: 1.7626e-04 - val_acc: 0.0000e+00
Epoch 28/80
13702/13702 [==============================] - 1s - loss: 1.0609e-04 - acc: 0.0000e+00 - val_loss: 2.2839e-04 - val_acc: 0.0000e+00
Epoch 29/80
13702/13702 [==============================] - 1s - loss: 1.2693e-04 - acc: 0.0000e+00 - val_loss: 3.9463e-04 - val_acc: 0.0000e+00
Epoch 30/80
13702/13702 [==============================] - 1s - loss: 9.6310e-05 - acc: 0.0000e+00 - val_loss: 1.7287e-04 - val_acc: 0.0000e+00
Epoch 31/80
13702/13702 [==============================] - 1s - loss: 9.8896e-05 - acc: 0.0000e+00 - val_loss: 2.1525e-04 - val_acc: 0.0000e+00
Epoch 32/80
13702/13702 [==============================] - 1s - loss: 1.0166e-04 - acc: 0.0000e+00 - val_loss: 2.3176e-04 - val_acc: 0.0000e+00
Epoch 33/80
13702/13702 [==============================] - 1s - loss: 9.4258e-05 - acc: 0.0000e+00 - val_loss: 2.6754e-04 - val_acc: 0.0000e+00
Epoch 34/80
13702/13702 [==============================] - 1s - loss: 9.6097e-05 - acc: 0.0000e+00 - val_loss: 1.6225e-04 - val_acc: 0.0000e+00
Epoch 35/80
13702/13702 [==============================] - 1s - loss: 9.8813e-05 - acc: 0.0000e+00 - val_loss: 1.6897e-04 - val_acc: 0.0000e+00
Epoch 36/80
13702/13702 [==============================] - 1s - loss: 9.3064e-05 - acc: 0.0000e+00 - val_loss: 1.7338e-04 - val_acc: 0.0000e+00
Epoch 37/80
13702/13702 [==============================] - 1s - loss: 1.2087e-04 - acc: 0.0000e+00 - val_loss: 1.8374e-04 - val_acc: 0.0000e+00
Epoch 38/80
13702/13702 [==============================] - 1s - loss: 1.0334e-04 - acc: 0.0000e+00 - val_loss: 1.7992e-04 - val_acc: 0.0000e+00
Epoch 39/80
13702/13702 [==============================] - 1s - loss: 1.0013e-04 - acc: 0.0000e+00 - val_loss: 1.7599e-04 - val_acc: 0.0000e+00
Epoch 40/80
13702/13702 [==============================] - 1s - loss: 9.7584e-05 - acc: 0.0000e+00 - val_loss: 2.8530e-04 - val_acc: 0.0000e+00
Epoch 41/80
13702/13702 [==============================] - 1s - loss: 9.0456e-05 - acc: 0.0000e+00 - val_loss: 1.5835e-04 - val_acc: 0.0000e+00
Epoch 42/80
13702/13702 [==============================] - 1s - loss: 8.7274e-05 - acc: 0.0000e+00 - val_loss: 3.0339e-04 - val_acc: 0.0000e+00
Epoch 43/80
13702/13702 [==============================] - 1s - loss: 8.9506e-05 - acc: 0.0000e+00 - val_loss: 1.7252e-04 - val_acc: 0.0000e+00
Epoch 44/80
13702/13702 [==============================] - 1s - loss: 9.5057e-05 - acc: 0.0000e+00 - val_loss: 2.2257e-04 - val_acc: 0.0000e+00
Epoch 45/80
13702/13702 [==============================] - 1s - loss: 8.5424e-05 - acc: 0.0000e+00 - val_loss: 2.0976e-04 - val_acc: 0.0000e+00
Epoch 46/80
13702/13702 [==============================] - 1s - loss: 9.1165e-05 - acc: 0.0000e+00 - val_loss: 2.2740e-04 - val_acc: 0.0000e+00
Epoch 47/80
13702/13702 [==============================] - 1s - loss: 9.0630e-05 - acc: 0.0000e+00 - val_loss: 1.6769e-04 - val_acc: 0.0000e+00
Epoch 48/80
13702/13702 [==============================] - 1s - loss: 8.8074e-05 - acc: 0.0000e+00 - val_loss: 1.4536e-04 - val_acc: 0.0000e+00
Epoch 49/80
13702/13702 [==============================] - 1s - loss: 9.2209e-05 - acc: 0.0000e+00 - val_loss: 1.7540e-04 - val_acc: 0.0000e+00
Epoch 50/80
13702/13702 [==============================] - 1s - loss: 9.2851e-05 - acc: 0.0000e+00 - val_loss: 2.5243e-04 - val_acc: 0.0000e+00
Epoch 51/80
13702/13702 [==============================] - 1s - loss: 8.7746e-05 - acc: 0.0000e+00 - val_loss: 1.4287e-04 - val_acc: 0.0000e+00
Epoch 52/80
13702/13702 [==============================] - 1s - loss: 9.5136e-05 - acc: 0.0000e+00 - val_loss: 1.7713e-04 - val_acc: 0.0000e+00
Epoch 53/80
13702/13702 [==============================] - 1s - loss: 8.4671e-05 - acc: 0.0000e+00 - val_loss: 1.5599e-04 - val_acc: 0.0000e+00
Epoch 54/80
13702/13702 [==============================] - 1s - loss: 9.1294e-05 - acc: 0.0000e+00 - val_loss: 5.7339e-04 - val_acc: 0.0000e+00
Epoch 55/80
13702/13702 [==============================] - 1s - loss: 8.7756e-05 - acc: 0.0000e+00 - val_loss: 2.1205e-04 - val_acc: 0.0000e+00
Epoch 56/80
13702/13702 [==============================] - 1s - loss: 9.5863e-05 - acc: 0.0000e+00 - val_loss: 2.8925e-04 - val_acc: 0.0000e+00
Epoch 57/80
13702/13702 [==============================] - 1s - loss: 9.4177e-05 - acc: 0.0000e+00 - val_loss: 2.1622e-04 - val_acc: 0.0000e+00
Epoch 58/80
13702/13702 [==============================] - 1s - loss: 8.4812e-05 - acc: 0.0000e+00 - val_loss: 4.5734e-04 - val_acc: 0.0000e+00
Epoch 59/80
13702/13702 [==============================] - 1s - loss: 9.0191e-05 - acc: 0.0000e+00 - val_loss: 1.3329e-04 - val_acc: 0.0000e+00
Epoch 60/80
13702/13702 [==============================] - 1s - loss: 7.9338e-05 - acc: 0.0000e+00 - val_loss: 1.4307e-04 - val_acc: 0.0000e+00
Epoch 61/80
13702/13702 [==============================] - 1s - loss: 7.8886e-05 - acc: 0.0000e+00 - val_loss: 1.3166e-04 - val_acc: 0.0000e+00
Epoch 62/80
13702/13702 [==============================] - 1s - loss: 8.0856e-05 - acc: 0.0000e+00 - val_loss: 4.1046e-04 - val_acc: 0.0000e+00
Epoch 63/80
13702/13702 [==============================] - 1s - loss: 8.5818e-05 - acc: 0.0000e+00 - val_loss: 1.5463e-04 - val_acc: 0.0000e+00
Epoch 64/80
13702/13702 [==============================] - 1s - loss: 7.6955e-05 - acc: 0.0000e+00 - val_loss: 2.3536e-04 - val_acc: 0.0000e+00
Epoch 65/80
13702/13702 [==============================] - 1s - loss: 8.5087e-05 - acc: 0.0000e+00 - val_loss: 1.3392e-04 - val_acc: 0.0000e+00
Epoch 66/80
13702/13702 [==============================] - 1s - loss: 8.4392e-05 - acc: 0.0000e+00 - val_loss: 2.1612e-04 - val_acc: 0.0000e+00
Epoch 67/80
13702/13702 [==============================] - 1s - loss: 8.1212e-05 - acc: 0.0000e+00 - val_loss: 1.4533e-04 - val_acc: 0.0000e+00
Epoch 68/80
13702/13702 [==============================] - 1s - loss: 7.6028e-05 - acc: 0.0000e+00 - val_loss: 1.2536e-04 - val_acc: 0.0000e+00
Epoch 69/80
13702/13702 [==============================] - 1s - loss: 7.5669e-05 - acc: 0.0000e+00 - val_loss: 1.2413e-04 - val_acc: 0.0000e+00
Epoch 70/80
13702/13702 [==============================] - 1s - loss: 8.5304e-05 - acc: 0.0000e+00 - val_loss: 4.5873e-04 - val_acc: 0.0000e+00
Epoch 71/80
13702/13702 [==============================] - 1s - loss: 9.2571e-05 - acc: 0.0000e+00 - val_loss: 1.8844e-04 - val_acc: 0.0000e+00
Epoch 72/80
13702/13702 [==============================] - 1s - loss: 7.7260e-05 - acc: 0.0000e+00 - val_loss: 1.3345e-04 - val_acc: 0.0000e+00
Epoch 73/80
13702/13702 [==============================] - 1s - loss: 7.1495e-05 - acc: 0.0000e+00 - val_loss: 1.5215e-04 - val_acc: 0.0000e+00
Epoch 74/80
13702/13702 [==============================] - 1s - loss: 8.2013e-05 - acc: 0.0000e+00 - val_loss: 1.5635e-04 - val_acc: 0.0000e+00
Epoch 75/80
13702/13702 [==============================] - 1s - loss: 7.6542e-05 - acc: 0.0000e+00 - val_loss: 2.3782e-04 - val_acc: 0.0000e+00
Epoch 76/80
13702/13702 [==============================] - 1s - loss: 7.9235e-05 - acc: 0.0000e+00 - val_loss: 1.4939e-04 - val_acc: 0.0000e+00
Epoch 77/80
13702/13702 [==============================] - 1s - loss: 8.4172e-05 - acc: 0.0000e+00 - val_loss: 1.2089e-04 - val_acc: 0.0000e+00
Epoch 78/80
13702/13702 [==============================] - 1s - loss: 7.3653e-05 - acc: 0.0000e+00 - val_loss: 1.7867e-04 - val_acc: 0.0000e+00
Epoch 79/80
13702/13702 [==============================] - 1s - loss: 8.6238e-05 - acc: 0.0000e+00 - val_loss: 1.2428e-04 - val_acc: 0.0000e+00
Epoch 80/80
13702/13702 [==============================] - 1s - loss: 7.3179e-05 - acc: 0.0000e+00 - val_loss: 1.3517e-04 - val_acc: 0.0000e+00
Train Score: 0.00003 MSE (0.01 RMSE)
Test Score: 0.00088 MSE (0.03 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_27 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_27 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_28 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_28 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_27 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_28 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/90
13702/13702 [==============================] - 3s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 2/90
13702/13702 [==============================] - 1s - loss: 6.6089e-04 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00
Epoch 3/90
13702/13702 [==============================] - 1s - loss: 2.5927e-04 - acc: 0.0000e+00 - val_loss: 5.5867e-04 - val_acc: 0.0000e+00
Epoch 4/90
13702/13702 [==============================] - 1s - loss: 1.6719e-04 - acc: 0.0000e+00 - val_loss: 3.8243e-04 - val_acc: 0.0000e+00
Epoch 5/90
13702/13702 [==============================] - 1s - loss: 1.5463e-04 - acc: 0.0000e+00 - val_loss: 3.4004e-04 - val_acc: 0.0000e+00
Epoch 6/90
13702/13702 [==============================] - 1s - loss: 1.4104e-04 - acc: 0.0000e+00 - val_loss: 2.8467e-04 - val_acc: 0.0000e+00
Epoch 7/90
13702/13702 [==============================] - 1s - loss: 1.4020e-04 - acc: 0.0000e+00 - val_loss: 2.3961e-04 - val_acc: 0.0000e+00
Epoch 8/90
13702/13702 [==============================] - 1s - loss: 1.4476e-04 - acc: 0.0000e+00 - val_loss: 2.4082e-04 - val_acc: 0.0000e+00
Epoch 9/90
13702/13702 [==============================] - 1s - loss: 1.2445e-04 - acc: 0.0000e+00 - val_loss: 2.7234e-04 - val_acc: 0.0000e+00
Epoch 10/90
13702/13702 [==============================] - 1s - loss: 1.3098e-04 - acc: 0.0000e+00 - val_loss: 2.3597e-04 - val_acc: 0.0000e+00
Epoch 11/90
13702/13702 [==============================] - 1s - loss: 1.3057e-04 - acc: 0.0000e+00 - val_loss: 2.2887e-04 - val_acc: 0.0000e+00
Epoch 12/90
13702/13702 [==============================] - 1s - loss: 1.5747e-04 - acc: 0.0000e+00 - val_loss: 3.1798e-04 - val_acc: 0.0000e+00
Epoch 13/90
13702/13702 [==============================] - 1s - loss: 1.2063e-04 - acc: 0.0000e+00 - val_loss: 2.2660e-04 - val_acc: 0.0000e+00
Epoch 14/90
13702/13702 [==============================] - 1s - loss: 1.2787e-04 - acc: 0.0000e+00 - val_loss: 4.2290e-04 - val_acc: 0.0000e+00
Epoch 15/90
13702/13702 [==============================] - 1s - loss: 1.2890e-04 - acc: 0.0000e+00 - val_loss: 2.6728e-04 - val_acc: 0.0000e+00
Epoch 16/90
13702/13702 [==============================] - 1s - loss: 1.1739e-04 - acc: 0.0000e+00 - val_loss: 2.2743e-04 - val_acc: 0.0000e+00
Epoch 17/90
13702/13702 [==============================] - 1s - loss: 1.1680e-04 - acc: 0.0000e+00 - val_loss: 2.1673e-04 - val_acc: 0.0000e+00
Epoch 18/90
13702/13702 [==============================] - 1s - loss: 1.1155e-04 - acc: 0.0000e+00 - val_loss: 3.0512e-04 - val_acc: 0.0000e+00
Epoch 19/90
13702/13702 [==============================] - 1s - loss: 1.1115e-04 - acc: 0.0000e+00 - val_loss: 2.3868e-04 - val_acc: 0.0000e+00
Epoch 20/90
13702/13702 [==============================] - 1s - loss: 1.0944e-04 - acc: 0.0000e+00 - val_loss: 4.0689e-04 - val_acc: 0.0000e+00
Epoch 21/90
13702/13702 [==============================] - 1s - loss: 1.1379e-04 - acc: 0.0000e+00 - val_loss: 4.3390e-04 - val_acc: 0.0000e+00
Epoch 22/90
13702/13702 [==============================] - 1s - loss: 1.0414e-04 - acc: 0.0000e+00 - val_loss: 1.9290e-04 - val_acc: 0.0000e+00
Epoch 23/90
13702/13702 [==============================] - 1s - loss: 1.0869e-04 - acc: 0.0000e+00 - val_loss: 1.8922e-04 - val_acc: 0.0000e+00
Epoch 24/90
13702/13702 [==============================] - 1s - loss: 1.0707e-04 - acc: 0.0000e+00 - val_loss: 2.2854e-04 - val_acc: 0.0000e+00
Epoch 25/90
13702/13702 [==============================] - 1s - loss: 1.0978e-04 - acc: 0.0000e+00 - val_loss: 1.9289e-04 - val_acc: 0.0000e+00
Epoch 26/90
13702/13702 [==============================] - 1s - loss: 1.0658e-04 - acc: 0.0000e+00 - val_loss: 4.5127e-04 - val_acc: 0.0000e+00
Epoch 27/90
13702/13702 [==============================] - 1s - loss: 1.1089e-04 - acc: 0.0000e+00 - val_loss: 2.4062e-04 - val_acc: 0.0000e+00
Epoch 28/90
13702/13702 [==============================] - 1s - loss: 1.0182e-04 - acc: 0.0000e+00 - val_loss: 1.7791e-04 - val_acc: 0.0000e+00
Epoch 29/90
13702/13702 [==============================] - 1s - loss: 9.7911e-05 - acc: 0.0000e+00 - val_loss: 2.1099e-04 - val_acc: 0.0000e+00
Epoch 30/90
13702/13702 [==============================] - 1s - loss: 9.8986e-05 - acc: 0.0000e+00 - val_loss: 2.5993e-04 - val_acc: 0.0000e+00
Epoch 31/90
13702/13702 [==============================] - 1s - loss: 9.6738e-05 - acc: 0.0000e+00 - val_loss: 4.7060e-04 - val_acc: 0.0000e+00
Epoch 32/90
13702/13702 [==============================] - 1s - loss: 1.0112e-04 - acc: 0.0000e+00 - val_loss: 1.6995e-04 - val_acc: 0.0000e+00
Epoch 33/90
13702/13702 [==============================] - 1s - loss: 9.1422e-05 - acc: 0.0000e+00 - val_loss: 1.8584e-04 - val_acc: 0.0000e+00
Epoch 34/90
13702/13702 [==============================] - 1s - loss: 9.4577e-05 - acc: 0.0000e+00 - val_loss: 2.0896e-04 - val_acc: 0.0000e+00
Epoch 35/90
13702/13702 [==============================] - 1s - loss: 9.4214e-05 - acc: 0.0000e+00 - val_loss: 1.7028e-04 - val_acc: 0.0000e+00
Epoch 36/90
13702/13702 [==============================] - 1s - loss: 9.6956e-05 - acc: 0.0000e+00 - val_loss: 1.7034e-04 - val_acc: 0.0000e+00
Epoch 37/90
13702/13702 [==============================] - 1s - loss: 9.6899e-05 - acc: 0.0000e+00 - val_loss: 3.3117e-04 - val_acc: 0.0000e+00
Epoch 38/90
13702/13702 [==============================] - 1s - loss: 9.0311e-05 - acc: 0.0000e+00 - val_loss: 1.7297e-04 - val_acc: 0.0000e+00
Epoch 39/90
13702/13702 [==============================] - 1s - loss: 8.4781e-05 - acc: 0.0000e+00 - val_loss: 2.3161e-04 - val_acc: 0.0000e+00
Epoch 40/90
13702/13702 [==============================] - 1s - loss: 9.2546e-05 - acc: 0.0000e+00 - val_loss: 4.0132e-04 - val_acc: 0.0000e+00
Epoch 41/90
13702/13702 [==============================] - 1s - loss: 9.0944e-05 - acc: 0.0000e+00 - val_loss: 2.1072e-04 - val_acc: 0.0000e+00
Epoch 42/90
13702/13702 [==============================] - 1s - loss: 8.4480e-05 - acc: 0.0000e+00 - val_loss: 1.9303e-04 - val_acc: 0.0000e+00
Epoch 43/90
13702/13702 [==============================] - 1s - loss: 1.0171e-04 - acc: 0.0000e+00 - val_loss: 7.2657e-04 - val_acc: 0.0000e+00
Epoch 44/90
13702/13702 [==============================] - 1s - loss: 1.0352e-04 - acc: 0.0000e+00 - val_loss: 2.7287e-04 - val_acc: 0.0000e+00
Epoch 45/90
13702/13702 [==============================] - 1s - loss: 9.9083e-05 - acc: 0.0000e+00 - val_loss: 6.3949e-04 - val_acc: 0.0000e+00
Epoch 46/90
13702/13702 [==============================] - 1s - loss: 1.0643e-04 - acc: 0.0000e+00 - val_loss: 2.8457e-04 - val_acc: 0.0000e+00
Epoch 47/90
13702/13702 [==============================] - 1s - loss: 9.4561e-05 - acc: 0.0000e+00 - val_loss: 1.4829e-04 - val_acc: 0.0000e+00
Epoch 48/90
13702/13702 [==============================] - 1s - loss: 8.3505e-05 - acc: 0.0000e+00 - val_loss: 2.7795e-04 - val_acc: 0.0000e+00
Epoch 49/90
13702/13702 [==============================] - 1s - loss: 1.0506e-04 - acc: 0.0000e+00 - val_loss: 1.5058e-04 - val_acc: 0.0000e+00
Epoch 50/90
13702/13702 [==============================] - 1s - loss: 9.1471e-05 - acc: 0.0000e+00 - val_loss: 1.8744e-04 - val_acc: 0.0000e+00
Epoch 51/90
13702/13702 [==============================] - 1s - loss: 8.4561e-05 - acc: 0.0000e+00 - val_loss: 1.8416e-04 - val_acc: 0.0000e+00
Epoch 52/90
13702/13702 [==============================] - 1s - loss: 7.8978e-05 - acc: 0.0000e+00 - val_loss: 1.9445e-04 - val_acc: 0.0000e+00
Epoch 53/90
13702/13702 [==============================] - 1s - loss: 9.8354e-05 - acc: 0.0000e+00 - val_loss: 1.4585e-04 - val_acc: 0.0000e+00
Epoch 54/90
13702/13702 [==============================] - 1s - loss: 9.3938e-05 - acc: 0.0000e+00 - val_loss: 1.3755e-04 - val_acc: 0.0000e+00
Epoch 55/90
13702/13702 [==============================] - 1s - loss: 9.1736e-05 - acc: 0.0000e+00 - val_loss: 1.6637e-04 - val_acc: 0.0000e+00
Epoch 56/90
13702/13702 [==============================] - 1s - loss: 7.7534e-05 - acc: 0.0000e+00 - val_loss: 1.3509e-04 - val_acc: 0.0000e+00
Epoch 57/90
13702/13702 [==============================] - 1s - loss: 9.3059e-05 - acc: 0.0000e+00 - val_loss: 1.4621e-04 - val_acc: 0.0000e+00
Epoch 58/90
13702/13702 [==============================] - 1s - loss: 8.5632e-05 - acc: 0.0000e+00 - val_loss: 2.0207e-04 - val_acc: 0.0000e+00
Epoch 59/90
13702/13702 [==============================] - 1s - loss: 8.5058e-05 - acc: 0.0000e+00 - val_loss: 1.4563e-04 - val_acc: 0.0000e+00
Epoch 60/90
13702/13702 [==============================] - 1s - loss: 9.3599e-05 - acc: 0.0000e+00 - val_loss: 2.7951e-04 - val_acc: 0.0000e+00
Epoch 61/90
13702/13702 [==============================] - 1s - loss: 1.0955e-04 - acc: 0.0000e+00 - val_loss: 2.6817e-04 - val_acc: 0.0000e+00
Epoch 62/90
13702/13702 [==============================] - 1s - loss: 1.1209e-04 - acc: 0.0000e+00 - val_loss: 1.5426e-04 - val_acc: 0.0000e+00
Epoch 63/90
13702/13702 [==============================] - 1s - loss: 9.3626e-05 - acc: 0.0000e+00 - val_loss: 3.6299e-04 - val_acc: 0.0000e+00
Epoch 64/90
13702/13702 [==============================] - 1s - loss: 8.8951e-05 - acc: 0.0000e+00 - val_loss: 2.5082e-04 - val_acc: 0.0000e+00
Epoch 65/90
13702/13702 [==============================] - 1s - loss: 8.6647e-05 - acc: 0.0000e+00 - val_loss: 1.3087e-04 - val_acc: 0.0000e+00
Epoch 66/90
13702/13702 [==============================] - 1s - loss: 7.4261e-05 - acc: 0.0000e+00 - val_loss: 1.5975e-04 - val_acc: 0.0000e+00
Epoch 67/90
13702/13702 [==============================] - 1s - loss: 7.6224e-05 - acc: 0.0000e+00 - val_loss: 1.9494e-04 - val_acc: 0.0000e+00
Epoch 68/90
13702/13702 [==============================] - 1s - loss: 7.7741e-05 - acc: 0.0000e+00 - val_loss: 1.4849e-04 - val_acc: 0.0000e+00
Epoch 69/90
13702/13702 [==============================] - 1s - loss: 7.2226e-05 - acc: 0.0000e+00 - val_loss: 1.2277e-04 - val_acc: 0.0000e+00
Epoch 70/90
13702/13702 [==============================] - 1s - loss: 7.3065e-05 - acc: 0.0000e+00 - val_loss: 2.9331e-04 - val_acc: 0.0000e+00
Epoch 71/90
13702/13702 [==============================] - 1s - loss: 7.1996e-05 - acc: 0.0000e+00 - val_loss: 1.7227e-04 - val_acc: 0.0000e+00
Epoch 72/90
13702/13702 [==============================] - 1s - loss: 7.0804e-05 - acc: 0.0000e+00 - val_loss: 1.5162e-04 - val_acc: 0.0000e+00
Epoch 73/90
13702/13702 [==============================] - 1s - loss: 7.6628e-05 - acc: 0.0000e+00 - val_loss: 2.8939e-04 - val_acc: 0.0000e+00
Epoch 74/90
13702/13702 [==============================] - 1s - loss: 8.1165e-05 - acc: 0.0000e+00 - val_loss: 4.1937e-04 - val_acc: 0.0000e+00
Epoch 75/90
13702/13702 [==============================] - 1s - loss: 8.6824e-05 - acc: 0.0000e+00 - val_loss: 4.4946e-04 - val_acc: 0.0000e+00
Epoch 76/90
13702/13702 [==============================] - 1s - loss: 1.0001e-04 - acc: 0.0000e+00 - val_loss: 6.7111e-04 - val_acc: 0.0000e+00
Epoch 77/90
13702/13702 [==============================] - 1s - loss: 9.8819e-05 - acc: 0.0000e+00 - val_loss: 1.3231e-04 - val_acc: 0.0000e+00
Epoch 78/90
13702/13702 [==============================] - 1s - loss: 8.2757e-05 - acc: 0.0000e+00 - val_loss: 1.1742e-04 - val_acc: 0.0000e+00
Epoch 79/90
13702/13702 [==============================] - 1s - loss: 6.6830e-05 - acc: 0.0000e+00 - val_loss: 1.1508e-04 - val_acc: 0.0000e+00
Epoch 80/90
13702/13702 [==============================] - 1s - loss: 7.4761e-05 - acc: 0.0000e+00 - val_loss: 1.1612e-04 - val_acc: 0.0000e+00
Epoch 81/90
13702/13702 [==============================] - 1s - loss: 7.0506e-05 - acc: 0.0000e+00 - val_loss: 1.1273e-04 - val_acc: 0.0000e+00
Epoch 82/90
13702/13702 [==============================] - 1s - loss: 7.3993e-05 - acc: 0.0000e+00 - val_loss: 1.1665e-04 - val_acc: 0.0000e+00
Epoch 83/90
13702/13702 [==============================] - 1s - loss: 7.2404e-05 - acc: 0.0000e+00 - val_loss: 1.8418e-04 - val_acc: 0.0000e+00
Epoch 84/90
13702/13702 [==============================] - 1s - loss: 8.3052e-05 - acc: 0.0000e+00 - val_loss: 1.3797e-04 - val_acc: 0.0000e+00
Epoch 85/90
13702/13702 [==============================] - 1s - loss: 7.3539e-05 - acc: 0.0000e+00 - val_loss: 1.1409e-04 - val_acc: 0.0000e+00
Epoch 86/90
13702/13702 [==============================] - 1s - loss: 7.0346e-05 - acc: 0.0000e+00 - val_loss: 1.1774e-04 - val_acc: 0.0000e+00
Epoch 87/90
13702/13702 [==============================] - 1s - loss: 7.2422e-05 - acc: 0.0000e+00 - val_loss: 1.1808e-04 - val_acc: 0.0000e+00
Epoch 88/90
13702/13702 [==============================] - 1s - loss: 6.8385e-05 - acc: 0.0000e+00 - val_loss: 1.0849e-04 - val_acc: 0.0000e+00
Epoch 89/90
13702/13702 [==============================] - 1s - loss: 7.2681e-05 - acc: 0.0000e+00 - val_loss: 1.0970e-04 - val_acc: 0.0000e+00
Epoch 90/90
13702/13702 [==============================] - 1s - loss: 6.5907e-05 - acc: 0.0000e+00 - val_loss: 1.4743e-04 - val_acc: 0.0000e+00
Train Score: 0.00004 MSE (0.01 RMSE)
Test Score: 0.00032 MSE (0.02 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_29 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_29 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_30 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_30 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_29 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_30 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13702 samples, validate on 1523 samples
Epoch 1/100
13702/13702 [==============================] - 3s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00
Epoch 2/100
13702/13702 [==============================] - 1s - loss: 6.3824e-04 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 3/100
13702/13702 [==============================] - 1s - loss: 2.3779e-04 - acc: 0.0000e+00 - val_loss: 5.2778e-04 - val_acc: 0.0000e+00
Epoch 4/100
13702/13702 [==============================] - 1s - loss: 1.5669e-04 - acc: 0.0000e+00 - val_loss: 3.4203e-04 - val_acc: 0.0000e+00
Epoch 5/100
13702/13702 [==============================] - 1s - loss: 1.4916e-04 - acc: 0.0000e+00 - val_loss: 3.9398e-04 - val_acc: 0.0000e+00
Epoch 6/100
13702/13702 [==============================] - 1s - loss: 1.3578e-04 - acc: 0.0000e+00 - val_loss: 2.6067e-04 - val_acc: 0.0000e+00
Epoch 7/100
13702/13702 [==============================] - 1s - loss: 1.3061e-04 - acc: 0.0000e+00 - val_loss: 2.4931e-04 - val_acc: 0.0000e+00
Epoch 8/100
13702/13702 [==============================] - 1s - loss: 1.4447e-04 - acc: 0.0000e+00 - val_loss: 2.4685e-04 - val_acc: 0.0000e+00
Epoch 9/100
13702/13702 [==============================] - 1s - loss: 1.3549e-04 - acc: 0.0000e+00 - val_loss: 3.0951e-04 - val_acc: 0.0000e+00
Epoch 10/100
13702/13702 [==============================] - 1s - loss: 1.1994e-04 - acc: 0.0000e+00 - val_loss: 2.3173e-04 - val_acc: 0.0000e+00
Epoch 11/100
13702/13702 [==============================] - 1s - loss: 1.1903e-04 - acc: 0.0000e+00 - val_loss: 3.9398e-04 - val_acc: 0.0000e+00
Epoch 12/100
13702/13702 [==============================] - 1s - loss: 1.2634e-04 - acc: 0.0000e+00 - val_loss: 2.4660e-04 - val_acc: 0.0000e+00
Epoch 13/100
13702/13702 [==============================] - 1s - loss: 1.1832e-04 - acc: 0.0000e+00 - val_loss: 2.2484e-04 - val_acc: 0.0000e+00
Epoch 14/100
13702/13702 [==============================] - 1s - loss: 1.2101e-04 - acc: 0.0000e+00 - val_loss: 2.2053e-04 - val_acc: 0.0000e+00
Epoch 15/100
13702/13702 [==============================] - 1s - loss: 1.2049e-04 - acc: 0.0000e+00 - val_loss: 2.2910e-04 - val_acc: 0.0000e+00
Epoch 16/100
13702/13702 [==============================] - 1s - loss: 1.1224e-04 - acc: 0.0000e+00 - val_loss: 2.2444e-04 - val_acc: 0.0000e+00
Epoch 17/100
13702/13702 [==============================] - 1s - loss: 1.1165e-04 - acc: 0.0000e+00 - val_loss: 3.2504e-04 - val_acc: 0.0000e+00
Epoch 18/100
13702/13702 [==============================] - 1s - loss: 1.0988e-04 - acc: 0.0000e+00 - val_loss: 2.0211e-04 - val_acc: 0.0000e+00
Epoch 19/100
13702/13702 [==============================] - 1s - loss: 1.1368e-04 - acc: 0.0000e+00 - val_loss: 2.5594e-04 - val_acc: 0.0000e+00
Epoch 20/100
13702/13702 [==============================] - 1s - loss: 1.3644e-04 - acc: 0.0000e+00 - val_loss: 2.6951e-04 - val_acc: 0.0000e+00
Epoch 21/100
13702/13702 [==============================] - 1s - loss: 1.1308e-04 - acc: 0.0000e+00 - val_loss: 2.6979e-04 - val_acc: 0.0000e+00
Epoch 22/100
13702/13702 [==============================] - 1s - loss: 1.0683e-04 - acc: 0.0000e+00 - val_loss: 1.9775e-04 - val_acc: 0.0000e+00
Epoch 23/100
13702/13702 [==============================] - 1s - loss: 1.0029e-04 - acc: 0.0000e+00 - val_loss: 2.0724e-04 - val_acc: 0.0000e+00
Epoch 24/100
13702/13702 [==============================] - 1s - loss: 1.0851e-04 - acc: 0.0000e+00 - val_loss: 3.7251e-04 - val_acc: 0.0000e+00
Epoch 25/100
13702/13702 [==============================] - 1s - loss: 1.0738e-04 - acc: 0.0000e+00 - val_loss: 2.5096e-04 - val_acc: 0.0000e+00
Epoch 26/100
13702/13702 [==============================] - 1s - loss: 1.1324e-04 - acc: 0.0000e+00 - val_loss: 2.0262e-04 - val_acc: 0.0000e+00
Epoch 27/100
13702/13702 [==============================] - 1s - loss: 1.0401e-04 - acc: 0.0000e+00 - val_loss: 3.0671e-04 - val_acc: 0.0000e+00
Epoch 28/100
13702/13702 [==============================] - 1s - loss: 1.0098e-04 - acc: 0.0000e+00 - val_loss: 4.7762e-04 - val_acc: 0.0000e+00
Epoch 29/100
13702/13702 [==============================] - 1s - loss: 1.1684e-04 - acc: 0.0000e+00 - val_loss: 1.8941e-04 - val_acc: 0.0000e+00
Epoch 30/100
13702/13702 [==============================] - 1s - loss: 1.0706e-04 - acc: 0.0000e+00 - val_loss: 3.4940e-04 - val_acc: 0.0000e+00
Epoch 31/100
13702/13702 [==============================] - 1s - loss: 1.2781e-04 - acc: 0.0000e+00 - val_loss: 1.8850e-04 - val_acc: 0.0000e+00
Epoch 32/100
13702/13702 [==============================] - 1s - loss: 1.0194e-04 - acc: 0.0000e+00 - val_loss: 1.9970e-04 - val_acc: 0.0000e+00
Epoch 33/100
13702/13702 [==============================] - 1s - loss: 1.2364e-04 - acc: 0.0000e+00 - val_loss: 2.8180e-04 - val_acc: 0.0000e+00
Epoch 34/100
13702/13702 [==============================] - 1s - loss: 1.0743e-04 - acc: 0.0000e+00 - val_loss: 1.7792e-04 - val_acc: 0.0000e+00
Epoch 35/100
13702/13702 [==============================] - 1s - loss: 9.4082e-05 - acc: 0.0000e+00 - val_loss: 1.9092e-04 - val_acc: 0.0000e+00
Epoch 36/100
13702/13702 [==============================] - 1s - loss: 1.0805e-04 - acc: 0.0000e+00 - val_loss: 1.7552e-04 - val_acc: 0.0000e+00
Epoch 37/100
13702/13702 [==============================] - 1s - loss: 9.7711e-05 - acc: 0.0000e+00 - val_loss: 2.2564e-04 - val_acc: 0.0000e+00
Epoch 38/100
13702/13702 [==============================] - 1s - loss: 9.8585e-05 - acc: 0.0000e+00 - val_loss: 1.8616e-04 - val_acc: 0.0000e+00
Epoch 39/100
13702/13702 [==============================] - 1s - loss: 1.1931e-04 - acc: 0.0000e+00 - val_loss: 1.8008e-04 - val_acc: 0.0000e+00
Epoch 40/100
13702/13702 [==============================] - 1s - loss: 9.9851e-05 - acc: 0.0000e+00 - val_loss: 1.7408e-04 - val_acc: 0.0000e+00
Epoch 41/100
13702/13702 [==============================] - 1s - loss: 1.0531e-04 - acc: 0.0000e+00 - val_loss: 1.8432e-04 - val_acc: 0.0000e+00
Epoch 42/100
13702/13702 [==============================] - 1s - loss: 1.0655e-04 - acc: 0.0000e+00 - val_loss: 3.1590e-04 - val_acc: 0.0000e+00
Epoch 43/100
13702/13702 [==============================] - 1s - loss: 1.0102e-04 - acc: 0.0000e+00 - val_loss: 2.0471e-04 - val_acc: 0.0000e+00
Epoch 44/100
13702/13702 [==============================] - 1s - loss: 9.4763e-05 - acc: 0.0000e+00 - val_loss: 1.6779e-04 - val_acc: 0.0000e+00
Epoch 45/100
13702/13702 [==============================] - 1s - loss: 9.4119e-05 - acc: 0.0000e+00 - val_loss: 1.7765e-04 - val_acc: 0.0000e+00
Epoch 46/100
13702/13702 [==============================] - 1s - loss: 9.6664e-05 - acc: 0.0000e+00 - val_loss: 2.2464e-04 - val_acc: 0.0000e+00
Epoch 47/100
13702/13702 [==============================] - 1s - loss: 8.9921e-05 - acc: 0.0000e+00 - val_loss: 1.7101e-04 - val_acc: 0.0000e+00
Epoch 48/100
13702/13702 [==============================] - 1s - loss: 8.3820e-05 - acc: 0.0000e+00 - val_loss: 2.1212e-04 - val_acc: 0.0000e+00
Epoch 49/100
13702/13702 [==============================] - 1s - loss: 8.7753e-05 - acc: 0.0000e+00 - val_loss: 1.5561e-04 - val_acc: 0.0000e+00
Epoch 50/100
13702/13702 [==============================] - 1s - loss: 8.1039e-05 - acc: 0.0000e+00 - val_loss: 2.8530e-04 - val_acc: 0.0000e+00
Epoch 51/100
13702/13702 [==============================] - 1s - loss: 8.6943e-05 - acc: 0.0000e+00 - val_loss: 2.1737e-04 - val_acc: 0.0000e+00
Epoch 52/100
13702/13702 [==============================] - 1s - loss: 9.0655e-05 - acc: 0.0000e+00 - val_loss: 1.8859e-04 - val_acc: 0.0000e+00
Epoch 53/100
13702/13702 [==============================] - 1s - loss: 8.2950e-05 - acc: 0.0000e+00 - val_loss: 1.4745e-04 - val_acc: 0.0000e+00
Epoch 54/100
13702/13702 [==============================] - 1s - loss: 1.0031e-04 - acc: 0.0000e+00 - val_loss: 4.0727e-04 - val_acc: 0.0000e+00
Epoch 55/100
13702/13702 [==============================] - 1s - loss: 8.7670e-05 - acc: 0.0000e+00 - val_loss: 1.7582e-04 - val_acc: 0.0000e+00
Epoch 56/100
13702/13702 [==============================] - 1s - loss: 8.5274e-05 - acc: 0.0000e+00 - val_loss: 3.6586e-04 - val_acc: 0.0000e+00
Epoch 57/100
13702/13702 [==============================] - 1s - loss: 1.1647e-04 - acc: 0.0000e+00 - val_loss: 2.0324e-04 - val_acc: 0.0000e+00
Epoch 58/100
13702/13702 [==============================] - 1s - loss: 1.1154e-04 - acc: 0.0000e+00 - val_loss: 1.9097e-04 - val_acc: 0.0000e+00
Epoch 59/100
13702/13702 [==============================] - 1s - loss: 8.1990e-05 - acc: 0.0000e+00 - val_loss: 1.5139e-04 - val_acc: 0.0000e+00
Epoch 60/100
13702/13702 [==============================] - 1s - loss: 7.9407e-05 - acc: 0.0000e+00 - val_loss: 1.4982e-04 - val_acc: 0.0000e+00
Epoch 61/100
13702/13702 [==============================] - 1s - loss: 8.8360e-05 - acc: 0.0000e+00 - val_loss: 2.7859e-04 - val_acc: 0.0000e+00
Epoch 62/100
13702/13702 [==============================] - 1s - loss: 9.9392e-05 - acc: 0.0000e+00 - val_loss: 1.3830e-04 - val_acc: 0.0000e+00
Epoch 63/100
13702/13702 [==============================] - 1s - loss: 8.5109e-05 - acc: 0.0000e+00 - val_loss: 1.8507e-04 - val_acc: 0.0000e+00
Epoch 64/100
13702/13702 [==============================] - 1s - loss: 7.9735e-05 - acc: 0.0000e+00 - val_loss: 1.3697e-04 - val_acc: 0.0000e+0000
Epoch 65/100
13702/13702 [==============================] - 1s - loss: 8.6296e-05 - acc: 0.0000e+00 - val_loss: 1.5770e-04 - val_acc: 0.0000e+00
Epoch 66/100
13702/13702 [==============================] - 1s - loss: 8.0695e-05 - acc: 0.0000e+00 - val_loss: 1.3700e-04 - val_acc: 0.0000e+00
Epoch 67/100
13702/13702 [==============================] - 1s - loss: 7.6389e-05 - acc: 0.0000e+00 - val_loss: 3.4893e-04 - val_acc: 0.0000e+00
Epoch 68/100
13702/13702 [==============================] - 1s - loss: 8.2476e-05 - acc: 0.0000e+00 - val_loss: 5.0106e-04 - val_acc: 0.0000e+00
Epoch 69/100
13702/13702 [==============================] - 1s - loss: 9.0194e-05 - acc: 0.0000e+00 - val_loss: 1.5320e-04 - val_acc: 0.0000e+00
Epoch 70/100
13702/13702 [==============================] - 1s - loss: 7.5919e-05 - acc: 0.0000e+00 - val_loss: 1.3555e-04 - val_acc: 0.0000e+00
Epoch 71/100
13702/13702 [==============================] - 1s - loss: 8.5843e-05 - acc: 0.0000e+00 - val_loss: 2.4102e-04 - val_acc: 0.0000e+00
Epoch 72/100
13702/13702 [==============================] - 1s - loss: 9.3884e-05 - acc: 0.0000e+00 - val_loss: 1.9546e-04 - val_acc: 0.0000e+00
Epoch 73/100
13702/13702 [==============================] - 1s - loss: 8.2290e-05 - acc: 0.0000e+00 - val_loss: 1.4807e-04 - val_acc: 0.0000e+00
Epoch 74/100
13702/13702 [==============================] - 1s - loss: 7.5925e-05 - acc: 0.0000e+00 - val_loss: 1.7940e-04 - val_acc: 0.0000e+00
Epoch 75/100
13702/13702 [==============================] - 1s - loss: 8.0279e-05 - acc: 0.0000e+00 - val_loss: 1.2596e-04 - val_acc: 0.0000e+00
Epoch 76/100
13702/13702 [==============================] - 1s - loss: 8.1769e-05 - acc: 0.0000e+00 - val_loss: 1.2512e-04 - val_acc: 0.0000e+00
Epoch 77/100
13702/13702 [==============================] - 1s - loss: 7.4212e-05 - acc: 0.0000e+00 - val_loss: 1.2629e-04 - val_acc: 0.0000e+00
Epoch 78/100
13702/13702 [==============================] - 1s - loss: 7.7390e-05 - acc: 0.0000e+00 - val_loss: 3.4290e-04 - val_acc: 0.0000e+00
Epoch 79/100
13702/13702 [==============================] - 1s - loss: 8.2783e-05 - acc: 0.0000e+00 - val_loss: 4.3815e-04 - val_acc: 0.0000e+00
Epoch 80/100
13702/13702 [==============================] - 1s - loss: 8.0093e-05 - acc: 0.0000e+00 - val_loss: 1.2592e-04 - val_acc: 0.0000e+00
Epoch 81/100
13702/13702 [==============================] - 1s - loss: 7.3900e-05 - acc: 0.0000e+00 - val_loss: 1.3849e-04 - val_acc: 0.0000e+00
Epoch 82/100
13702/13702 [==============================] - 1s - loss: 7.2417e-05 - acc: 0.0000e+00 - val_loss: 1.4017e-04 - val_acc: 0.0000e+00
Epoch 83/100
13702/13702 [==============================] - 1s - loss: 7.2069e-05 - acc: 0.0000e+00 - val_loss: 1.7169e-04 - val_acc: 0.0000e+00
Epoch 84/100
13702/13702 [==============================] - 1s - loss: 7.2940e-05 - acc: 0.0000e+00 - val_loss: 1.1742e-04 - val_acc: 0.0000e+00
Epoch 85/100
13702/13702 [==============================] - 1s - loss: 8.1765e-05 - acc: 0.0000e+00 - val_loss: 1.2945e-04 - val_acc: 0.0000e+00
Epoch 86/100
13702/13702 [==============================] - 1s - loss: 6.6684e-05 - acc: 0.0000e+00 - val_loss: 1.1967e-04 - val_acc: 0.0000e+00
Epoch 87/100
13702/13702 [==============================] - 1s - loss: 6.7971e-05 - acc: 0.0000e+00 - val_loss: 1.4357e-04 - val_acc: 0.0000e+00
Epoch 88/100
13702/13702 [==============================] - 1s - loss: 7.7052e-05 - acc: 0.0000e+00 - val_loss: 2.5226e-04 - val_acc: 0.0000e+00
Epoch 89/100
13702/13702 [==============================] - 1s - loss: 7.0868e-05 - acc: 0.0000e+00 - val_loss: 1.5820e-04 - val_acc: 0.0000e+00
Epoch 90/100
13702/13702 [==============================] - 1s - loss: 7.2275e-05 - acc: 0.0000e+00 - val_loss: 1.1268e-04 - val_acc: 0.0000e+00
Epoch 91/100
13702/13702 [==============================] - 1s - loss: 8.9113e-05 - acc: 0.0000e+00 - val_loss: 5.6330e-04 - val_acc: 0.0000e+00
Epoch 92/100
13702/13702 [==============================] - 1s - loss: 8.9301e-05 - acc: 0.0000e+00 - val_loss: 2.4219e-04 - val_acc: 0.0000e+00
Epoch 93/100
13702/13702 [==============================] - 1s - loss: 6.8177e-05 - acc: 0.0000e+00 - val_loss: 1.1760e-04 - val_acc: 0.0000e+00
Epoch 94/100
13702/13702 [==============================] - 1s - loss: 7.6104e-05 - acc: 0.0000e+00 - val_loss: 1.7106e-04 - val_acc: 0.0000e+00
Epoch 95/100
13702/13702 [==============================] - 1s - loss: 7.0517e-05 - acc: 0.0000e+00 - val_loss: 1.3550e-04 - val_acc: 0.0000e+00
Epoch 96/100
13702/13702 [==============================] - 1s - loss: 6.3658e-05 - acc: 0.0000e+00 - val_loss: 1.7458e-04 - val_acc: 0.0000e+00
Epoch 97/100
13702/13702 [==============================] - 1s - loss: 7.0193e-05 - acc: 0.0000e+00 - val_loss: 1.0757e-04 - val_acc: 0.0000e+00
Epoch 98/100
13702/13702 [==============================] - 1s - loss: 6.5794e-05 - acc: 0.0000e+00 - val_loss: 1.1581e-04 - val_acc: 0.0000e+00
Epoch 99/100
13702/13702 [==============================] - 1s - loss: 6.3649e-05 - acc: 0.0000e+00 - val_loss: 1.9939e-04 - val_acc: 0.0000e+00
Epoch 100/100
13702/13702 [==============================] - 1s - loss: 7.0166e-05 - acc: 0.0000e+00 - val_loss: 1.4096e-04 - val_acc: 0.0000e+00
Train Score: 0.00003 MSE (0.01 RMSE)
Test Score: 0.00086 MSE (0.03 RMSE)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lists</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">epochs_result</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">lists</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Finding the best hyperparameter&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Square Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>12.3 Optimal number of neurons</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stock_name</span> <span class="o">=</span> <span class="s1">&#39;^GSPC&#39;</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># feature, window, output</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">neuronlist1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
<span class="n">neuronlist2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="n">neurons_result</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">neuron_lstm</span> <span class="ow">in</span> <span class="n">neuronlist1</span><span class="p">:</span>
    <span class="n">neurons</span> <span class="o">=</span> <span class="p">[</span><span class="n">neuron_lstm</span><span class="p">,</span> <span class="n">neuron_lstm</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">activation</span> <span class="ow">in</span> <span class="n">neuronlist2</span><span class="p">:</span>
        <span class="n">neurons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="n">neurons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">trainScore</span><span class="p">,</span> <span class="n">testScore</span> <span class="o">=</span> <span class="n">quick_measure</span><span class="p">(</span><span class="n">stock_name</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">epochs</span><span class="p">)</span>
        <span class="n">neurons_result</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">neurons</span><span class="p">)]</span> <span class="o">=</span> <span class="n">testScore</span>
        <span class="n">neurons</span> <span class="o">=</span> <span class="n">neurons</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>    
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 22, 32)            4736      
_________________________________________________________________
dropout_1 (Dropout)          (None, 22, 32)            0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 32)                8320      
_________________________________________________________________
dropout_2 (Dropout)          (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 17        
=================================================================
Total params: 13,601
Trainable params: 13,601
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 2s - loss: 0.0286 - acc: 0.0000e+00 - val_loss: 0.0841 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 1s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 9.8019e-04 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 1s - loss: 7.1308e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 1s - loss: 4.5217e-04 - acc: 0.0000e+00 - val_loss: 6.0801e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 1s - loss: 3.6125e-04 - acc: 0.0000e+00 - val_loss: 3.2174e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 1s - loss: 3.4450e-04 - acc: 0.0000e+00 - val_loss: 3.1738e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 1s - loss: 3.3887e-04 - acc: 0.0000e+00 - val_loss: 3.4616e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 1s - loss: 3.3747e-04 - acc: 0.0000e+00 - val_loss: 3.0563e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 1s - loss: 3.2240e-04 - acc: 0.0000e+00 - val_loss: 4.5232e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 1s - loss: 3.1574e-04 - acc: 0.0000e+00 - val_loss: 3.1352e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 1s - loss: 3.0739e-04 - acc: 0.0000e+00 - val_loss: 5.6274e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 1s - loss: 2.9365e-04 - acc: 0.0000e+00 - val_loss: 2.5694e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 1s - loss: 2.8381e-04 - acc: 0.0000e+00 - val_loss: 2.2885e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 1s - loss: 2.7394e-04 - acc: 0.0000e+00 - val_loss: 2.4461e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 1s - loss: 2.7457e-04 - acc: 0.0000e+00 - val_loss: 2.3578e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 1s - loss: 2.6887e-04 - acc: 0.0000e+00 - val_loss: 2.2138e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 1s - loss: 2.6127e-04 - acc: 0.0000e+00 - val_loss: 2.4881e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 1s - loss: 2.7469e-04 - acc: 0.0000e+00 - val_loss: 2.6547e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 1s - loss: 2.6144e-04 - acc: 0.0000e+00 - val_loss: 4.7081e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 1s - loss: 2.7553e-04 - acc: 0.0000e+00 - val_loss: 2.4093e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 1s - loss: 2.7055e-04 - acc: 0.0000e+00 - val_loss: 3.2517e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 1s - loss: 2.3755e-04 - acc: 0.0000e+00 - val_loss: 2.0021e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 1s - loss: 2.5404e-04 - acc: 0.0000e+00 - val_loss: 5.1493e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 1s - loss: 2.3615e-04 - acc: 0.0000e+00 - val_loss: 2.1185e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 1s - loss: 2.3941e-04 - acc: 0.0000e+00 - val_loss: 2.4835e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 1s - loss: 2.4132e-04 - acc: 0.0000e+00 - val_loss: 3.2255e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 1s - loss: 2.3250e-04 - acc: 0.0000e+00 - val_loss: 2.9311e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 1s - loss: 2.3988e-04 - acc: 0.0000e+00 - val_loss: 3.2929e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 1s - loss: 2.4467e-04 - acc: 0.0000e+00 - val_loss: 2.4528e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 1s - loss: 2.2333e-04 - acc: 0.0000e+00 - val_loss: 2.7753e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 1s - loss: 2.1993e-04 - acc: 0.0000e+00 - val_loss: 3.1895e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 1s - loss: 2.2244e-04 - acc: 0.0000e+00 - val_loss: 1.9019e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 1s - loss: 2.4162e-04 - acc: 0.0000e+00 - val_loss: 1.9278e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 1s - loss: 2.2809e-04 - acc: 0.0000e+00 - val_loss: 2.6528e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 1s - loss: 2.2006e-04 - acc: 0.0000e+00 - val_loss: 1.8928e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 1s - loss: 2.0997e-04 - acc: 0.0000e+00 - val_loss: 1.8447e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 1s - loss: 2.1408e-04 - acc: 0.0000e+00 - val_loss: 3.0332e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 1s - loss: 2.0831e-04 - acc: 0.0000e+00 - val_loss: 1.7982e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 1s - loss: 2.1007e-04 - acc: 0.0000e+00 - val_loss: 1.9491e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 1s - loss: 1.9675e-04 - acc: 0.0000e+00 - val_loss: 1.9274e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 1s - loss: 2.1145e-04 - acc: 0.0000e+00 - val_loss: 1.7986e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 1s - loss: 2.0728e-04 - acc: 0.0000e+00 - val_loss: 1.9320e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 1s - loss: 2.0651e-04 - acc: 0.0000e+00 - val_loss: 1.7766e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 1s - loss: 2.0627e-04 - acc: 0.0000e+00 - val_loss: 1.7718e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 1s - loss: 2.1044e-04 - acc: 0.0000e+00 - val_loss: 2.0235e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 1s - loss: 1.8987e-04 - acc: 0.0000e+00 - val_loss: 1.7146e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 1s - loss: 1.8103e-04 - acc: 0.0000e+00 - val_loss: 2.0828e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 1s - loss: 1.9037e-04 - acc: 0.0000e+00 - val_loss: 2.2063e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 1s - loss: 1.9263e-04 - acc: 0.0000e+00 - val_loss: 1.8588e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 1s - loss: 1.8914e-04 - acc: 0.0000e+00 - val_loss: 2.5464e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 1s - loss: 2.1299e-04 - acc: 0.0000e+00 - val_loss: 1.7413e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 1s - loss: 1.7338e-04 - acc: 0.0000e+00 - val_loss: 1.8425e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 1s - loss: 1.7103e-04 - acc: 0.0000e+00 - val_loss: 1.9687e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 1s - loss: 1.7167e-04 - acc: 0.0000e+00 - val_loss: 1.9969e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 1s - loss: 1.7282e-04 - acc: 0.0000e+00 - val_loss: 1.8351e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 1s - loss: 1.6896e-04 - acc: 0.0000e+00 - val_loss: 3.3851e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 1s - loss: 1.8303e-04 - acc: 0.0000e+00 - val_loss: 2.1563e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 1s - loss: 1.5803e-04 - acc: 0.0000e+00 - val_loss: 1.7686e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 1s - loss: 1.6912e-04 - acc: 0.0000e+00 - val_loss: 1.6861e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 1s - loss: 1.6470e-04 - acc: 0.0000e+00 - val_loss: 1.8890e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 1s - loss: 1.6077e-04 - acc: 0.0000e+00 - val_loss: 1.8594e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 1s - loss: 1.6172e-04 - acc: 0.0000e+00 - val_loss: 1.6547e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 1s - loss: 1.6332e-04 - acc: 0.0000e+00 - val_loss: 1.7847e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 1s - loss: 1.5917e-04 - acc: 0.0000e+00 - val_loss: 2.1358e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 1s - loss: 1.5229e-04 - acc: 0.0000e+00 - val_loss: 1.7062e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 1s - loss: 1.5705e-04 - acc: 0.0000e+00 - val_loss: 1.7001e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 1s - loss: 1.4763e-04 - acc: 0.0000e+00 - val_loss: 1.6984e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 1s - loss: 1.5539e-04 - acc: 0.0000e+00 - val_loss: 2.2463e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 1s - loss: 1.4528e-04 - acc: 0.0000e+00 - val_loss: 1.7183e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 1s - loss: 1.4018e-04 - acc: 0.0000e+00 - val_loss: 2.2541e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 1s - loss: 1.4930e-04 - acc: 0.0000e+00 - val_loss: 2.1791e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 1s - loss: 1.4455e-04 - acc: 0.0000e+00 - val_loss: 1.7050e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 1s - loss: 1.4507e-04 - acc: 0.0000e+00 - val_loss: 1.6679e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 1s - loss: 1.3132e-04 - acc: 0.0000e+00 - val_loss: 2.0364e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 1s - loss: 1.4370e-04 - acc: 0.0000e+00 - val_loss: 1.8145e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 1s - loss: 1.3519e-04 - acc: 0.0000e+00 - val_loss: 3.7377e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 1s - loss: 1.4701e-04 - acc: 0.0000e+00 - val_loss: 4.1943e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 1s - loss: 1.3439e-04 - acc: 0.0000e+00 - val_loss: 2.0473e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 1s - loss: 1.2879e-04 - acc: 0.0000e+00 - val_loss: 1.7536e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 1s - loss: 1.2293e-04 - acc: 0.0000e+00 - val_loss: 1.7878e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 1s - loss: 1.2901e-04 - acc: 0.0000e+00 - val_loss: 3.0140e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 1s - loss: 1.3271e-04 - acc: 0.0000e+00 - val_loss: 3.2067e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 1s - loss: 1.2968e-04 - acc: 0.0000e+00 - val_loss: 2.1139e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 1s - loss: 1.1750e-04 - acc: 0.0000e+00 - val_loss: 1.8835e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 1s - loss: 1.2748e-04 - acc: 0.0000e+00 - val_loss: 1.9147e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 1s - loss: 1.2567e-04 - acc: 0.0000e+00 - val_loss: 2.1327e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 1s - loss: 1.2347e-04 - acc: 0.0000e+00 - val_loss: 1.7156e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 1s - loss: 1.1792e-04 - acc: 0.0000e+00 - val_loss: 1.8401e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 1s - loss: 1.1104e-04 - acc: 0.0000e+00 - val_loss: 1.7696e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 1s - loss: 1.1118e-04 - acc: 0.0000e+00 - val_loss: 2.2785e-04 - val_acc: 0.0000e+00
Train Score: 0.00005 MSE (0.01 RMSE)
Test Score: 0.00809 MSE (0.09 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_3 (LSTM)                (None, 22, 32)            4736      
_________________________________________________________________
dropout_3 (Dropout)          (None, 22, 32)            0         
_________________________________________________________________
lstm_4 (LSTM)                (None, 32)                8320      
_________________________________________________________________
dropout_4 (Dropout)          (None, 32)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 32)                1056      
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 33        
=================================================================
Total params: 14,145
Trainable params: 14,145
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - ETA: 0s - loss: 0.0239 - acc: 0.0000e+0 - 2s - loss: 0.0235 - acc: 0.0000e+00 - val_loss: 0.0137 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 1s - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 1s - loss: 6.6673e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 1s - loss: 4.5537e-04 - acc: 0.0000e+00 - val_loss: 9.7508e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 1s - loss: 4.0013e-04 - acc: 0.0000e+00 - val_loss: 5.1892e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 1s - loss: 3.7835e-04 - acc: 0.0000e+00 - val_loss: 3.7573e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 1s - loss: 3.7209e-04 - acc: 0.0000e+00 - val_loss: 3.4938e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 1s - loss: 3.3857e-04 - acc: 0.0000e+00 - val_loss: 2.8594e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 1s - loss: 3.3065e-04 - acc: 0.0000e+00 - val_loss: 3.7322e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 1s - loss: 3.1574e-04 - acc: 0.0000e+00 - val_loss: 2.4896e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 1s - loss: 3.0677e-04 - acc: 0.0000e+00 - val_loss: 2.3476e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 1s - loss: 3.1454e-04 - acc: 0.0000e+00 - val_loss: 2.1837e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 1s - loss: 3.3050e-04 - acc: 0.0000e+00 - val_loss: 2.3540e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 1s - loss: 2.7673e-04 - acc: 0.0000e+00 - val_loss: 4.4183e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 1s - loss: 2.7449e-04 - acc: 0.0000e+00 - val_loss: 2.2809e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 1s - loss: 2.6815e-04 - acc: 0.0000e+00 - val_loss: 2.3394e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 1s - loss: 2.6183e-04 - acc: 0.0000e+00 - val_loss: 2.1246e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 1s - loss: 2.6673e-04 - acc: 0.0000e+00 - val_loss: 2.5620e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 1s - loss: 2.5763e-04 - acc: 0.0000e+00 - val_loss: 2.1607e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 1s - loss: 2.4085e-04 - acc: 0.0000e+00 - val_loss: 7.3177e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 1s - loss: 2.6710e-04 - acc: 0.0000e+00 - val_loss: 2.0957e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 1s - loss: 2.2858e-04 - acc: 0.0000e+00 - val_loss: 5.0905e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 1s - loss: 2.3903e-04 - acc: 0.0000e+00 - val_loss: 2.7810e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 1s - loss: 2.3128e-04 - acc: 0.0000e+00 - val_loss: 2.5337e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 1s - loss: 2.4047e-04 - acc: 0.0000e+00 - val_loss: 6.6178e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 1s - loss: 2.3320e-04 - acc: 0.0000e+00 - val_loss: 5.3161e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 1s - loss: 2.2623e-04 - acc: 0.0000e+00 - val_loss: 2.0268e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 1s - loss: 2.1202e-04 - acc: 0.0000e+00 - val_loss: 2.0359e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 1s - loss: 2.0972e-04 - acc: 0.0000e+00 - val_loss: 3.9546e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 1s - loss: 2.2268e-04 - acc: 0.0000e+00 - val_loss: 2.0290e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 1s - loss: 2.0346e-04 - acc: 0.0000e+00 - val_loss: 3.9124e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 1s - loss: 2.0151e-04 - acc: 0.0000e+00 - val_loss: 2.2003e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 1s - loss: 2.0388e-04 - acc: 0.0000e+00 - val_loss: 2.5419e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 1s - loss: 1.9070e-04 - acc: 0.0000e+00 - val_loss: 2.0046e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 1s - loss: 1.9934e-04 - acc: 0.0000e+00 - val_loss: 3.4510e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 1s - loss: 2.0421e-04 - acc: 0.0000e+00 - val_loss: 1.8840e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 1s - loss: 1.9383e-04 - acc: 0.0000e+00 - val_loss: 8.0139e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 1s - loss: 1.8778e-04 - acc: 0.0000e+00 - val_loss: 2.0490e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 1s - loss: 1.9604e-04 - acc: 0.0000e+00 - val_loss: 4.8461e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 1s - loss: 1.8282e-04 - acc: 0.0000e+00 - val_loss: 2.4399e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 1s - loss: 1.7226e-04 - acc: 0.0000e+00 - val_loss: 1.8196e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 1s - loss: 1.7449e-04 - acc: 0.0000e+00 - val_loss: 2.6886e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 1s - loss: 1.6390e-04 - acc: 0.0000e+00 - val_loss: 4.3978e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 1s - loss: 1.6134e-04 - acc: 0.0000e+00 - val_loss: 2.6092e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 1s - loss: 1.6577e-04 - acc: 0.0000e+00 - val_loss: 1.8296e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 1s - loss: 1.6382e-04 - acc: 0.0000e+00 - val_loss: 3.2945e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 1s - loss: 1.5330e-04 - acc: 0.0000e+00 - val_loss: 1.8462e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 1s - loss: 1.5972e-04 - acc: 0.0000e+00 - val_loss: 6.4905e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 1s - loss: 1.5776e-04 - acc: 0.0000e+00 - val_loss: 2.0012e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 1s - loss: 1.4338e-04 - acc: 0.0000e+00 - val_loss: 2.0442e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 1s - loss: 1.4535e-04 - acc: 0.0000e+00 - val_loss: 5.0196e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 1s - loss: 1.2988e-04 - acc: 0.0000e+00 - val_loss: 1.8333e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 1s - loss: 1.4286e-04 - acc: 0.0000e+00 - val_loss: 4.0023e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 1s - loss: 1.2560e-04 - acc: 0.0000e+00 - val_loss: 2.8604e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 1s - loss: 1.2104e-04 - acc: 0.0000e+00 - val_loss: 3.8935e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 1s - loss: 1.2313e-04 - acc: 0.0000e+00 - val_loss: 1.8014e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 1s - loss: 1.2327e-04 - acc: 0.0000e+00 - val_loss: 3.4365e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 1s - loss: 1.1252e-04 - acc: 0.0000e+00 - val_loss: 3.4353e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 1s - loss: 1.1358e-04 - acc: 0.0000e+00 - val_loss: 3.6140e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 1s - loss: 1.1349e-04 - acc: 0.0000e+00 - val_loss: 3.4278e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 1s - loss: 1.0809e-04 - acc: 0.0000e+00 - val_loss: 3.7535e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 1s - loss: 9.9576e-05 - acc: 0.0000e+00 - val_loss: 2.9398e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 1s - loss: 1.0462e-04 - acc: 0.0000e+00 - val_loss: 2.7304e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 1s - loss: 1.0765e-04 - acc: 0.0000e+00 - val_loss: 3.1029e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 1s - loss: 1.0486e-04 - acc: 0.0000e+00 - val_loss: 2.6433e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 1s - loss: 1.0384e-04 - acc: 0.0000e+00 - val_loss: 5.6881e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 1s - loss: 1.0465e-04 - acc: 0.0000e+00 - val_loss: 2.1451e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 1s - loss: 1.0251e-04 - acc: 0.0000e+00 - val_loss: 4.4722e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 1s - loss: 9.5657e-05 - acc: 0.0000e+00 - val_loss: 2.9252e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 1s - loss: 1.0183e-04 - acc: 0.0000e+00 - val_loss: 4.3997e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 1s - loss: 9.9849e-05 - acc: 0.0000e+00 - val_loss: 3.2019e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 1s - loss: 9.2292e-05 - acc: 0.0000e+00 - val_loss: 2.3244e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 1s - loss: 9.3430e-05 - acc: 0.0000e+00 - val_loss: 2.3518e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 1s - loss: 9.3135e-05 - acc: 0.0000e+00 - val_loss: 2.5017e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 1s - loss: 9.0136e-05 - acc: 0.0000e+00 - val_loss: 4.5469e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 1s - loss: 8.9215e-05 - acc: 0.0000e+00 - val_loss: 2.8440e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 1s - loss: 9.3321e-05 - acc: 0.0000e+00 - val_loss: 3.8150e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 1s - loss: 8.9715e-05 - acc: 0.0000e+00 - val_loss: 6.2238e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 1s - loss: 9.1365e-05 - acc: 0.0000e+00 - val_loss: 4.0256e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 1s - loss: 8.8879e-05 - acc: 0.0000e+00 - val_loss: 7.0585e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 1s - loss: 8.4942e-05 - acc: 0.0000e+00 - val_loss: 5.9327e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 1s - loss: 7.9788e-05 - acc: 0.0000e+00 - val_loss: 5.4626e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 1s - loss: 8.9706e-05 - acc: 0.0000e+00 - val_loss: 9.3332e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 1s - loss: 8.3022e-05 - acc: 0.0000e+00 - val_loss: 8.0722e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 1s - loss: 8.3190e-05 - acc: 0.0000e+00 - val_loss: 6.7787e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 1s - loss: 8.1818e-05 - acc: 0.0000e+00 - val_loss: 6.8897e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 1s - loss: 7.4998e-05 - acc: 0.0000e+00 - val_loss: 4.9481e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 1s - loss: 7.7689e-05 - acc: 0.0000e+00 - val_loss: 4.0089e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 1s - loss: 7.8570e-05 - acc: 0.0000e+00 - val_loss: 7.6558e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 1s - loss: 7.4375e-05 - acc: 0.0000e+00 - val_loss: 5.0817e-04 - val_acc: 0.0000e+00
Train Score: 0.00011 MSE (0.01 RMSE)
Test Score: 0.00266 MSE (0.05 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_5 (LSTM)                (None, 22, 32)            4736      
_________________________________________________________________
dropout_5 (Dropout)          (None, 22, 32)            0         
_________________________________________________________________
lstm_6 (LSTM)                (None, 32)                8320      
_________________________________________________________________
dropout_6 (Dropout)          (None, 32)                0         
_________________________________________________________________
dense_5 (Dense)              (None, 64)                2112      
_________________________________________________________________
dense_6 (Dense)              (None, 1)                 65        
=================================================================
Total params: 15,233
Trainable params: 15,233
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 2s - loss: 0.0211 - acc: 0.0000e+00 - val_loss: 0.0149 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 1s - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 5.9589e-04 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 1s - loss: 5.0133e-04 - acc: 0.0000e+00 - val_loss: 8.2404e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 1s - loss: 4.0202e-04 - acc: 0.0000e+00 - val_loss: 6.6533e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 1s - loss: 3.3818e-04 - acc: 0.0000e+00 - val_loss: 5.6144e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 1s - loss: 3.3962e-04 - acc: 0.0000e+00 - val_loss: 3.2445e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 1s - loss: 2.9927e-04 - acc: 0.0000e+00 - val_loss: 2.8042e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 1s - loss: 2.8216e-04 - acc: 0.0000e+00 - val_loss: 2.5814e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 1s - loss: 2.6754e-04 - acc: 0.0000e+00 - val_loss: 2.4850e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 1s - loss: 2.6402e-04 - acc: 0.0000e+00 - val_loss: 2.4268e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 1s - loss: 2.5773e-04 - acc: 0.0000e+00 - val_loss: 4.0016e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 1s - loss: 2.4322e-04 - acc: 0.0000e+00 - val_loss: 2.3327e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 1s - loss: 2.4410e-04 - acc: 0.0000e+00 - val_loss: 2.2570e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 1s - loss: 2.4345e-04 - acc: 0.0000e+00 - val_loss: 2.5484e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 1s - loss: 2.3237e-04 - acc: 0.0000e+00 - val_loss: 5.3237e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 1s - loss: 2.2947e-04 - acc: 0.0000e+00 - val_loss: 2.6386e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 1s - loss: 2.1937e-04 - acc: 0.0000e+00 - val_loss: 3.1218e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 1s - loss: 2.2115e-04 - acc: 0.0000e+00 - val_loss: 2.3339e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 1s - loss: 2.1661e-04 - acc: 0.0000e+00 - val_loss: 4.5529e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 1s - loss: 2.0738e-04 - acc: 0.0000e+00 - val_loss: 2.5439e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 1s - loss: 2.0121e-04 - acc: 0.0000e+00 - val_loss: 3.5531e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 1s - loss: 1.9814e-04 - acc: 0.0000e+00 - val_loss: 6.5135e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 1s - loss: 1.9455e-04 - acc: 0.0000e+00 - val_loss: 2.9765e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 1s - loss: 1.8654e-04 - acc: 0.0000e+00 - val_loss: 7.0856e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 1s - loss: 1.8610e-04 - acc: 0.0000e+00 - val_loss: 7.8086e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 1s - loss: 1.7281e-04 - acc: 0.0000e+00 - val_loss: 3.5580e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 1s - loss: 1.6975e-04 - acc: 0.0000e+00 - val_loss: 4.4894e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 1s - loss: 1.8793e-04 - acc: 0.0000e+00 - val_loss: 2.3526e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 1s - loss: 1.6958e-04 - acc: 0.0000e+00 - val_loss: 6.5960e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 1s - loss: 1.5209e-04 - acc: 0.0000e+00 - val_loss: 2.7544e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 1s - loss: 1.6878e-04 - acc: 0.0000e+00 - val_loss: 8.1988e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 1s - loss: 1.5693e-04 - acc: 0.0000e+00 - val_loss: 3.3091e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 1s - loss: 1.4335e-04 - acc: 0.0000e+00 - val_loss: 5.0719e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 1s - loss: 1.4194e-04 - acc: 0.0000e+00 - val_loss: 6.1710e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 1s - loss: 1.4598e-04 - acc: 0.0000e+00 - val_loss: 6.7880e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 1s - loss: 1.4778e-04 - acc: 0.0000e+00 - val_loss: 8.6340e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 1s - loss: 1.4149e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 1s - loss: 1.3593e-04 - acc: 0.0000e+00 - val_loss: 7.0729e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 1s - loss: 1.1836e-04 - acc: 0.0000e+00 - val_loss: 5.0911e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 1s - loss: 1.2053e-04 - acc: 0.0000e+00 - val_loss: 8.4274e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 1s - loss: 1.2624e-04 - acc: 0.0000e+00 - val_loss: 5.4251e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 1s - loss: 1.1889e-04 - acc: 0.0000e+00 - val_loss: 9.6545e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 1s - loss: 1.0966e-04 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 1s - loss: 1.1389e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 1s - loss: 1.0837e-04 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 1s - loss: 1.0395e-04 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 1s - loss: 1.0222e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 1s - loss: 1.0680e-04 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 1s - loss: 9.7726e-05 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 1s - loss: 9.1378e-05 - acc: 0.0000e+00 - val_loss: 7.8944e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 1s - loss: 1.0154e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 1s - loss: 9.1396e-05 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 1s - loss: 9.2052e-05 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 1s - loss: 8.6839e-05 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 1s - loss: 8.5298e-05 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 1s - loss: 1.0312e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 1s - loss: 8.3334e-05 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 1s - loss: 7.8709e-05 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 1s - loss: 8.0145e-05 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 1s - loss: 7.6999e-05 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 1s - loss: 7.6172e-05 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 1s - loss: 7.8825e-05 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 1s - loss: 7.7180e-05 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 1s - loss: 7.5055e-05 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 1s - loss: 7.1416e-05 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 1s - loss: 7.7308e-05 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 1s - loss: 7.2139e-05 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 1s - loss: 6.9727e-05 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 1s - loss: 7.0371e-05 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 1s - loss: 7.2991e-05 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 1s - loss: 7.1147e-05 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 1s - loss: 6.8318e-05 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 1s - loss: 6.4916e-05 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 1s - loss: 6.1955e-05 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 1s - loss: 7.1073e-05 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 1s - loss: 6.9245e-05 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 1s - loss: 6.4233e-05 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 1s - loss: 6.8574e-05 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 1s - loss: 6.2959e-05 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 1s - loss: 6.6269e-05 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 1s - loss: 7.7170e-05 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 1s - loss: 7.6238e-05 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 1s - loss: 6.8216e-05 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 1s - loss: 6.1329e-05 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 1s - loss: 5.6402e-05 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 1s - loss: 6.1755e-05 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 1s - loss: 6.4283e-05 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 1s - loss: 6.5990e-05 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 1s - loss: 5.7552e-05 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 1s - loss: 6.2102e-05 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Train Score: 0.00033 MSE (0.02 RMSE)
Test Score: 0.01148 MSE (0.11 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_7 (LSTM)                (None, 22, 64)            17664     
_________________________________________________________________
dropout_7 (Dropout)          (None, 22, 64)            0         
_________________________________________________________________
lstm_8 (LSTM)                (None, 64)                33024     
_________________________________________________________________
dropout_8 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_7 (Dense)              (None, 16)                1040      
_________________________________________________________________
dense_8 (Dense)              (None, 1)                 17        
=================================================================
Total params: 51,745
Trainable params: 51,745
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 2s - loss: 0.0226 - acc: 0.0000e+00 - val_loss: 0.0183 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 1s - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 1s - loss: 3.6470e-04 - acc: 0.0000e+00 - val_loss: 6.2854e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 1s - loss: 2.3806e-04 - acc: 0.0000e+00 - val_loss: 3.3297e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 1s - loss: 2.2258e-04 - acc: 0.0000e+00 - val_loss: 3.1455e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 1s - loss: 2.1450e-04 - acc: 0.0000e+00 - val_loss: 2.6590e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 1s - loss: 2.1916e-04 - acc: 0.0000e+00 - val_loss: 3.8236e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 1s - loss: 1.9080e-04 - acc: 0.0000e+00 - val_loss: 2.5676e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 1s - loss: 1.8886e-04 - acc: 0.0000e+00 - val_loss: 3.4520e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 1s - loss: 1.8814e-04 - acc: 0.0000e+00 - val_loss: 3.1629e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 1s - loss: 1.9755e-04 - acc: 0.0000e+00 - val_loss: 2.4818e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 1s - loss: 1.8762e-04 - acc: 0.0000e+00 - val_loss: 2.4071e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 1s - loss: 1.8490e-04 - acc: 0.0000e+00 - val_loss: 2.3425e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 1s - loss: 1.9992e-04 - acc: 0.0000e+00 - val_loss: 4.4923e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 1s - loss: 1.7197e-04 - acc: 0.0000e+00 - val_loss: 2.6967e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 1s - loss: 1.7359e-04 - acc: 0.0000e+00 - val_loss: 4.3604e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 1s - loss: 1.8068e-04 - acc: 0.0000e+00 - val_loss: 2.1058e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 1s - loss: 1.6487e-04 - acc: 0.0000e+00 - val_loss: 2.1788e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 1s - loss: 1.7661e-04 - acc: 0.0000e+00 - val_loss: 2.2932e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 1s - loss: 1.6782e-04 - acc: 0.0000e+00 - val_loss: 2.7017e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 1s - loss: 1.6598e-04 - acc: 0.0000e+00 - val_loss: 2.1216e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 1s - loss: 1.5660e-04 - acc: 0.0000e+00 - val_loss: 2.4616e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 1s - loss: 1.6272e-04 - acc: 0.0000e+00 - val_loss: 1.9881e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 1s - loss: 1.4802e-04 - acc: 0.0000e+00 - val_loss: 2.4627e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 1s - loss: 1.6218e-04 - acc: 0.0000e+00 - val_loss: 2.4333e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 1s - loss: 1.5858e-04 - acc: 0.0000e+00 - val_loss: 2.3508e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 1s - loss: 1.5488e-04 - acc: 0.0000e+00 - val_loss: 2.4143e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 1s - loss: 1.5595e-04 - acc: 0.0000e+00 - val_loss: 2.6521e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 1s - loss: 1.6891e-04 - acc: 0.0000e+00 - val_loss: 1.9908e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 1s - loss: 1.3846e-04 - acc: 0.0000e+00 - val_loss: 1.8755e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 1s - loss: 1.4574e-04 - acc: 0.0000e+00 - val_loss: 1.8321e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 1s - loss: 1.4663e-04 - acc: 0.0000e+00 - val_loss: 2.3112e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 1s - loss: 1.3985e-04 - acc: 0.0000e+00 - val_loss: 1.9247e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 1s - loss: 1.7216e-04 - acc: 0.0000e+00 - val_loss: 2.8092e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 1s - loss: 1.4310e-04 - acc: 0.0000e+00 - val_loss: 1.9515e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 1s - loss: 1.3955e-04 - acc: 0.0000e+00 - val_loss: 2.7510e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 1s - loss: 1.4510e-04 - acc: 0.0000e+00 - val_loss: 2.8844e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 1s - loss: 1.4596e-04 - acc: 0.0000e+00 - val_loss: 2.4819e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 1s - loss: 1.5197e-04 - acc: 0.0000e+00 - val_loss: 2.4457e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 1s - loss: 1.4812e-04 - acc: 0.0000e+00 - val_loss: 2.3604e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 1s - loss: 1.3879e-04 - acc: 0.0000e+00 - val_loss: 1.7850e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 1s - loss: 1.3658e-04 - acc: 0.0000e+00 - val_loss: 2.0768e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 1s - loss: 1.3741e-04 - acc: 0.0000e+00 - val_loss: 2.9593e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 1s - loss: 1.5629e-04 - acc: 0.0000e+00 - val_loss: 2.2557e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 1s - loss: 1.5754e-04 - acc: 0.0000e+00 - val_loss: 1.9944e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 1s - loss: 1.2757e-04 - acc: 0.0000e+00 - val_loss: 2.2804e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 1s - loss: 1.3443e-04 - acc: 0.0000e+00 - val_loss: 1.8129e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 1s - loss: 1.4145e-04 - acc: 0.0000e+00 - val_loss: 1.7648e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 1s - loss: 1.3789e-04 - acc: 0.0000e+00 - val_loss: 3.0929e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 1s - loss: 1.4322e-04 - acc: 0.0000e+00 - val_loss: 5.8995e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 1s - loss: 1.3296e-04 - acc: 0.0000e+00 - val_loss: 1.7357e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 1s - loss: 1.2907e-04 - acc: 0.0000e+00 - val_loss: 1.9288e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 1s - loss: 1.2637e-04 - acc: 0.0000e+00 - val_loss: 2.2969e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 1s - loss: 1.3505e-04 - acc: 0.0000e+00 - val_loss: 1.7779e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 1s - loss: 1.2286e-04 - acc: 0.0000e+00 - val_loss: 1.9032e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 1s - loss: 1.3533e-04 - acc: 0.0000e+00 - val_loss: 3.6319e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 1s - loss: 1.3468e-04 - acc: 0.0000e+00 - val_loss: 3.9971e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 1s - loss: 1.2715e-04 - acc: 0.0000e+00 - val_loss: 3.1400e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 1s - loss: 1.4353e-04 - acc: 0.0000e+00 - val_loss: 3.3600e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 1s - loss: 1.2772e-04 - acc: 0.0000e+00 - val_loss: 2.1673e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 1s - loss: 1.3437e-04 - acc: 0.0000e+00 - val_loss: 3.5086e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 1s - loss: 1.3030e-04 - acc: 0.0000e+00 - val_loss: 1.7212e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 1s - loss: 1.2405e-04 - acc: 0.0000e+00 - val_loss: 1.8532e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 1s - loss: 1.2132e-04 - acc: 0.0000e+00 - val_loss: 1.5348e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 1s - loss: 1.1552e-04 - acc: 0.0000e+00 - val_loss: 1.6766e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 1s - loss: 1.1451e-04 - acc: 0.0000e+00 - val_loss: 1.7817e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 1s - loss: 1.1936e-04 - acc: 0.0000e+00 - val_loss: 1.7408e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 1s - loss: 1.2370e-04 - acc: 0.0000e+00 - val_loss: 1.5738e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 1s - loss: 1.1756e-04 - acc: 0.0000e+00 - val_loss: 1.7293e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 1s - loss: 1.2459e-04 - acc: 0.0000e+00 - val_loss: 3.7088e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 1s - loss: 1.2877e-04 - acc: 0.0000e+00 - val_loss: 3.3771e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 1s - loss: 1.1495e-04 - acc: 0.0000e+00 - val_loss: 3.1040e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 1s - loss: 1.1601e-04 - acc: 0.0000e+00 - val_loss: 1.8384e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 1s - loss: 1.1692e-04 - acc: 0.0000e+00 - val_loss: 1.8424e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 1s - loss: 1.0721e-04 - acc: 0.0000e+00 - val_loss: 1.9633e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 1s - loss: 1.1054e-04 - acc: 0.0000e+00 - val_loss: 2.3135e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 1s - loss: 1.0588e-04 - acc: 0.0000e+00 - val_loss: 1.8488e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 1s - loss: 1.0353e-04 - acc: 0.0000e+00 - val_loss: 2.2807e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 1s - loss: 1.0941e-04 - acc: 0.0000e+00 - val_loss: 1.5074e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 1s - loss: 1.1118e-04 - acc: 0.0000e+00 - val_loss: 1.5970e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 1s - loss: 1.0155e-04 - acc: 0.0000e+00 - val_loss: 1.4412e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 1s - loss: 9.4178e-05 - acc: 0.0000e+00 - val_loss: 1.4362e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 1s - loss: 1.0368e-04 - acc: 0.0000e+00 - val_loss: 1.4380e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 1s - loss: 9.4621e-05 - acc: 0.0000e+00 - val_loss: 3.2962e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 1s - loss: 9.7291e-05 - acc: 0.0000e+00 - val_loss: 3.5430e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 1s - loss: 1.1066e-04 - acc: 0.0000e+00 - val_loss: 1.5532e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 1s - loss: 9.2600e-05 - acc: 0.0000e+00 - val_loss: 1.7726e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 1s - loss: 9.9959e-05 - acc: 0.0000e+00 - val_loss: 1.9204e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 1s - loss: 9.4024e-05 - acc: 0.0000e+00 - val_loss: 1.4287e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 1s - loss: 9.3301e-05 - acc: 0.0000e+00 - val_loss: 1.3879e-04 - val_acc: 0.0000e+00
Train Score: 0.00003 MSE (0.01 RMSE)
Test Score: 0.00177 MSE (0.04 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_9 (LSTM)                (None, 22, 64)            17664     
_________________________________________________________________
dropout_9 (Dropout)          (None, 22, 64)            0         
_________________________________________________________________
lstm_10 (LSTM)               (None, 64)                33024     
_________________________________________________________________
dropout_10 (Dropout)         (None, 64)                0         
_________________________________________________________________
dense_9 (Dense)              (None, 32)                2080      
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 33        
=================================================================
Total params: 52,801
Trainable params: 52,801
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 3s - loss: 0.0187 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 1s - loss: 8.0165e-04 - acc: 0.0000e+00 - val_loss: 8.2970e-04 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 1s - loss: 3.7721e-04 - acc: 0.0000e+00 - val_loss: 7.6723e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 1s - loss: 2.8068e-04 - acc: 0.0000e+00 - val_loss: 7.3129e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 1s - loss: 2.3887e-04 - acc: 0.0000e+00 - val_loss: 2.6333e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 1s - loss: 1.9935e-04 - acc: 0.0000e+00 - val_loss: 2.4566e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 1s - loss: 2.0403e-04 - acc: 0.0000e+00 - val_loss: 3.4284e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 1s - loss: 1.8217e-04 - acc: 0.0000e+00 - val_loss: 3.3037e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 1s - loss: 1.9463e-04 - acc: 0.0000e+00 - val_loss: 2.3828e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 1s - loss: 1.8308e-04 - acc: 0.0000e+00 - val_loss: 3.0253e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 1s - loss: 1.8285e-04 - acc: 0.0000e+00 - val_loss: 2.8349e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 1s - loss: 1.8349e-04 - acc: 0.0000e+00 - val_loss: 4.1486e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 1s - loss: 1.6737e-04 - acc: 0.0000e+00 - val_loss: 3.7957e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 1s - loss: 1.7332e-04 - acc: 0.0000e+00 - val_loss: 2.8277e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 1s - loss: 1.6216e-04 - acc: 0.0000e+00 - val_loss: 2.3721e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 1s - loss: 1.7225e-04 - acc: 0.0000e+00 - val_loss: 2.7439e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 1s - loss: 1.6864e-04 - acc: 0.0000e+00 - val_loss: 2.0275e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 1s - loss: 1.6925e-04 - acc: 0.0000e+00 - val_loss: 4.0249e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 1s - loss: 1.6630e-04 - acc: 0.0000e+00 - val_loss: 2.1992e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 1s - loss: 1.7049e-04 - acc: 0.0000e+00 - val_loss: 2.4559e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 1s - loss: 1.6008e-04 - acc: 0.0000e+00 - val_loss: 3.3924e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 1s - loss: 1.5873e-04 - acc: 0.0000e+00 - val_loss: 1.9231e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 1s - loss: 1.5198e-04 - acc: 0.0000e+00 - val_loss: 3.9207e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 1s - loss: 1.6206e-04 - acc: 0.0000e+00 - val_loss: 1.8952e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 1s - loss: 1.5761e-04 - acc: 0.0000e+00 - val_loss: 1.9089e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 1s - loss: 1.4740e-04 - acc: 0.0000e+00 - val_loss: 2.1694e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 1s - loss: 1.4532e-04 - acc: 0.0000e+00 - val_loss: 2.1952e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 1s - loss: 1.4907e-04 - acc: 0.0000e+00 - val_loss: 3.6601e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 1s - loss: 1.5727e-04 - acc: 0.0000e+00 - val_loss: 1.7864e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 1s - loss: 1.6226e-04 - acc: 0.0000e+00 - val_loss: 4.0328e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 1s - loss: 1.4268e-04 - acc: 0.0000e+00 - val_loss: 4.0546e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 1s - loss: 1.3835e-04 - acc: 0.0000e+00 - val_loss: 2.0555e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 1s - loss: 1.4690e-04 - acc: 0.0000e+00 - val_loss: 1.7280e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 1s - loss: 1.3294e-04 - acc: 0.0000e+00 - val_loss: 1.7233e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 1s - loss: 1.5111e-04 - acc: 0.0000e+00 - val_loss: 3.2609e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 1s - loss: 1.4745e-04 - acc: 0.0000e+00 - val_loss: 4.2219e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 1s - loss: 1.4084e-04 - acc: 0.0000e+00 - val_loss: 2.0300e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 1s - loss: 1.3696e-04 - acc: 0.0000e+00 - val_loss: 1.7031e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 1s - loss: 1.3157e-04 - acc: 0.0000e+00 - val_loss: 1.9916e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 1s - loss: 1.3233e-04 - acc: 0.0000e+00 - val_loss: 2.1978e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 1s - loss: 1.3340e-04 - acc: 0.0000e+00 - val_loss: 2.3406e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 1s - loss: 1.3796e-04 - acc: 0.0000e+00 - val_loss: 3.0887e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 1s - loss: 1.4475e-04 - acc: 0.0000e+00 - val_loss: 2.6176e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 1s - loss: 1.3613e-04 - acc: 0.0000e+00 - val_loss: 3.5597e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 1s - loss: 1.3672e-04 - acc: 0.0000e+00 - val_loss: 3.5454e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 1s - loss: 1.2905e-04 - acc: 0.0000e+00 - val_loss: 1.7676e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 1s - loss: 1.4060e-04 - acc: 0.0000e+00 - val_loss: 1.6176e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 1s - loss: 1.3328e-04 - acc: 0.0000e+00 - val_loss: 4.9908e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 1s - loss: 1.1766e-04 - acc: 0.0000e+00 - val_loss: 1.6788e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 1s - loss: 1.1982e-04 - acc: 0.0000e+00 - val_loss: 2.2569e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 1s - loss: 1.0611e-04 - acc: 0.0000e+00 - val_loss: 2.4419e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 1s - loss: 1.0941e-04 - acc: 0.0000e+00 - val_loss: 2.2179e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 1s - loss: 1.1105e-04 - acc: 0.0000e+00 - val_loss: 1.5933e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 1s - loss: 1.1352e-04 - acc: 0.0000e+00 - val_loss: 4.8780e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 1s - loss: 1.1338e-04 - acc: 0.0000e+00 - val_loss: 2.2228e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 1s - loss: 1.1819e-04 - acc: 0.0000e+00 - val_loss: 6.5505e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 1s - loss: 1.1412e-04 - acc: 0.0000e+00 - val_loss: 3.1636e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 1s - loss: 1.0838e-04 - acc: 0.0000e+00 - val_loss: 1.6188e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 1s - loss: 1.0664e-04 - acc: 0.0000e+00 - val_loss: 2.4612e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 1s - loss: 1.0956e-04 - acc: 0.0000e+00 - val_loss: 1.8218e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 1s - loss: 1.0582e-04 - acc: 0.0000e+00 - val_loss: 1.7021e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 1s - loss: 1.0480e-04 - acc: 0.0000e+00 - val_loss: 1.4567e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 1s - loss: 1.0443e-04 - acc: 0.0000e+00 - val_loss: 2.3643e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 1s - loss: 1.0482e-04 - acc: 0.0000e+00 - val_loss: 2.0244e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 1s - loss: 9.5602e-05 - acc: 0.0000e+00 - val_loss: 1.7169e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 1s - loss: 9.7371e-05 - acc: 0.0000e+00 - val_loss: 1.4162e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 1s - loss: 1.0320e-04 - acc: 0.0000e+00 - val_loss: 1.4840e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 1s - loss: 9.6087e-05 - acc: 0.0000e+00 - val_loss: 3.9798e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 1s - loss: 9.2435e-05 - acc: 0.0000e+00 - val_loss: 1.5709e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 1s - loss: 9.5935e-05 - acc: 0.0000e+00 - val_loss: 2.3722e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 1s - loss: 1.1369e-04 - acc: 0.0000e+00 - val_loss: 6.1083e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 1s - loss: 9.8962e-05 - acc: 0.0000e+00 - val_loss: 4.6924e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 1s - loss: 9.6212e-05 - acc: 0.0000e+00 - val_loss: 2.6261e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 1s - loss: 9.4726e-05 - acc: 0.0000e+00 - val_loss: 1.4454e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 1s - loss: 8.2757e-05 - acc: 0.0000e+00 - val_loss: 1.4044e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 1s - loss: 8.3207e-05 - acc: 0.0000e+00 - val_loss: 4.2748e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 1s - loss: 8.5406e-05 - acc: 0.0000e+00 - val_loss: 1.4677e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 1s - loss: 8.3313e-05 - acc: 0.0000e+00 - val_loss: 1.4022e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 1s - loss: 7.7492e-05 - acc: 0.0000e+00 - val_loss: 3.0055e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 1s - loss: 7.9549e-05 - acc: 0.0000e+00 - val_loss: 1.4027e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 1s - loss: 7.7814e-05 - acc: 0.0000e+00 - val_loss: 1.5432e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 1s - loss: 7.8402e-05 - acc: 0.0000e+00 - val_loss: 1.5542e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 1s - loss: 8.4894e-05 - acc: 0.0000e+00 - val_loss: 3.0311e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 1s - loss: 7.7918e-05 - acc: 0.0000e+00 - val_loss: 1.7926e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 1s - loss: 7.6159e-05 - acc: 0.0000e+00 - val_loss: 1.4333e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 1s - loss: 7.6418e-05 - acc: 0.0000e+00 - val_loss: 1.5161e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 1s - loss: 7.5919e-05 - acc: 0.0000e+00 - val_loss: 1.7120e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 1s - loss: 8.5666e-05 - acc: 0.0000e+00 - val_loss: 1.6153e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 1s - loss: 7.4419e-05 - acc: 0.0000e+00 - val_loss: 1.3607e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 1s - loss: 7.3003e-05 - acc: 0.0000e+00 - val_loss: 1.6121e-04 - val_acc: 0.0000e+00
Train Score: 0.00004 MSE (0.01 RMSE)
Test Score: 0.00138 MSE (0.04 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_11 (LSTM)               (None, 22, 64)            17664     
_________________________________________________________________
dropout_11 (Dropout)         (None, 22, 64)            0         
_________________________________________________________________
lstm_12 (LSTM)               (None, 64)                33024     
_________________________________________________________________
dropout_12 (Dropout)         (None, 64)                0         
_________________________________________________________________
dense_11 (Dense)             (None, 64)                4160      
_________________________________________________________________
dense_12 (Dense)             (None, 1)                 65        
=================================================================
Total params: 54,913
Trainable params: 54,913
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 3s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 7.6493e-04 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 1s - loss: 7.1870e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 1s - loss: 3.2406e-04 - acc: 0.0000e+00 - val_loss: 8.7555e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 1s - loss: 2.5841e-04 - acc: 0.0000e+00 - val_loss: 4.5806e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 1s - loss: 2.3539e-04 - acc: 0.0000e+00 - val_loss: 3.8046e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 1s - loss: 2.1862e-04 - acc: 0.0000e+00 - val_loss: 2.8976e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 1s - loss: 2.0569e-04 - acc: 0.0000e+00 - val_loss: 2.7522e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 1s - loss: 1.8941e-04 - acc: 0.0000e+00 - val_loss: 2.5996e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 1s - loss: 1.8259e-04 - acc: 0.0000e+00 - val_loss: 2.8500e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 1s - loss: 1.7568e-04 - acc: 0.0000e+00 - val_loss: 2.4485e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 1s - loss: 1.7467e-04 - acc: 0.0000e+00 - val_loss: 2.2767e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 1s - loss: 1.6192e-04 - acc: 0.0000e+00 - val_loss: 2.2818e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 1s - loss: 1.7912e-04 - acc: 0.0000e+00 - val_loss: 3.0594e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 1s - loss: 1.7072e-04 - acc: 0.0000e+00 - val_loss: 3.2454e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 1s - loss: 1.5923e-04 - acc: 0.0000e+00 - val_loss: 2.2892e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 1s - loss: 1.5762e-04 - acc: 0.0000e+00 - val_loss: 2.4994e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 1s - loss: 1.5353e-04 - acc: 0.0000e+00 - val_loss: 5.3604e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 1s - loss: 1.5253e-04 - acc: 0.0000e+00 - val_loss: 2.1351e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 1s - loss: 1.4118e-04 - acc: 0.0000e+00 - val_loss: 2.1671e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 1s - loss: 1.4606e-04 - acc: 0.0000e+00 - val_loss: 2.3224e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 1s - loss: 1.4970e-04 - acc: 0.0000e+00 - val_loss: 2.0434e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 1s - loss: 1.4506e-04 - acc: 0.0000e+00 - val_loss: 3.2599e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 1s - loss: 1.4201e-04 - acc: 0.0000e+00 - val_loss: 2.0185e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 1s - loss: 1.3931e-04 - acc: 0.0000e+00 - val_loss: 2.2374e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 1s - loss: 1.4450e-04 - acc: 0.0000e+00 - val_loss: 2.5359e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 1s - loss: 1.3178e-04 - acc: 0.0000e+00 - val_loss: 1.9632e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 1s - loss: 1.3041e-04 - acc: 0.0000e+00 - val_loss: 1.9674e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 1s - loss: 1.2723e-04 - acc: 0.0000e+00 - val_loss: 2.2081e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 1s - loss: 1.3133e-04 - acc: 0.0000e+00 - val_loss: 3.2289e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 1s - loss: 1.3373e-04 - acc: 0.0000e+00 - val_loss: 2.2305e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 1s - loss: 1.2955e-04 - acc: 0.0000e+00 - val_loss: 1.9149e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 1s - loss: 1.2329e-04 - acc: 0.0000e+00 - val_loss: 2.8650e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 1s - loss: 1.4788e-04 - acc: 0.0000e+00 - val_loss: 2.1054e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 1s - loss: 1.3056e-04 - acc: 0.0000e+00 - val_loss: 2.0794e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 1s - loss: 1.1670e-04 - acc: 0.0000e+00 - val_loss: 2.1426e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 1s - loss: 1.2510e-04 - acc: 0.0000e+00 - val_loss: 2.3832e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 1s - loss: 1.3198e-04 - acc: 0.0000e+00 - val_loss: 2.6084e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 1s - loss: 1.2277e-04 - acc: 0.0000e+00 - val_loss: 1.7984e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 1s - loss: 1.1547e-04 - acc: 0.0000e+00 - val_loss: 1.9252e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 1s - loss: 1.1456e-04 - acc: 0.0000e+00 - val_loss: 2.1058e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 1s - loss: 1.0897e-04 - acc: 0.0000e+00 - val_loss: 1.9886e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 1s - loss: 1.1372e-04 - acc: 0.0000e+00 - val_loss: 3.1545e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 1s - loss: 1.2244e-04 - acc: 0.0000e+00 - val_loss: 1.8304e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 1s - loss: 1.0779e-04 - acc: 0.0000e+00 - val_loss: 1.8690e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 1s - loss: 1.0383e-04 - acc: 0.0000e+00 - val_loss: 1.7989e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 1s - loss: 1.0365e-04 - acc: 0.0000e+00 - val_loss: 1.7738e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 1s - loss: 9.9621e-05 - acc: 0.0000e+00 - val_loss: 2.4415e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 1s - loss: 1.1287e-04 - acc: 0.0000e+00 - val_loss: 1.7203e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 1s - loss: 1.0126e-04 - acc: 0.0000e+00 - val_loss: 1.8039e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 1s - loss: 9.4142e-05 - acc: 0.0000e+00 - val_loss: 1.6777e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 1s - loss: 1.0252e-04 - acc: 0.0000e+00 - val_loss: 1.6583e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 1s - loss: 9.6325e-05 - acc: 0.0000e+00 - val_loss: 1.9728e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 1s - loss: 1.0733e-04 - acc: 0.0000e+00 - val_loss: 1.8468e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 1s - loss: 9.1328e-05 - acc: 0.0000e+00 - val_loss: 1.6565e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 1s - loss: 1.0119e-04 - acc: 0.0000e+00 - val_loss: 2.8045e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 1s - loss: 9.4479e-05 - acc: 0.0000e+00 - val_loss: 1.6905e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 1s - loss: 9.6434e-05 - acc: 0.0000e+00 - val_loss: 2.1274e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 1s - loss: 9.5829e-05 - acc: 0.0000e+00 - val_loss: 3.5569e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 1s - loss: 8.8612e-05 - acc: 0.0000e+00 - val_loss: 1.6402e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 1s - loss: 8.7552e-05 - acc: 0.0000e+00 - val_loss: 3.3322e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 1s - loss: 8.9838e-05 - acc: 0.0000e+00 - val_loss: 2.6305e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 1s - loss: 8.7704e-05 - acc: 0.0000e+00 - val_loss: 1.7996e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 1s - loss: 8.2530e-05 - acc: 0.0000e+00 - val_loss: 1.9366e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 1s - loss: 8.8412e-05 - acc: 0.0000e+00 - val_loss: 2.1357e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 1s - loss: 7.9956e-05 - acc: 0.0000e+00 - val_loss: 3.8152e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 1s - loss: 8.1437e-05 - acc: 0.0000e+00 - val_loss: 1.6025e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 1s - loss: 8.6889e-05 - acc: 0.0000e+00 - val_loss: 1.8163e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 1s - loss: 7.8690e-05 - acc: 0.0000e+00 - val_loss: 1.5278e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 1s - loss: 8.9613e-05 - acc: 0.0000e+00 - val_loss: 2.7162e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 1s - loss: 8.5427e-05 - acc: 0.0000e+00 - val_loss: 1.8852e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 1s - loss: 8.2462e-05 - acc: 0.0000e+00 - val_loss: 1.6141e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 1s - loss: 7.8553e-05 - acc: 0.0000e+00 - val_loss: 1.4941e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 1s - loss: 7.7627e-05 - acc: 0.0000e+00 - val_loss: 2.2867e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 1s - loss: 7.4349e-05 - acc: 0.0000e+00 - val_loss: 3.2484e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 1s - loss: 7.5879e-05 - acc: 0.0000e+00 - val_loss: 2.2411e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 1s - loss: 7.6685e-05 - acc: 0.0000e+00 - val_loss: 3.4162e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 1s - loss: 7.3943e-05 - acc: 0.0000e+00 - val_loss: 1.4585e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 1s - loss: 8.2147e-05 - acc: 0.0000e+00 - val_loss: 3.3074e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 1s - loss: 8.3522e-05 - acc: 0.0000e+00 - val_loss: 1.6361e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 1s - loss: 7.3981e-05 - acc: 0.0000e+00 - val_loss: 2.8698e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 1s - loss: 6.9285e-05 - acc: 0.0000e+00 - val_loss: 2.5161e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 1s - loss: 7.6671e-05 - acc: 0.0000e+00 - val_loss: 3.2597e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 1s - loss: 7.2722e-05 - acc: 0.0000e+00 - val_loss: 1.4449e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 1s - loss: 6.8358e-05 - acc: 0.0000e+00 - val_loss: 1.6838e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 1s - loss: 6.8868e-05 - acc: 0.0000e+00 - val_loss: 2.6274e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 1s - loss: 6.7294e-05 - acc: 0.0000e+00 - val_loss: 1.5343e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 1s - loss: 6.9974e-05 - acc: 0.0000e+00 - val_loss: 1.4106e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 1s - loss: 7.0446e-05 - acc: 0.0000e+00 - val_loss: 1.5149e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 1s - loss: 6.7856e-05 - acc: 0.0000e+00 - val_loss: 2.3106e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 1s - loss: 6.3477e-05 - acc: 0.0000e+00 - val_loss: 1.4003e-04 - val_acc: 0.0000e+00
Train Score: 0.00003 MSE (0.01 RMSE)
Test Score: 0.00038 MSE (0.02 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_13 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_13 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_14 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_14 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_13 (Dense)             (None, 16)                2064      
_________________________________________________________________
dense_14 (Dense)             (None, 1)                 17        
=================================================================
Total params: 201,761
Trainable params: 201,761
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 3s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 1s - loss: 7.0135e-04 - acc: 0.0000e+00 - val_loss: 7.9778e-04 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 1s - loss: 2.2988e-04 - acc: 0.0000e+00 - val_loss: 4.0974e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 1s - loss: 1.5909e-04 - acc: 0.0000e+00 - val_loss: 2.7059e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 1s - loss: 1.6150e-04 - acc: 0.0000e+00 - val_loss: 2.8499e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 1s - loss: 1.4205e-04 - acc: 0.0000e+00 - val_loss: 2.7307e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 1s - loss: 1.3622e-04 - acc: 0.0000e+00 - val_loss: 2.6169e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 1s - loss: 1.3439e-04 - acc: 0.0000e+00 - val_loss: 3.6463e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 1s - loss: 1.3161e-04 - acc: 0.0000e+00 - val_loss: 3.2192e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 1s - loss: 1.2638e-04 - acc: 0.0000e+00 - val_loss: 2.4598e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 1s - loss: 1.3444e-04 - acc: 0.0000e+00 - val_loss: 2.5040e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 1s - loss: 1.5538e-04 - acc: 0.0000e+00 - val_loss: 2.6858e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 1s - loss: 1.2431e-04 - acc: 0.0000e+00 - val_loss: 2.9642e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 1s - loss: 1.2674e-04 - acc: 0.0000e+00 - val_loss: 2.2718e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 1s - loss: 1.2662e-04 - acc: 0.0000e+00 - val_loss: 2.3571e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 1s - loss: 1.1523e-04 - acc: 0.0000e+00 - val_loss: 3.5211e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 1s - loss: 1.1906e-04 - acc: 0.0000e+00 - val_loss: 2.3659e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 1s - loss: 1.1073e-04 - acc: 0.0000e+00 - val_loss: 2.4160e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 1s - loss: 1.3022e-04 - acc: 0.0000e+00 - val_loss: 5.2010e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 1s - loss: 1.1976e-04 - acc: 0.0000e+00 - val_loss: 2.2919e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 1s - loss: 1.2578e-04 - acc: 0.0000e+00 - val_loss: 1.9875e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 1s - loss: 1.1479e-04 - acc: 0.0000e+00 - val_loss: 5.4671e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 1s - loss: 1.2771e-04 - acc: 0.0000e+00 - val_loss: 2.2813e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 1s - loss: 1.1064e-04 - acc: 0.0000e+00 - val_loss: 2.1403e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 1s - loss: 1.1562e-04 - acc: 0.0000e+00 - val_loss: 4.6397e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 1s - loss: 1.1513e-04 - acc: 0.0000e+00 - val_loss: 2.0225e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 1s - loss: 1.1052e-04 - acc: 0.0000e+00 - val_loss: 2.1024e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 1s - loss: 1.2012e-04 - acc: 0.0000e+00 - val_loss: 3.0166e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 1s - loss: 1.3098e-04 - acc: 0.0000e+00 - val_loss: 2.8126e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 1s - loss: 1.0054e-04 - acc: 0.0000e+00 - val_loss: 1.8434e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 1s - loss: 1.0663e-04 - acc: 0.0000e+00 - val_loss: 1.9666e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 1s - loss: 1.0971e-04 - acc: 0.0000e+00 - val_loss: 4.8166e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 1s - loss: 1.0433e-04 - acc: 0.0000e+00 - val_loss: 1.7781e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 1s - loss: 1.0173e-04 - acc: 0.0000e+00 - val_loss: 1.7939e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 1s - loss: 1.0078e-04 - acc: 0.0000e+00 - val_loss: 3.4770e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 1s - loss: 1.0440e-04 - acc: 0.0000e+00 - val_loss: 1.7856e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 1s - loss: 9.9727e-05 - acc: 0.0000e+00 - val_loss: 2.0045e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 1s - loss: 8.8596e-05 - acc: 0.0000e+00 - val_loss: 2.6811e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 1s - loss: 9.3717e-05 - acc: 0.0000e+00 - val_loss: 1.8528e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 1s - loss: 9.5726e-05 - acc: 0.0000e+00 - val_loss: 1.7148e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 1s - loss: 9.3957e-05 - acc: 0.0000e+00 - val_loss: 1.9260e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 1s - loss: 8.9433e-05 - acc: 0.0000e+00 - val_loss: 2.8541e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 1s - loss: 1.1211e-04 - acc: 0.0000e+00 - val_loss: 1.7728e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 1s - loss: 8.8419e-05 - acc: 0.0000e+00 - val_loss: 1.8370e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 1s - loss: 8.9244e-05 - acc: 0.0000e+00 - val_loss: 1.6803e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 1s - loss: 9.8887e-05 - acc: 0.0000e+00 - val_loss: 2.5497e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 1s - loss: 1.0554e-04 - acc: 0.0000e+00 - val_loss: 8.6904e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 1s - loss: 1.0222e-04 - acc: 0.0000e+00 - val_loss: 1.5651e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 1s - loss: 9.9952e-05 - acc: 0.0000e+00 - val_loss: 2.0667e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 1s - loss: 9.9617e-05 - acc: 0.0000e+00 - val_loss: 3.3641e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 1s - loss: 1.0052e-04 - acc: 0.0000e+00 - val_loss: 3.1608e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 1s - loss: 8.7289e-05 - acc: 0.0000e+00 - val_loss: 2.0358e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 1s - loss: 8.9652e-05 - acc: 0.0000e+00 - val_loss: 3.1544e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 1s - loss: 8.7613e-05 - acc: 0.0000e+00 - val_loss: 1.7070e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 1s - loss: 9.0939e-05 - acc: 0.0000e+00 - val_loss: 1.4660e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 1s - loss: 9.8302e-05 - acc: 0.0000e+00 - val_loss: 1.4959e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 1s - loss: 9.2394e-05 - acc: 0.0000e+00 - val_loss: 2.2587e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 1s - loss: 8.5878e-05 - acc: 0.0000e+00 - val_loss: 1.4417e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 1s - loss: 8.0366e-05 - acc: 0.0000e+00 - val_loss: 1.3990e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 1s - loss: 8.9234e-05 - acc: 0.0000e+00 - val_loss: 5.4526e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 1s - loss: 1.0924e-04 - acc: 0.0000e+00 - val_loss: 1.6160e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 1s - loss: 8.3049e-05 - acc: 0.0000e+00 - val_loss: 1.5958e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 1s - loss: 8.7156e-05 - acc: 0.0000e+00 - val_loss: 1.4954e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 1s - loss: 8.4683e-05 - acc: 0.0000e+00 - val_loss: 1.3376e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 1s - loss: 8.5895e-05 - acc: 0.0000e+00 - val_loss: 1.3590e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 1s - loss: 9.2361e-05 - acc: 0.0000e+00 - val_loss: 3.2972e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 1s - loss: 9.3841e-05 - acc: 0.0000e+00 - val_loss: 1.5102e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 1s - loss: 7.6869e-05 - acc: 0.0000e+00 - val_loss: 1.3101e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 1s - loss: 7.9675e-05 - acc: 0.0000e+00 - val_loss: 2.9063e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 1s - loss: 8.3225e-05 - acc: 0.0000e+00 - val_loss: 1.6304e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 1s - loss: 7.9093e-05 - acc: 0.0000e+00 - val_loss: 1.5105e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 1s - loss: 8.5507e-05 - acc: 0.0000e+00 - val_loss: 1.2687e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 1s - loss: 7.8855e-05 - acc: 0.0000e+00 - val_loss: 1.4716e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 1s - loss: 7.5875e-05 - acc: 0.0000e+00 - val_loss: 1.2970e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 1s - loss: 8.9661e-05 - acc: 0.0000e+00 - val_loss: 2.9140e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 1s - loss: 9.0714e-05 - acc: 0.0000e+00 - val_loss: 1.5282e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 1s - loss: 7.8692e-05 - acc: 0.0000e+00 - val_loss: 3.0283e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 1s - loss: 7.7298e-05 - acc: 0.0000e+00 - val_loss: 1.2766e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 1s - loss: 7.7579e-05 - acc: 0.0000e+00 - val_loss: 1.2711e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 1s - loss: 7.8256e-05 - acc: 0.0000e+00 - val_loss: 1.3092e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 1s - loss: 7.2849e-05 - acc: 0.0000e+00 - val_loss: 1.2915e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 1s - loss: 8.0962e-05 - acc: 0.0000e+00 - val_loss: 1.8090e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 1s - loss: 9.0272e-05 - acc: 0.0000e+00 - val_loss: 2.5761e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 1s - loss: 8.1490e-05 - acc: 0.0000e+00 - val_loss: 2.3482e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 1s - loss: 8.4252e-05 - acc: 0.0000e+00 - val_loss: 1.9887e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 1s - loss: 7.6815e-05 - acc: 0.0000e+00 - val_loss: 1.7044e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 1s - loss: 7.1103e-05 - acc: 0.0000e+00 - val_loss: 1.1659e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 1s - loss: 7.3005e-05 - acc: 0.0000e+00 - val_loss: 1.1951e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 1s - loss: 7.6756e-05 - acc: 0.0000e+00 - val_loss: 1.7315e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 1s - loss: 7.5129e-05 - acc: 0.0000e+00 - val_loss: 1.2389e-04 - val_acc: 0.0000e+00
Train Score: 0.00003 MSE (0.01 RMSE)
Test Score: 0.00177 MSE (0.04 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_15 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_15 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_16 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_16 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_15 (Dense)             (None, 32)                4128      
_________________________________________________________________
dense_16 (Dense)             (None, 1)                 33        
=================================================================
Total params: 203,841
Trainable params: 203,841
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 3s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.0062 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 1s - loss: 6.4685e-04 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 1s - loss: 2.3079e-04 - acc: 0.0000e+00 - val_loss: 4.8296e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 1s - loss: 1.4901e-04 - acc: 0.0000e+00 - val_loss: 3.1693e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 1s - loss: 1.4651e-04 - acc: 0.0000e+00 - val_loss: 3.4361e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 1s - loss: 1.4371e-04 - acc: 0.0000e+00 - val_loss: 4.0962e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 1s - loss: 1.3268e-04 - acc: 0.0000e+00 - val_loss: 2.8050e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 1s - loss: 1.2866e-04 - acc: 0.0000e+00 - val_loss: 2.2919e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 1s - loss: 1.2785e-04 - acc: 0.0000e+00 - val_loss: 3.1446e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 1s - loss: 1.2105e-04 - acc: 0.0000e+00 - val_loss: 2.3474e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 1s - loss: 1.1481e-04 - acc: 0.0000e+00 - val_loss: 2.2996e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 1s - loss: 1.2460e-04 - acc: 0.0000e+00 - val_loss: 2.5498e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 1s - loss: 1.1191e-04 - acc: 0.0000e+00 - val_loss: 2.2189e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 1s - loss: 1.0823e-04 - acc: 0.0000e+00 - val_loss: 2.1247e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 1s - loss: 1.2236e-04 - acc: 0.0000e+00 - val_loss: 3.5636e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 1s - loss: 1.1671e-04 - acc: 0.0000e+00 - val_loss: 2.0682e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 1s - loss: 1.1988e-04 - acc: 0.0000e+00 - val_loss: 2.1105e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 1s - loss: 1.1019e-04 - acc: 0.0000e+00 - val_loss: 2.2230e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 1s - loss: 1.2445e-04 - acc: 0.0000e+00 - val_loss: 3.1447e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 1s - loss: 1.3697e-04 - acc: 0.0000e+00 - val_loss: 9.0660e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 1s - loss: 1.4049e-04 - acc: 0.0000e+00 - val_loss: 2.6466e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 1s - loss: 1.0399e-04 - acc: 0.0000e+00 - val_loss: 2.2812e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 1s - loss: 1.1088e-04 - acc: 0.0000e+00 - val_loss: 2.5135e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 1s - loss: 9.8831e-05 - acc: 0.0000e+00 - val_loss: 1.9055e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 1s - loss: 9.7984e-05 - acc: 0.0000e+00 - val_loss: 2.5349e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 1s - loss: 1.0425e-04 - acc: 0.0000e+00 - val_loss: 3.5400e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 1s - loss: 9.9999e-05 - acc: 0.0000e+00 - val_loss: 2.4460e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 1s - loss: 9.4188e-05 - acc: 0.0000e+00 - val_loss: 1.8002e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 1s - loss: 1.0128e-04 - acc: 0.0000e+00 - val_loss: 2.4929e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 1s - loss: 9.3605e-05 - acc: 0.0000e+00 - val_loss: 3.5280e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 1s - loss: 1.0502e-04 - acc: 0.0000e+00 - val_loss: 1.9450e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 1s - loss: 1.0064e-04 - acc: 0.0000e+00 - val_loss: 5.6009e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 1s - loss: 1.1432e-04 - acc: 0.0000e+00 - val_loss: 1.7679e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 1s - loss: 9.2862e-05 - acc: 0.0000e+00 - val_loss: 1.7638e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 1s - loss: 9.8320e-05 - acc: 0.0000e+00 - val_loss: 6.0299e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 1s - loss: 1.0669e-04 - acc: 0.0000e+00 - val_loss: 1.6939e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 1s - loss: 9.4543e-05 - acc: 0.0000e+00 - val_loss: 2.0192e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 1s - loss: 1.0118e-04 - acc: 0.0000e+00 - val_loss: 2.3486e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 1s - loss: 8.5330e-05 - acc: 0.0000e+00 - val_loss: 1.6804e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 1s - loss: 9.2218e-05 - acc: 0.0000e+00 - val_loss: 1.8588e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 1s - loss: 1.0036e-04 - acc: 0.0000e+00 - val_loss: 2.4266e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 1s - loss: 8.8635e-05 - acc: 0.0000e+00 - val_loss: 1.5695e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 1s - loss: 8.3445e-05 - acc: 0.0000e+00 - val_loss: 1.5683e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 1s - loss: 8.4116e-05 - acc: 0.0000e+00 - val_loss: 1.5286e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 1s - loss: 9.4973e-05 - acc: 0.0000e+00 - val_loss: 1.5757e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 1s - loss: 9.8143e-05 - acc: 0.0000e+00 - val_loss: 2.6552e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 1s - loss: 9.6381e-05 - acc: 0.0000e+00 - val_loss: 1.6570e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 1s - loss: 1.0738e-04 - acc: 0.0000e+00 - val_loss: 2.0487e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 1s - loss: 8.9032e-05 - acc: 0.0000e+00 - val_loss: 1.4960e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 1s - loss: 8.2504e-05 - acc: 0.0000e+00 - val_loss: 2.9658e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 1s - loss: 8.5187e-05 - acc: 0.0000e+00 - val_loss: 1.9009e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 1s - loss: 8.6104e-05 - acc: 0.0000e+00 - val_loss: 1.5197e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 1s - loss: 7.6806e-05 - acc: 0.0000e+00 - val_loss: 1.5614e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 1s - loss: 8.6898e-05 - acc: 0.0000e+00 - val_loss: 1.9437e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 1s - loss: 9.6926e-05 - acc: 0.0000e+00 - val_loss: 1.8327e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 1s - loss: 1.0379e-04 - acc: 0.0000e+00 - val_loss: 1.4271e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 1s - loss: 8.3747e-05 - acc: 0.0000e+00 - val_loss: 1.6740e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 1s - loss: 8.2297e-05 - acc: 0.0000e+00 - val_loss: 2.3617e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 1s - loss: 7.4170e-05 - acc: 0.0000e+00 - val_loss: 1.4652e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 1s - loss: 7.2720e-05 - acc: 0.0000e+00 - val_loss: 1.3791e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 1s - loss: 7.3972e-05 - acc: 0.0000e+00 - val_loss: 1.6514e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 1s - loss: 8.3344e-05 - acc: 0.0000e+00 - val_loss: 3.4760e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 1s - loss: 8.1770e-05 - acc: 0.0000e+00 - val_loss: 1.5910e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 1s - loss: 7.7348e-05 - acc: 0.0000e+00 - val_loss: 1.3755e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 1s - loss: 7.5690e-05 - acc: 0.0000e+00 - val_loss: 2.0227e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 1s - loss: 8.4634e-05 - acc: 0.0000e+00 - val_loss: 3.2888e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 1s - loss: 9.2604e-05 - acc: 0.0000e+00 - val_loss: 2.7122e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 1s - loss: 7.6580e-05 - acc: 0.0000e+00 - val_loss: 2.4608e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 1s - loss: 7.8147e-05 - acc: 0.0000e+00 - val_loss: 2.0378e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 1s - loss: 7.4151e-05 - acc: 0.0000e+00 - val_loss: 1.6642e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 1s - loss: 7.2906e-05 - acc: 0.0000e+00 - val_loss: 1.3460e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 1s - loss: 7.1060e-05 - acc: 0.0000e+00 - val_loss: 1.6483e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 1s - loss: 7.2112e-05 - acc: 0.0000e+00 - val_loss: 2.2681e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 1s - loss: 7.2737e-05 - acc: 0.0000e+00 - val_loss: 1.8742e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 1s - loss: 7.2589e-05 - acc: 0.0000e+00 - val_loss: 1.3082e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 1s - loss: 7.4040e-05 - acc: 0.0000e+00 - val_loss: 1.2492e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 1s - loss: 6.6149e-05 - acc: 0.0000e+00 - val_loss: 1.3526e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 1s - loss: 6.4875e-05 - acc: 0.0000e+00 - val_loss: 2.4930e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 1s - loss: 6.7066e-05 - acc: 0.0000e+00 - val_loss: 1.5552e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 1s - loss: 7.4858e-05 - acc: 0.0000e+00 - val_loss: 1.5645e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 1s - loss: 7.7532e-05 - acc: 0.0000e+00 - val_loss: 1.1672e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 1s - loss: 7.3302e-05 - acc: 0.0000e+00 - val_loss: 1.7040e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 1s - loss: 6.5768e-05 - acc: 0.0000e+00 - val_loss: 1.4459e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 1s - loss: 7.3349e-05 - acc: 0.0000e+00 - val_loss: 1.2094e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 1s - loss: 7.0339e-05 - acc: 0.0000e+00 - val_loss: 1.2544e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 1s - loss: 7.0084e-05 - acc: 0.0000e+00 - val_loss: 1.3450e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 1s - loss: 6.4950e-05 - acc: 0.0000e+00 - val_loss: 2.5771e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 1s - loss: 6.4149e-05 - acc: 0.0000e+00 - val_loss: 1.1225e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 1s - loss: 7.1039e-05 - acc: 0.0000e+00 - val_loss: 1.3706e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 1s - loss: 6.8161e-05 - acc: 0.0000e+00 - val_loss: 1.4752e-04 - val_acc: 0.0000e+00
Train Score: 0.00004 MSE (0.01 RMSE)
Test Score: 0.00058 MSE (0.02 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_17 (LSTM)               (None, 22, 128)           68096     
_________________________________________________________________
dropout_17 (Dropout)         (None, 22, 128)           0         
_________________________________________________________________
lstm_18 (LSTM)               (None, 128)               131584    
_________________________________________________________________
dropout_18 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_17 (Dense)             (None, 64)                8256      
_________________________________________________________________
dense_18 (Dense)             (None, 1)                 65        
=================================================================
Total params: 208,001
Trainable params: 208,001
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 3s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 1s - loss: 5.3170e-04 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 1s - loss: 2.4575e-04 - acc: 0.0000e+00 - val_loss: 7.3277e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 1s - loss: 1.6794e-04 - acc: 0.0000e+00 - val_loss: 3.8844e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 1s - loss: 1.4654e-04 - acc: 0.0000e+00 - val_loss: 3.0690e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 1s - loss: 1.3218e-04 - acc: 0.0000e+00 - val_loss: 2.6919e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 1s - loss: 1.2376e-04 - acc: 0.0000e+00 - val_loss: 4.8150e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 1s - loss: 1.2782e-04 - acc: 0.0000e+00 - val_loss: 2.5125e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 1s - loss: 1.1812e-04 - acc: 0.0000e+00 - val_loss: 2.2962e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 1s - loss: 1.1712e-04 - acc: 0.0000e+00 - val_loss: 2.5547e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 1s - loss: 1.1125e-04 - acc: 0.0000e+00 - val_loss: 2.1821e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 1s - loss: 1.0734e-04 - acc: 0.0000e+00 - val_loss: 2.6543e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 1s - loss: 1.0973e-04 - acc: 0.0000e+00 - val_loss: 2.5447e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 1s - loss: 1.0790e-04 - acc: 0.0000e+00 - val_loss: 2.3483e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 1s - loss: 1.1635e-04 - acc: 0.0000e+00 - val_loss: 3.1654e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 1s - loss: 1.1173e-04 - acc: 0.0000e+00 - val_loss: 2.5463e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 1s - loss: 1.0314e-04 - acc: 0.0000e+00 - val_loss: 2.2820e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 1s - loss: 9.8267e-05 - acc: 0.0000e+00 - val_loss: 2.6831e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 1s - loss: 1.0017e-04 - acc: 0.0000e+00 - val_loss: 2.5274e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 1s - loss: 1.0263e-04 - acc: 0.0000e+00 - val_loss: 1.9952e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 1s - loss: 9.6244e-05 - acc: 0.0000e+00 - val_loss: 3.9944e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 1s - loss: 1.0117e-04 - acc: 0.0000e+00 - val_loss: 3.7485e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 1s - loss: 9.9505e-05 - acc: 0.0000e+00 - val_loss: 2.2896e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 1s - loss: 9.9569e-05 - acc: 0.0000e+00 - val_loss: 3.4030e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 1s - loss: 1.0822e-04 - acc: 0.0000e+00 - val_loss: 4.0790e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 1s - loss: 1.0860e-04 - acc: 0.0000e+00 - val_loss: 1.9156e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 1s - loss: 1.0410e-04 - acc: 0.0000e+00 - val_loss: 2.0479e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 1s - loss: 1.0539e-04 - acc: 0.0000e+00 - val_loss: 4.8511e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 1s - loss: 1.0613e-04 - acc: 0.0000e+00 - val_loss: 2.3689e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 1s - loss: 9.6721e-05 - acc: 0.0000e+00 - val_loss: 2.4239e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 1s - loss: 9.4572e-05 - acc: 0.0000e+00 - val_loss: 2.5441e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 1s - loss: 9.6483e-05 - acc: 0.0000e+00 - val_loss: 1.7625e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 1s - loss: 9.7408e-05 - acc: 0.0000e+00 - val_loss: 1.8031e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 1s - loss: 1.0514e-04 - acc: 0.0000e+00 - val_loss: 2.8330e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 1s - loss: 1.0244e-04 - acc: 0.0000e+00 - val_loss: 2.0231e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 1s - loss: 9.2074e-05 - acc: 0.0000e+00 - val_loss: 4.2372e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 1s - loss: 9.3310e-05 - acc: 0.0000e+00 - val_loss: 2.0393e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 1s - loss: 9.2361e-05 - acc: 0.0000e+00 - val_loss: 2.0808e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 1s - loss: 8.5475e-05 - acc: 0.0000e+00 - val_loss: 1.6378e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 1s - loss: 8.9638e-05 - acc: 0.0000e+00 - val_loss: 1.7345e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 1s - loss: 9.2546e-05 - acc: 0.0000e+00 - val_loss: 5.1234e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 1s - loss: 9.8866e-05 - acc: 0.0000e+00 - val_loss: 1.6806e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 1s - loss: 8.7333e-05 - acc: 0.0000e+00 - val_loss: 1.5935e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 1s - loss: 8.5070e-05 - acc: 0.0000e+00 - val_loss: 1.7870e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 1s - loss: 1.0402e-04 - acc: 0.0000e+00 - val_loss: 1.6310e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 1s - loss: 8.2837e-05 - acc: 0.0000e+00 - val_loss: 2.0760e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 1s - loss: 8.6093e-05 - acc: 0.0000e+00 - val_loss: 1.7476e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 1s - loss: 9.1053e-05 - acc: 0.0000e+00 - val_loss: 2.3940e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 1s - loss: 9.3303e-05 - acc: 0.0000e+00 - val_loss: 2.1411e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 1s - loss: 9.9595e-05 - acc: 0.0000e+00 - val_loss: 1.7173e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 1s - loss: 1.0123e-04 - acc: 0.0000e+00 - val_loss: 1.4850e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 1s - loss: 8.3607e-05 - acc: 0.0000e+00 - val_loss: 2.6273e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 1s - loss: 8.4974e-05 - acc: 0.0000e+00 - val_loss: 2.7776e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 1s - loss: 9.0545e-05 - acc: 0.0000e+00 - val_loss: 2.4902e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 1s - loss: 8.3915e-05 - acc: 0.0000e+00 - val_loss: 1.4514e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 1s - loss: 8.2979e-05 - acc: 0.0000e+00 - val_loss: 1.6482e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 1s - loss: 8.4259e-05 - acc: 0.0000e+00 - val_loss: 1.7087e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 1s - loss: 7.5439e-05 - acc: 0.0000e+00 - val_loss: 1.3558e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 1s - loss: 7.2382e-05 - acc: 0.0000e+00 - val_loss: 3.6089e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 1s - loss: 7.7982e-05 - acc: 0.0000e+00 - val_loss: 2.9310e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 1s - loss: 7.6975e-05 - acc: 0.0000e+00 - val_loss: 1.8401e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 1s - loss: 8.7190e-05 - acc: 0.0000e+00 - val_loss: 1.7782e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 1s - loss: 7.5438e-05 - acc: 0.0000e+00 - val_loss: 1.5962e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 1s - loss: 8.1334e-05 - acc: 0.0000e+00 - val_loss: 2.3654e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 1s - loss: 8.2133e-05 - acc: 0.0000e+00 - val_loss: 1.3777e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 1s - loss: 8.7748e-05 - acc: 0.0000e+00 - val_loss: 1.9043e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 1s - loss: 8.1454e-05 - acc: 0.0000e+00 - val_loss: 1.7700e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 1s - loss: 7.0152e-05 - acc: 0.0000e+00 - val_loss: 1.4367e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 1s - loss: 7.7906e-05 - acc: 0.0000e+00 - val_loss: 1.5113e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 1s - loss: 7.7129e-05 - acc: 0.0000e+00 - val_loss: 1.4507e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 1s - loss: 7.6346e-05 - acc: 0.0000e+00 - val_loss: 2.3820e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 1s - loss: 6.8821e-05 - acc: 0.0000e+00 - val_loss: 1.4567e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 1s - loss: 6.6241e-05 - acc: 0.0000e+00 - val_loss: 2.9517e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 1s - loss: 8.7688e-05 - acc: 0.0000e+00 - val_loss: 1.5904e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 1s - loss: 8.0465e-05 - acc: 0.0000e+00 - val_loss: 1.4471e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 1s - loss: 6.8593e-05 - acc: 0.0000e+00 - val_loss: 2.0261e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 1s - loss: 6.9944e-05 - acc: 0.0000e+00 - val_loss: 1.4383e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 1s - loss: 6.5650e-05 - acc: 0.0000e+00 - val_loss: 1.8689e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 1s - loss: 6.4978e-05 - acc: 0.0000e+00 - val_loss: 1.2999e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 1s - loss: 6.5564e-05 - acc: 0.0000e+00 - val_loss: 1.1914e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 1s - loss: 6.2767e-05 - acc: 0.0000e+00 - val_loss: 1.4582e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 1s - loss: 6.5553e-05 - acc: 0.0000e+00 - val_loss: 1.1507e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 1s - loss: 6.7178e-05 - acc: 0.0000e+00 - val_loss: 1.2124e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 1s - loss: 6.6201e-05 - acc: 0.0000e+00 - val_loss: 2.4094e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 1s - loss: 6.9020e-05 - acc: 0.0000e+00 - val_loss: 1.2595e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 1s - loss: 6.6232e-05 - acc: 0.0000e+00 - val_loss: 1.9633e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 1s - loss: 6.4495e-05 - acc: 0.0000e+00 - val_loss: 1.1766e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 1s - loss: 6.8396e-05 - acc: 0.0000e+00 - val_loss: 1.3237e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 1s - loss: 6.7739e-05 - acc: 0.0000e+00 - val_loss: 3.2675e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 1s - loss: 6.5728e-05 - acc: 0.0000e+00 - val_loss: 1.2978e-04 - val_acc: 0.0000e+00
Train Score: 0.00003 MSE (0.01 RMSE)
Test Score: 0.00051 MSE (0.02 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_19 (LSTM)               (None, 22, 256)           267264    
_________________________________________________________________
dropout_19 (Dropout)         (None, 22, 256)           0         
_________________________________________________________________
lstm_20 (LSTM)               (None, 256)               525312    
_________________________________________________________________
dropout_20 (Dropout)         (None, 256)               0         
_________________________________________________________________
dense_19 (Dense)             (None, 16)                4112      
_________________________________________________________________
dense_20 (Dense)             (None, 1)                 17        
=================================================================
Total params: 796,705
Trainable params: 796,705
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 4s - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 2s - loss: 4.3310e-04 - acc: 0.0000e+00 - val_loss: 6.3033e-04 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 2s - loss: 1.3745e-04 - acc: 0.0000e+00 - val_loss: 2.9478e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 2s - loss: 1.0845e-04 - acc: 0.0000e+00 - val_loss: 2.6393e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 2s - loss: 9.8539e-05 - acc: 0.0000e+00 - val_loss: 2.8786e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 2s - loss: 1.0005e-04 - acc: 0.0000e+00 - val_loss: 3.8208e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 2s - loss: 1.0627e-04 - acc: 0.0000e+00 - val_loss: 2.9814e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 2s - loss: 9.3142e-05 - acc: 0.0000e+00 - val_loss: 2.4569e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 2s - loss: 1.0297e-04 - acc: 0.0000e+00 - val_loss: 8.0937e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 2s - loss: 1.1450e-04 - acc: 0.0000e+00 - val_loss: 2.9584e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 2s - loss: 9.7232e-05 - acc: 0.0000e+00 - val_loss: 3.6543e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 2s - loss: 1.0030e-04 - acc: 0.0000e+00 - val_loss: 2.7120e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 2s - loss: 8.4092e-05 - acc: 0.0000e+00 - val_loss: 2.2967e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 2s - loss: 8.4319e-05 - acc: 0.0000e+00 - val_loss: 2.2113e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 2s - loss: 9.0924e-05 - acc: 0.0000e+00 - val_loss: 2.4672e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 2s - loss: 9.2067e-05 - acc: 0.0000e+00 - val_loss: 2.5258e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 2s - loss: 8.9537e-05 - acc: 0.0000e+00 - val_loss: 2.2433e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 2s - loss: 1.0070e-04 - acc: 0.0000e+00 - val_loss: 2.3401e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 2s - loss: 8.3551e-05 - acc: 0.0000e+00 - val_loss: 2.6915e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 2s - loss: 8.6260e-05 - acc: 0.0000e+00 - val_loss: 3.4556e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 2s - loss: 8.6131e-05 - acc: 0.0000e+00 - val_loss: 2.3066e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 2s - loss: 9.1438e-05 - acc: 0.0000e+00 - val_loss: 1.9621e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 2s - loss: 7.6994e-05 - acc: 0.0000e+00 - val_loss: 2.9049e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 2s - loss: 1.0167e-04 - acc: 0.0000e+00 - val_loss: 3.0819e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 2s - loss: 1.5200e-04 - acc: 0.0000e+00 - val_loss: 3.5665e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 2s - loss: 9.1190e-05 - acc: 0.0000e+00 - val_loss: 2.7108e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 2s - loss: 8.5759e-05 - acc: 0.0000e+00 - val_loss: 1.9469e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 2s - loss: 7.1196e-05 - acc: 0.0000e+00 - val_loss: 2.8074e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 2s - loss: 7.5423e-05 - acc: 0.0000e+00 - val_loss: 2.2285e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 2s - loss: 7.8698e-05 - acc: 0.0000e+00 - val_loss: 1.8129e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 2s - loss: 7.9218e-05 - acc: 0.0000e+00 - val_loss: 1.8054e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 2s - loss: 6.3541e-05 - acc: 0.0000e+00 - val_loss: 2.4826e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 2s - loss: 8.4460e-05 - acc: 0.0000e+00 - val_loss: 2.6781e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 2s - loss: 9.1310e-05 - acc: 0.0000e+00 - val_loss: 8.5075e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 2s - loss: 8.3961e-05 - acc: 0.0000e+00 - val_loss: 2.6197e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 2s - loss: 6.8477e-05 - acc: 0.0000e+00 - val_loss: 1.7291e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 2s - loss: 6.5672e-05 - acc: 0.0000e+00 - val_loss: 1.9632e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 2s - loss: 6.5011e-05 - acc: 0.0000e+00 - val_loss: 1.6803e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 2s - loss: 6.4966e-05 - acc: 0.0000e+00 - val_loss: 1.5507e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 2s - loss: 8.1242e-05 - acc: 0.0000e+00 - val_loss: 4.5114e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 2s - loss: 7.0893e-05 - acc: 0.0000e+00 - val_loss: 2.1917e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 2s - loss: 1.0144e-04 - acc: 0.0000e+00 - val_loss: 1.5423e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 2s - loss: 1.0315e-04 - acc: 0.0000e+00 - val_loss: 2.3157e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 2s - loss: 7.5310e-05 - acc: 0.0000e+00 - val_loss: 1.8406e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 2s - loss: 6.5867e-05 - acc: 0.0000e+00 - val_loss: 1.4676e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 2s - loss: 7.8867e-05 - acc: 0.0000e+00 - val_loss: 1.4567e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 2s - loss: 7.0101e-05 - acc: 0.0000e+00 - val_loss: 2.1274e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 2s - loss: 7.0356e-05 - acc: 0.0000e+00 - val_loss: 1.5297e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 2s - loss: 7.0867e-05 - acc: 0.0000e+00 - val_loss: 2.0406e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 2s - loss: 6.5659e-05 - acc: 0.0000e+00 - val_loss: 2.1243e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 2s - loss: 6.5254e-05 - acc: 0.0000e+00 - val_loss: 1.7563e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 2s - loss: 6.4401e-05 - acc: 0.0000e+00 - val_loss: 1.9959e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 2s - loss: 6.9934e-05 - acc: 0.0000e+00 - val_loss: 1.5515e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 2s - loss: 6.0821e-05 - acc: 0.0000e+00 - val_loss: 1.6017e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 2s - loss: 6.4790e-05 - acc: 0.0000e+00 - val_loss: 1.7770e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 2s - loss: 5.8293e-05 - acc: 0.0000e+00 - val_loss: 1.5406e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 2s - loss: 6.8009e-05 - acc: 0.0000e+00 - val_loss: 5.7840e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 2s - loss: 8.1708e-05 - acc: 0.0000e+00 - val_loss: 1.3741e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 2s - loss: 5.9262e-05 - acc: 0.0000e+00 - val_loss: 1.7759e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 2s - loss: 6.5764e-05 - acc: 0.0000e+00 - val_loss: 1.8133e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 2s - loss: 5.4565e-05 - acc: 0.0000e+00 - val_loss: 1.2009e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 2s - loss: 6.4235e-05 - acc: 0.0000e+00 - val_loss: 1.3678e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 2s - loss: 6.7988e-05 - acc: 0.0000e+00 - val_loss: 2.6540e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 2s - loss: 6.1905e-05 - acc: 0.0000e+00 - val_loss: 2.0152e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 2s - loss: 7.2395e-05 - acc: 0.0000e+00 - val_loss: 5.3506e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 2s - loss: 7.9165e-05 - acc: 0.0000e+00 - val_loss: 1.4375e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 2s - loss: 5.6399e-05 - acc: 0.0000e+00 - val_loss: 1.1962e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 2s - loss: 6.0283e-05 - acc: 0.0000e+00 - val_loss: 2.7642e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 2s - loss: 6.2015e-05 - acc: 0.0000e+00 - val_loss: 1.1984e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 2s - loss: 5.5632e-05 - acc: 0.0000e+00 - val_loss: 2.4695e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 2s - loss: 6.7413e-05 - acc: 0.0000e+00 - val_loss: 1.3103e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 2s - loss: 5.8937e-05 - acc: 0.0000e+00 - val_loss: 1.1271e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 2s - loss: 6.1963e-05 - acc: 0.0000e+00 - val_loss: 1.3947e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 2s - loss: 5.4133e-05 - acc: 0.0000e+00 - val_loss: 1.1294e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 2s - loss: 5.8761e-05 - acc: 0.0000e+00 - val_loss: 1.1375e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 2s - loss: 4.8949e-05 - acc: 0.0000e+00 - val_loss: 1.0669e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 2s - loss: 5.7898e-05 - acc: 0.0000e+00 - val_loss: 2.4619e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 2s - loss: 5.8353e-05 - acc: 0.0000e+00 - val_loss: 1.1384e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 2s - loss: 5.1247e-05 - acc: 0.0000e+00 - val_loss: 1.6741e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 2s - loss: 6.0448e-05 - acc: 0.0000e+00 - val_loss: 1.0701e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 2s - loss: 6.1989e-05 - acc: 0.0000e+00 - val_loss: 3.3952e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 2s - loss: 6.0954e-05 - acc: 0.0000e+00 - val_loss: 1.0516e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 2s - loss: 5.5193e-05 - acc: 0.0000e+00 - val_loss: 1.4999e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 2s - loss: 5.5840e-05 - acc: 0.0000e+00 - val_loss: 1.0229e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 2s - loss: 4.9783e-05 - acc: 0.0000e+00 - val_loss: 1.0082e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 2s - loss: 5.0107e-05 - acc: 0.0000e+00 - val_loss: 1.5523e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 2s - loss: 5.2912e-05 - acc: 0.0000e+00 - val_loss: 1.3979e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 2s - loss: 5.1143e-05 - acc: 0.0000e+00 - val_loss: 9.8613e-05 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 2s - loss: 4.9673e-05 - acc: 0.0000e+00 - val_loss: 1.0354e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 2s - loss: 4.9925e-05 - acc: 0.0000e+00 - val_loss: 9.6869e-05 - val_acc: 0.0000e+00
Train Score: 0.00002 MSE (0.00 RMSE)
Test Score: 0.00071 MSE (0.03 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_21 (LSTM)               (None, 22, 256)           267264    
_________________________________________________________________
dropout_21 (Dropout)         (None, 22, 256)           0         
_________________________________________________________________
lstm_22 (LSTM)               (None, 256)               525312    
_________________________________________________________________
dropout_22 (Dropout)         (None, 256)               0         
_________________________________________________________________
dense_21 (Dense)             (None, 32)                8224      
_________________________________________________________________
dense_22 (Dense)             (None, 1)                 33        
=================================================================
Total params: 800,833
Trainable params: 800,833
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 4s - loss: 0.0077 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 2s - loss: 3.1762e-04 - acc: 0.0000e+00 - val_loss: 6.2745e-04 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 2s - loss: 1.5307e-04 - acc: 0.0000e+00 - val_loss: 3.8348e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 2s - loss: 1.0878e-04 - acc: 0.0000e+00 - val_loss: 2.7381e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 2s - loss: 1.0411e-04 - acc: 0.0000e+00 - val_loss: 2.5320e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 2s - loss: 9.5639e-05 - acc: 0.0000e+00 - val_loss: 2.8773e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 2s - loss: 1.0412e-04 - acc: 0.0000e+00 - val_loss: 2.4197e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 2s - loss: 8.5853e-05 - acc: 0.0000e+00 - val_loss: 2.9283e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 2s - loss: 8.5361e-05 - acc: 0.0000e+00 - val_loss: 3.1665e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 2s - loss: 9.9827e-05 - acc: 0.0000e+00 - val_loss: 2.1170e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 2s - loss: 8.2749e-05 - acc: 0.0000e+00 - val_loss: 3.4663e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 2s - loss: 9.6069e-05 - acc: 0.0000e+00 - val_loss: 2.1086e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 2s - loss: 9.1938e-05 - acc: 0.0000e+00 - val_loss: 2.0096e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 2s - loss: 8.3655e-05 - acc: 0.0000e+00 - val_loss: 3.1467e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 2s - loss: 7.7304e-05 - acc: 0.0000e+00 - val_loss: 3.9102e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 2s - loss: 8.5494e-05 - acc: 0.0000e+00 - val_loss: 1.8815e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 2s - loss: 7.6201e-05 - acc: 0.0000e+00 - val_loss: 2.9243e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 2s - loss: 7.7635e-05 - acc: 0.0000e+00 - val_loss: 1.8097e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 2s - loss: 7.1000e-05 - acc: 0.0000e+00 - val_loss: 2.0198e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 2s - loss: 7.7215e-05 - acc: 0.0000e+00 - val_loss: 1.7100e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 2s - loss: 6.9287e-05 - acc: 0.0000e+00 - val_loss: 3.3076e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 2s - loss: 7.0768e-05 - acc: 0.0000e+00 - val_loss: 1.7367e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 2s - loss: 6.8671e-05 - acc: 0.0000e+00 - val_loss: 1.7448e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 2s - loss: 9.3348e-05 - acc: 0.0000e+00 - val_loss: 1.6685e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 2s - loss: 1.0435e-04 - acc: 0.0000e+00 - val_loss: 4.8952e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 2s - loss: 7.4527e-05 - acc: 0.0000e+00 - val_loss: 1.6342e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 2s - loss: 6.9623e-05 - acc: 0.0000e+00 - val_loss: 1.6144e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 2s - loss: 7.9107e-05 - acc: 0.0000e+00 - val_loss: 2.1771e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 2s - loss: 7.1061e-05 - acc: 0.0000e+00 - val_loss: 2.1561e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 2s - loss: 7.0758e-05 - acc: 0.0000e+00 - val_loss: 1.5033e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 2s - loss: 7.8513e-05 - acc: 0.0000e+00 - val_loss: 1.5234e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 2s - loss: 6.1976e-05 - acc: 0.0000e+00 - val_loss: 1.9570e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 2s - loss: 7.0862e-05 - acc: 0.0000e+00 - val_loss: 1.8302e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 2s - loss: 6.5913e-05 - acc: 0.0000e+00 - val_loss: 2.4115e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 2s - loss: 7.3368e-05 - acc: 0.0000e+00 - val_loss: 3.6891e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 2s - loss: 7.6337e-05 - acc: 0.0000e+00 - val_loss: 3.9069e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 2s - loss: 8.1995e-05 - acc: 0.0000e+00 - val_loss: 6.9418e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 2s - loss: 8.1108e-05 - acc: 0.0000e+00 - val_loss: 1.4640e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 2s - loss: 5.9820e-05 - acc: 0.0000e+00 - val_loss: 2.0474e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 2s - loss: 6.5930e-05 - acc: 0.0000e+00 - val_loss: 1.4271e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 2s - loss: 7.9180e-05 - acc: 0.0000e+00 - val_loss: 1.2974e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 2s - loss: 7.0244e-05 - acc: 0.0000e+00 - val_loss: 2.1810e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 2s - loss: 6.1317e-05 - acc: 0.0000e+00 - val_loss: 1.3586e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 2s - loss: 7.0566e-05 - acc: 0.0000e+00 - val_loss: 4.3881e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 2s - loss: 6.5703e-05 - acc: 0.0000e+00 - val_loss: 1.7405e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 2s - loss: 5.6752e-05 - acc: 0.0000e+00 - val_loss: 1.2157e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 2s - loss: 6.2369e-05 - acc: 0.0000e+00 - val_loss: 1.3106e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 2s - loss: 7.1495e-05 - acc: 0.0000e+00 - val_loss: 1.3340e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 2s - loss: 1.1741e-04 - acc: 0.0000e+00 - val_loss: 2.0397e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 2s - loss: 6.2194e-05 - acc: 0.0000e+00 - val_loss: 1.3704e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 2s - loss: 5.7236e-05 - acc: 0.0000e+00 - val_loss: 1.5559e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 2s - loss: 6.2989e-05 - acc: 0.0000e+00 - val_loss: 1.1665e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 2s - loss: 5.4549e-05 - acc: 0.0000e+00 - val_loss: 1.6413e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 2s - loss: 5.5304e-05 - acc: 0.0000e+00 - val_loss: 2.7490e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 2s - loss: 5.6171e-05 - acc: 0.0000e+00 - val_loss: 1.9201e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 2s - loss: 5.8330e-05 - acc: 0.0000e+00 - val_loss: 4.1592e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 2s - loss: 5.9335e-05 - acc: 0.0000e+00 - val_loss: 1.1460e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 2s - loss: 5.7448e-05 - acc: 0.0000e+00 - val_loss: 1.2008e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 2s - loss: 4.9417e-05 - acc: 0.0000e+00 - val_loss: 1.0950e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 2s - loss: 5.1140e-05 - acc: 0.0000e+00 - val_loss: 1.0766e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 2s - loss: 5.1937e-05 - acc: 0.0000e+00 - val_loss: 2.3076e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 2s - loss: 5.8266e-05 - acc: 0.0000e+00 - val_loss: 1.0760e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 2s - loss: 5.5861e-05 - acc: 0.0000e+00 - val_loss: 1.4369e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 2s - loss: 5.4926e-05 - acc: 0.0000e+00 - val_loss: 3.2232e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 2s - loss: 6.3248e-05 - acc: 0.0000e+00 - val_loss: 1.8481e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 2s - loss: 6.7679e-05 - acc: 0.0000e+00 - val_loss: 1.2871e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 2s - loss: 5.7880e-05 - acc: 0.0000e+00 - val_loss: 2.4993e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 2s - loss: 5.6346e-05 - acc: 0.0000e+00 - val_loss: 1.0069e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 2s - loss: 8.1183e-05 - acc: 0.0000e+00 - val_loss: 1.0771e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 2s - loss: 5.7082e-05 - acc: 0.0000e+00 - val_loss: 2.9158e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 2s - loss: 5.0512e-05 - acc: 0.0000e+00 - val_loss: 1.2987e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 2s - loss: 4.9147e-05 - acc: 0.0000e+00 - val_loss: 2.5739e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 2s - loss: 6.1296e-05 - acc: 0.0000e+00 - val_loss: 9.9959e-05 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 2s - loss: 4.9777e-05 - acc: 0.0000e+00 - val_loss: 9.6162e-05 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 2s - loss: 5.1116e-05 - acc: 0.0000e+00 - val_loss: 1.1731e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 2s - loss: 5.0422e-05 - acc: 0.0000e+00 - val_loss: 1.4577e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 2s - loss: 7.1013e-05 - acc: 0.0000e+00 - val_loss: 1.0623e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 2s - loss: 4.9559e-05 - acc: 0.0000e+00 - val_loss: 1.3815e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 2s - loss: 4.6423e-05 - acc: 0.0000e+00 - val_loss: 1.4018e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 2s - loss: 6.2166e-05 - acc: 0.0000e+00 - val_loss: 1.5155e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 2s - loss: 4.8065e-05 - acc: 0.0000e+00 - val_loss: 1.1399e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 2s - loss: 4.9102e-05 - acc: 0.0000e+00 - val_loss: 8.9431e-05 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 2s - loss: 5.1838e-05 - acc: 0.0000e+00 - val_loss: 1.7270e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 2s - loss: 5.4163e-05 - acc: 0.0000e+00 - val_loss: 1.6806e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 2s - loss: 5.7365e-05 - acc: 0.0000e+00 - val_loss: 1.2908e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 2s - loss: 4.6763e-05 - acc: 0.0000e+00 - val_loss: 1.0595e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 2s - loss: 5.0788e-05 - acc: 0.0000e+00 - val_loss: 1.5208e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 2s - loss: 5.0592e-05 - acc: 0.0000e+00 - val_loss: 1.0276e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 2s - loss: 5.5310e-05 - acc: 0.0000e+00 - val_loss: 1.6509e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 2s - loss: 4.6720e-05 - acc: 0.0000e+00 - val_loss: 1.3959e-04 - val_acc: 0.0000e+00
Train Score: 0.00003 MSE (0.01 RMSE)
Test Score: 0.00021 MSE (0.01 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_23 (LSTM)               (None, 22, 256)           267264    
_________________________________________________________________
dropout_23 (Dropout)         (None, 22, 256)           0         
_________________________________________________________________
lstm_24 (LSTM)               (None, 256)               525312    
_________________________________________________________________
dropout_24 (Dropout)         (None, 256)               0         
_________________________________________________________________
dense_23 (Dense)             (None, 64)                16448     
_________________________________________________________________
dense_24 (Dense)             (None, 1)                 65        
=================================================================
Total params: 809,089
Trainable params: 809,089
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 4s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.0096 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 2s - loss: 5.0181e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 2s - loss: 1.6371e-04 - acc: 0.0000e+00 - val_loss: 4.1609e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 2s - loss: 1.2213e-04 - acc: 0.0000e+00 - val_loss: 3.2139e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 2s - loss: 1.1409e-04 - acc: 0.0000e+00 - val_loss: 4.1379e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 2s - loss: 1.0606e-04 - acc: 0.0000e+00 - val_loss: 2.4285e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 2s - loss: 9.8232e-05 - acc: 0.0000e+00 - val_loss: 2.3283e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 2s - loss: 1.0070e-04 - acc: 0.0000e+00 - val_loss: 2.2279e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 2s - loss: 9.0621e-05 - acc: 0.0000e+00 - val_loss: 2.2914e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 2s - loss: 8.8094e-05 - acc: 0.0000e+00 - val_loss: 2.1350e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 2s - loss: 8.7727e-05 - acc: 0.0000e+00 - val_loss: 2.0742e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 2s - loss: 9.6307e-05 - acc: 0.0000e+00 - val_loss: 2.3852e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 2s - loss: 8.4454e-05 - acc: 0.0000e+00 - val_loss: 1.9922e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 2s - loss: 7.7632e-05 - acc: 0.0000e+00 - val_loss: 1.9435e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 2s - loss: 7.8439e-05 - acc: 0.0000e+00 - val_loss: 1.9311e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 2s - loss: 7.6799e-05 - acc: 0.0000e+00 - val_loss: 2.7103e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 2s - loss: 8.0145e-05 - acc: 0.0000e+00 - val_loss: 1.8856e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 2s - loss: 1.0098e-04 - acc: 0.0000e+00 - val_loss: 2.2187e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 2s - loss: 8.4428e-05 - acc: 0.0000e+00 - val_loss: 2.2218e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 2s - loss: 7.1758e-05 - acc: 0.0000e+00 - val_loss: 1.9211e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 2s - loss: 7.2834e-05 - acc: 0.0000e+00 - val_loss: 2.1215e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 2s - loss: 7.7957e-05 - acc: 0.0000e+00 - val_loss: 2.0983e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 2s - loss: 7.5747e-05 - acc: 0.0000e+00 - val_loss: 1.7711e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 2s - loss: 6.9023e-05 - acc: 0.0000e+00 - val_loss: 2.0176e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 2s - loss: 8.1649e-05 - acc: 0.0000e+00 - val_loss: 1.8603e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 2s - loss: 8.3357e-05 - acc: 0.0000e+00 - val_loss: 3.3086e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 2s - loss: 7.5898e-05 - acc: 0.0000e+00 - val_loss: 1.9545e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 2s - loss: 6.7264e-05 - acc: 0.0000e+00 - val_loss: 1.6782e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 2s - loss: 7.6493e-05 - acc: 0.0000e+00 - val_loss: 4.3663e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 2s - loss: 6.6357e-05 - acc: 0.0000e+00 - val_loss: 1.6497e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 2s - loss: 6.0731e-05 - acc: 0.0000e+00 - val_loss: 1.5683e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 2s - loss: 6.3049e-05 - acc: 0.0000e+00 - val_loss: 1.5698e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 2s - loss: 6.0850e-05 - acc: 0.0000e+00 - val_loss: 1.8113e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 2s - loss: 6.4386e-05 - acc: 0.0000e+00 - val_loss: 1.6883e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 2s - loss: 6.3276e-05 - acc: 0.0000e+00 - val_loss: 2.7601e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 2s - loss: 7.1896e-05 - acc: 0.0000e+00 - val_loss: 1.6007e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 2s - loss: 6.1360e-05 - acc: 0.0000e+00 - val_loss: 1.7175e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 2s - loss: 6.3682e-05 - acc: 0.0000e+00 - val_loss: 1.6422e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 2s - loss: 6.7071e-05 - acc: 0.0000e+00 - val_loss: 1.7621e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 2s - loss: 5.7570e-05 - acc: 0.0000e+00 - val_loss: 1.5119e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 2s - loss: 6.1288e-05 - acc: 0.0000e+00 - val_loss: 2.1572e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 2s - loss: 6.3713e-05 - acc: 0.0000e+00 - val_loss: 1.6333e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 2s - loss: 5.9881e-05 - acc: 0.0000e+00 - val_loss: 1.7215e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 2s - loss: 6.2633e-05 - acc: 0.0000e+00 - val_loss: 1.3589e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 2s - loss: 6.4513e-05 - acc: 0.0000e+00 - val_loss: 1.7724e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 2s - loss: 6.2045e-05 - acc: 0.0000e+00 - val_loss: 1.9163e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 2s - loss: 7.5259e-05 - acc: 0.0000e+00 - val_loss: 2.2824e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 2s - loss: 6.8696e-05 - acc: 0.0000e+00 - val_loss: 1.4655e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 2s - loss: 6.6153e-05 - acc: 0.0000e+00 - val_loss: 1.3362e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 2s - loss: 5.9115e-05 - acc: 0.0000e+00 - val_loss: 1.6425e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 2s - loss: 6.0320e-05 - acc: 0.0000e+00 - val_loss: 1.2688e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 2s - loss: 6.4085e-05 - acc: 0.0000e+00 - val_loss: 1.9757e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 2s - loss: 8.0367e-05 - acc: 0.0000e+00 - val_loss: 1.6377e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 2s - loss: 5.9877e-05 - acc: 0.0000e+00 - val_loss: 1.2305e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 2s - loss: 6.0607e-05 - acc: 0.0000e+00 - val_loss: 3.8728e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 2s - loss: 9.2545e-05 - acc: 0.0000e+00 - val_loss: 3.0147e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 2s - loss: 6.0281e-05 - acc: 0.0000e+00 - val_loss: 1.4459e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 2s - loss: 5.4326e-05 - acc: 0.0000e+00 - val_loss: 1.2723e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 2s - loss: 5.0050e-05 - acc: 0.0000e+00 - val_loss: 1.2949e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 2s - loss: 5.5365e-05 - acc: 0.0000e+00 - val_loss: 1.2775e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 2s - loss: 5.4587e-05 - acc: 0.0000e+00 - val_loss: 1.1553e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 2s - loss: 5.6952e-05 - acc: 0.0000e+00 - val_loss: 3.5440e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 2s - loss: 5.7505e-05 - acc: 0.0000e+00 - val_loss: 1.2739e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 2s - loss: 5.1206e-05 - acc: 0.0000e+00 - val_loss: 1.3347e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 2s - loss: 4.9365e-05 - acc: 0.0000e+00 - val_loss: 2.1700e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 2s - loss: 5.9185e-05 - acc: 0.0000e+00 - val_loss: 3.3951e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 2s - loss: 5.8418e-05 - acc: 0.0000e+00 - val_loss: 2.6330e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 2s - loss: 5.6553e-05 - acc: 0.0000e+00 - val_loss: 1.4572e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 2s - loss: 6.9944e-05 - acc: 0.0000e+00 - val_loss: 3.8270e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 2s - loss: 6.6265e-05 - acc: 0.0000e+00 - val_loss: 1.9433e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 2s - loss: 4.9580e-05 - acc: 0.0000e+00 - val_loss: 1.0720e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 2s - loss: 5.1305e-05 - acc: 0.0000e+00 - val_loss: 1.8352e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 2s - loss: 5.3313e-05 - acc: 0.0000e+00 - val_loss: 1.0967e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 2s - loss: 5.5718e-05 - acc: 0.0000e+00 - val_loss: 1.1168e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 2s - loss: 4.7179e-05 - acc: 0.0000e+00 - val_loss: 1.6778e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 2s - loss: 4.9833e-05 - acc: 0.0000e+00 - val_loss: 1.8654e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 2s - loss: 4.6820e-05 - acc: 0.0000e+00 - val_loss: 1.1794e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 2s - loss: 5.3134e-05 - acc: 0.0000e+00 - val_loss: 1.5263e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 2s - loss: 5.0241e-05 - acc: 0.0000e+00 - val_loss: 1.0799e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 2s - loss: 4.7767e-05 - acc: 0.0000e+00 - val_loss: 1.0010e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 2s - loss: 5.0136e-05 - acc: 0.0000e+00 - val_loss: 1.0205e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 2s - loss: 4.8949e-05 - acc: 0.0000e+00 - val_loss: 1.3785e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 2s - loss: 4.9370e-05 - acc: 0.0000e+00 - val_loss: 9.9040e-05 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 2s - loss: 4.7166e-05 - acc: 0.0000e+00 - val_loss: 1.1173e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 2s - loss: 5.8248e-05 - acc: 0.0000e+00 - val_loss: 1.7276e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 2s - loss: 6.4496e-05 - acc: 0.0000e+00 - val_loss: 1.9274e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 2s - loss: 4.9201e-05 - acc: 0.0000e+00 - val_loss: 2.4660e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 2s - loss: 4.6730e-05 - acc: 0.0000e+00 - val_loss: 9.9719e-05 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 2s - loss: 4.6232e-05 - acc: 0.0000e+00 - val_loss: 2.0847e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 2s - loss: 5.2217e-05 - acc: 0.0000e+00 - val_loss: 9.5365e-05 - val_acc: 0.0000e+00
Train Score: 0.00002 MSE (0.00 RMSE)
Test Score: 0.00030 MSE (0.02 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_25 (LSTM)               (None, 22, 512)           1058816   
_________________________________________________________________
dropout_25 (Dropout)         (None, 22, 512)           0         
_________________________________________________________________
lstm_26 (LSTM)               (None, 512)               2099200   
_________________________________________________________________
dropout_26 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_25 (Dense)             (None, 16)                8208      
_________________________________________________________________
dense_26 (Dense)             (None, 1)                 17        
=================================================================
Total params: 3,166,241
Trainable params: 3,166,241
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 7s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 5s - loss: 1.7744e-04 - acc: 0.0000e+00 - val_loss: 3.4495e-04 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 5s - loss: 9.2383e-05 - acc: 0.0000e+00 - val_loss: 3.1878e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 5s - loss: 8.8010e-05 - acc: 0.0000e+00 - val_loss: 5.9432e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 5s - loss: 8.6156e-05 - acc: 0.0000e+00 - val_loss: 2.6485e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 5s - loss: 9.8003e-05 - acc: 0.0000e+00 - val_loss: 3.1075e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 5s - loss: 9.6360e-05 - acc: 0.0000e+00 - val_loss: 3.4285e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 5s - loss: 8.0516e-05 - acc: 0.0000e+00 - val_loss: 3.9601e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 5s - loss: 1.0337e-04 - acc: 0.0000e+00 - val_loss: 3.1765e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 5s - loss: 1.0934e-04 - acc: 0.0000e+00 - val_loss: 3.2519e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 5s - loss: 7.8272e-05 - acc: 0.0000e+00 - val_loss: 4.7571e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 5s - loss: 7.4994e-05 - acc: 0.0000e+00 - val_loss: 2.9675e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 5s - loss: 8.2102e-05 - acc: 0.0000e+00 - val_loss: 2.8599e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 5s - loss: 1.0325e-04 - acc: 0.0000e+00 - val_loss: 2.4849e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 5s - loss: 8.5081e-05 - acc: 0.0000e+00 - val_loss: 2.3204e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 5s - loss: 7.4864e-05 - acc: 0.0000e+00 - val_loss: 2.2429e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 5s - loss: 7.8728e-05 - acc: 0.0000e+00 - val_loss: 2.4237e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 5s - loss: 7.3073e-05 - acc: 0.0000e+00 - val_loss: 2.3263e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 5s - loss: 7.4732e-05 - acc: 0.0000e+00 - val_loss: 9.5417e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 5s - loss: 8.2801e-05 - acc: 0.0000e+00 - val_loss: 2.3274e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 5s - loss: 9.1270e-05 - acc: 0.0000e+00 - val_loss: 3.0606e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 5s - loss: 6.6542e-05 - acc: 0.0000e+00 - val_loss: 2.1356e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 5s - loss: 6.3672e-05 - acc: 0.0000e+00 - val_loss: 2.0471e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 5s - loss: 9.2687e-05 - acc: 0.0000e+00 - val_loss: 2.1525e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 5s - loss: 7.1923e-05 - acc: 0.0000e+00 - val_loss: 2.6906e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 5s - loss: 7.5529e-05 - acc: 0.0000e+00 - val_loss: 2.2161e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 5s - loss: 7.6567e-05 - acc: 0.0000e+00 - val_loss: 3.0321e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 5s - loss: 6.3865e-05 - acc: 0.0000e+00 - val_loss: 2.1432e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 5s - loss: 5.8791e-05 - acc: 0.0000e+00 - val_loss: 3.4520e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 5s - loss: 5.7491e-05 - acc: 0.0000e+00 - val_loss: 2.4891e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 5s - loss: 6.4428e-05 - acc: 0.0000e+00 - val_loss: 2.2451e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 5s - loss: 6.6747e-05 - acc: 0.0000e+00 - val_loss: 2.7320e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 5s - loss: 8.5721e-05 - acc: 0.0000e+00 - val_loss: 2.4247e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 5s - loss: 5.5621e-05 - acc: 0.0000e+00 - val_loss: 1.9781e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 5s - loss: 6.1395e-05 - acc: 0.0000e+00 - val_loss: 5.5046e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 5s - loss: 6.9735e-05 - acc: 0.0000e+00 - val_loss: 1.7398e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 5s - loss: 8.6767e-05 - acc: 0.0000e+00 - val_loss: 8.1312e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 5s - loss: 6.8467e-05 - acc: 0.0000e+00 - val_loss: 1.9842e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 5s - loss: 5.2052e-05 - acc: 0.0000e+00 - val_loss: 2.2533e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 5s - loss: 7.1900e-05 - acc: 0.0000e+00 - val_loss: 1.7377e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 5s - loss: 5.4266e-05 - acc: 0.0000e+00 - val_loss: 4.0736e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 5s - loss: 6.4379e-05 - acc: 0.0000e+00 - val_loss: 2.7438e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 5s - loss: 5.3307e-05 - acc: 0.0000e+00 - val_loss: 1.4855e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 5s - loss: 5.0383e-05 - acc: 0.0000e+00 - val_loss: 1.6908e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 5s - loss: 5.2109e-05 - acc: 0.0000e+00 - val_loss: 1.4925e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 5s - loss: 4.8929e-05 - acc: 0.0000e+00 - val_loss: 2.0280e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 5s - loss: 5.6286e-05 - acc: 0.0000e+00 - val_loss: 3.6762e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 5s - loss: 4.9321e-05 - acc: 0.0000e+00 - val_loss: 4.1044e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 5s - loss: 6.7442e-05 - acc: 0.0000e+00 - val_loss: 2.6415e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 5s - loss: 5.1384e-05 - acc: 0.0000e+00 - val_loss: 1.2662e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 5s - loss: 5.3324e-05 - acc: 0.0000e+00 - val_loss: 1.7271e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 5s - loss: 7.5706e-05 - acc: 0.0000e+00 - val_loss: 3.7834e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 5s - loss: 4.7666e-05 - acc: 0.0000e+00 - val_loss: 1.3999e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 5s - loss: 5.3377e-05 - acc: 0.0000e+00 - val_loss: 1.2388e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 5s - loss: 8.0172e-05 - acc: 0.0000e+00 - val_loss: 2.4319e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 5s - loss: 6.6979e-05 - acc: 0.0000e+00 - val_loss: 1.2924e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 5s - loss: 4.7535e-05 - acc: 0.0000e+00 - val_loss: 4.3677e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 5s - loss: 6.4995e-05 - acc: 0.0000e+00 - val_loss: 2.8436e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 5s - loss: 5.0257e-05 - acc: 0.0000e+00 - val_loss: 1.1430e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 5s - loss: 4.0646e-05 - acc: 0.0000e+00 - val_loss: 1.9424e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 5s - loss: 5.6332e-05 - acc: 0.0000e+00 - val_loss: 7.3764e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 5s - loss: 5.5294e-05 - acc: 0.0000e+00 - val_loss: 1.1102e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 5s - loss: 4.0142e-05 - acc: 0.0000e+00 - val_loss: 4.0685e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 5s - loss: 5.0925e-05 - acc: 0.0000e+00 - val_loss: 1.3003e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 5s - loss: 5.4738e-05 - acc: 0.0000e+00 - val_loss: 1.0553e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 5s - loss: 5.0880e-05 - acc: 0.0000e+00 - val_loss: 1.5358e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 5s - loss: 3.8448e-05 - acc: 0.0000e+00 - val_loss: 2.8436e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 5s - loss: 4.0430e-05 - acc: 0.0000e+00 - val_loss: 1.8442e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 5s - loss: 4.0367e-05 - acc: 0.0000e+00 - val_loss: 2.5187e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 5s - loss: 4.2968e-05 - acc: 0.0000e+00 - val_loss: 3.0867e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 5s - loss: 4.6618e-05 - acc: 0.0000e+00 - val_loss: 1.6065e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 5s - loss: 4.2401e-05 - acc: 0.0000e+00 - val_loss: 1.6702e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 5s - loss: 4.0544e-05 - acc: 0.0000e+00 - val_loss: 9.5525e-05 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 5s - loss: 3.9828e-05 - acc: 0.0000e+00 - val_loss: 1.8311e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 5s - loss: 6.1247e-05 - acc: 0.0000e+00 - val_loss: 3.4708e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 5s - loss: 7.0362e-05 - acc: 0.0000e+00 - val_loss: 1.1589e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 5s - loss: 4.2499e-05 - acc: 0.0000e+00 - val_loss: 1.3025e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 5s - loss: 4.2591e-05 - acc: 0.0000e+00 - val_loss: 1.2744e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 5s - loss: 3.5940e-05 - acc: 0.0000e+00 - val_loss: 9.2073e-05 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 5s - loss: 3.7204e-05 - acc: 0.0000e+00 - val_loss: 1.3093e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 5s - loss: 3.6906e-05 - acc: 0.0000e+00 - val_loss: 9.7412e-05 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 5s - loss: 5.2656e-05 - acc: 0.0000e+00 - val_loss: 2.3259e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 5s - loss: 4.9680e-05 - acc: 0.0000e+00 - val_loss: 1.3716e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 5s - loss: 4.5916e-05 - acc: 0.0000e+00 - val_loss: 9.1353e-05 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 5s - loss: 4.7735e-05 - acc: 0.0000e+00 - val_loss: 9.1079e-05 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 5s - loss: 5.5627e-05 - acc: 0.0000e+00 - val_loss: 1.7377e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 5s - loss: 6.1069e-05 - acc: 0.0000e+00 - val_loss: 2.5054e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 5s - loss: 7.1901e-05 - acc: 0.0000e+00 - val_loss: 3.0488e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 5s - loss: 5.3568e-05 - acc: 0.0000e+00 - val_loss: 1.4197e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 5s - loss: 3.6352e-05 - acc: 0.0000e+00 - val_loss: 8.7662e-05 - val_acc: 0.0000e+00
Train Score: 0.00002 MSE (0.00 RMSE)
Test Score: 0.00056 MSE (0.02 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_27 (LSTM)               (None, 22, 512)           1058816   
_________________________________________________________________
dropout_27 (Dropout)         (None, 22, 512)           0         
_________________________________________________________________
lstm_28 (LSTM)               (None, 512)               2099200   
_________________________________________________________________
dropout_28 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_27 (Dense)             (None, 32)                16416     
_________________________________________________________________
dense_28 (Dense)             (None, 1)                 33        
=================================================================
Total params: 3,174,465
Trainable params: 3,174,465
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 7s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 5s - loss: 6.7627e-04 - acc: 0.0000e+00 - val_loss: 8.7711e-04 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 5s - loss: 1.6591e-04 - acc: 0.0000e+00 - val_loss: 4.3464e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 5s - loss: 1.1055e-04 - acc: 0.0000e+00 - val_loss: 3.1364e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 5s - loss: 8.6402e-05 - acc: 0.0000e+00 - val_loss: 2.6699e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 5s - loss: 8.0669e-05 - acc: 0.0000e+00 - val_loss: 2.3708e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 5s - loss: 7.7800e-05 - acc: 0.0000e+00 - val_loss: 2.6208e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 5s - loss: 9.0497e-05 - acc: 0.0000e+00 - val_loss: 2.3972e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 5s - loss: 7.1067e-05 - acc: 0.0000e+00 - val_loss: 2.4296e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 5s - loss: 7.9885e-05 - acc: 0.0000e+00 - val_loss: 2.5970e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 5s - loss: 7.1293e-05 - acc: 0.0000e+00 - val_loss: 2.2061e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 5s - loss: 7.0748e-05 - acc: 0.0000e+00 - val_loss: 2.1607e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 5s - loss: 6.5438e-05 - acc: 0.0000e+00 - val_loss: 3.1982e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 5s - loss: 6.7950e-05 - acc: 0.0000e+00 - val_loss: 2.6940e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 5s - loss: 7.7328e-05 - acc: 0.0000e+00 - val_loss: 2.0519e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 5s - loss: 6.3397e-05 - acc: 0.0000e+00 - val_loss: 2.2156e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 5s - loss: 7.0194e-05 - acc: 0.0000e+00 - val_loss: 2.1489e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 5s - loss: 6.1922e-05 - acc: 0.0000e+00 - val_loss: 2.2043e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 5s - loss: 5.7475e-05 - acc: 0.0000e+00 - val_loss: 1.9719e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 5s - loss: 6.9206e-05 - acc: 0.0000e+00 - val_loss: 3.4417e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 5s - loss: 6.3915e-05 - acc: 0.0000e+00 - val_loss: 2.8902e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 5s - loss: 5.9033e-05 - acc: 0.0000e+00 - val_loss: 2.5501e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 5s - loss: 6.8111e-05 - acc: 0.0000e+00 - val_loss: 6.4109e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 5s - loss: 7.6134e-05 - acc: 0.0000e+00 - val_loss: 2.0495e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 5s - loss: 5.7076e-05 - acc: 0.0000e+00 - val_loss: 1.9230e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 5s - loss: 5.9483e-05 - acc: 0.0000e+00 - val_loss: 2.2378e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 5s - loss: 5.6974e-05 - acc: 0.0000e+00 - val_loss: 3.5238e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 5s - loss: 5.4746e-05 - acc: 0.0000e+00 - val_loss: 1.6960e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 5s - loss: 5.7180e-05 - acc: 0.0000e+00 - val_loss: 2.9533e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 5s - loss: 5.3362e-05 - acc: 0.0000e+00 - val_loss: 1.7395e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 5s - loss: 5.9658e-05 - acc: 0.0000e+00 - val_loss: 2.7860e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 5s - loss: 7.8084e-05 - acc: 0.0000e+00 - val_loss: 2.2948e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 5s - loss: 5.0821e-05 - acc: 0.0000e+00 - val_loss: 1.6719e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 5s - loss: 6.8743e-05 - acc: 0.0000e+00 - val_loss: 3.2100e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 5s - loss: 6.7563e-05 - acc: 0.0000e+00 - val_loss: 1.5917e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 5s - loss: 6.9278e-05 - acc: 0.0000e+00 - val_loss: 1.8111e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 5s - loss: 6.4663e-05 - acc: 0.0000e+00 - val_loss: 1.6178e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 5s - loss: 5.3156e-05 - acc: 0.0000e+00 - val_loss: 1.6793e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 5s - loss: 5.2560e-05 - acc: 0.0000e+00 - val_loss: 1.7672e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 5s - loss: 5.2714e-05 - acc: 0.0000e+00 - val_loss: 1.9043e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 5s - loss: 4.7904e-05 - acc: 0.0000e+00 - val_loss: 1.8789e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 5s - loss: 5.2191e-05 - acc: 0.0000e+00 - val_loss: 1.9801e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 5s - loss: 5.2251e-05 - acc: 0.0000e+00 - val_loss: 2.5759e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 5s - loss: 5.1228e-05 - acc: 0.0000e+00 - val_loss: 2.9425e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 5s - loss: 6.4749e-05 - acc: 0.0000e+00 - val_loss: 3.1890e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 5s - loss: 5.8371e-05 - acc: 0.0000e+00 - val_loss: 1.5237e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 5s - loss: 5.6497e-05 - acc: 0.0000e+00 - val_loss: 3.3804e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 5s - loss: 5.4504e-05 - acc: 0.0000e+00 - val_loss: 1.3825e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 5s - loss: 5.0490e-05 - acc: 0.0000e+00 - val_loss: 1.4198e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 5s - loss: 4.4790e-05 - acc: 0.0000e+00 - val_loss: 2.0868e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 5s - loss: 4.3868e-05 - acc: 0.0000e+00 - val_loss: 1.4702e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 5s - loss: 4.2877e-05 - acc: 0.0000e+00 - val_loss: 1.2672e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 5s - loss: 5.3234e-05 - acc: 0.0000e+00 - val_loss: 1.7786e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 5s - loss: 5.1293e-05 - acc: 0.0000e+00 - val_loss: 2.4828e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 5s - loss: 4.8906e-05 - acc: 0.0000e+00 - val_loss: 3.9230e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 5s - loss: 5.3556e-05 - acc: 0.0000e+00 - val_loss: 5.8902e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 5s - loss: 6.4967e-05 - acc: 0.0000e+00 - val_loss: 2.3537e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 5s - loss: 4.3928e-05 - acc: 0.0000e+00 - val_loss: 1.9513e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 5s - loss: 4.1963e-05 - acc: 0.0000e+00 - val_loss: 1.3201e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 5s - loss: 5.0514e-05 - acc: 0.0000e+00 - val_loss: 2.3785e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 5s - loss: 4.1522e-05 - acc: 0.0000e+00 - val_loss: 1.3313e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 5s - loss: 4.1900e-05 - acc: 0.0000e+00 - val_loss: 1.2340e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 5s - loss: 5.1488e-05 - acc: 0.0000e+00 - val_loss: 1.8833e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 5s - loss: 4.8815e-05 - acc: 0.0000e+00 - val_loss: 2.8125e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 5s - loss: 4.1729e-05 - acc: 0.0000e+00 - val_loss: 1.1163e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 5s - loss: 4.0148e-05 - acc: 0.0000e+00 - val_loss: 1.4508e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 5s - loss: 4.5945e-05 - acc: 0.0000e+00 - val_loss: 1.2213e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 5s - loss: 4.7222e-05 - acc: 0.0000e+00 - val_loss: 1.7338e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 5s - loss: 4.6965e-05 - acc: 0.0000e+00 - val_loss: 1.3902e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 5s - loss: 3.8000e-05 - acc: 0.0000e+00 - val_loss: 2.2710e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 5s - loss: 5.4208e-05 - acc: 0.0000e+00 - val_loss: 1.1956e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 5s - loss: 4.4768e-05 - acc: 0.0000e+00 - val_loss: 2.4201e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 5s - loss: 4.6059e-05 - acc: 0.0000e+00 - val_loss: 2.6171e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 5s - loss: 4.2302e-05 - acc: 0.0000e+00 - val_loss: 1.0281e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 5s - loss: 4.0051e-05 - acc: 0.0000e+00 - val_loss: 1.1258e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 5s - loss: 6.0282e-05 - acc: 0.0000e+00 - val_loss: 1.8706e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 5s - loss: 4.0816e-05 - acc: 0.0000e+00 - val_loss: 1.0030e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 5s - loss: 4.3881e-05 - acc: 0.0000e+00 - val_loss: 3.4460e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 5s - loss: 4.6086e-05 - acc: 0.0000e+00 - val_loss: 1.7175e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 5s - loss: 4.4014e-05 - acc: 0.0000e+00 - val_loss: 9.8474e-05 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 5s - loss: 4.2938e-05 - acc: 0.0000e+00 - val_loss: 3.6452e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 5s - loss: 4.2751e-05 - acc: 0.0000e+00 - val_loss: 1.5063e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 5s - loss: 4.0101e-05 - acc: 0.0000e+00 - val_loss: 1.1372e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 5s - loss: 3.7873e-05 - acc: 0.0000e+00 - val_loss: 1.1552e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 5s - loss: 3.9376e-05 - acc: 0.0000e+00 - val_loss: 9.7031e-05 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 5s - loss: 4.4795e-05 - acc: 0.0000e+00 - val_loss: 1.2941e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 5s - loss: 4.8544e-05 - acc: 0.0000e+00 - val_loss: 1.1371e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 5s - loss: 3.9842e-05 - acc: 0.0000e+00 - val_loss: 1.2123e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 5s - loss: 3.9124e-05 - acc: 0.0000e+00 - val_loss: 9.4820e-05 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 5s - loss: 3.7951e-05 - acc: 0.0000e+00 - val_loss: 2.2827e-04 - val_acc: 0.0000e+00
Train Score: 0.00006 MSE (0.01 RMSE)
Test Score: 0.00021 MSE (0.01 RMSE)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_29 (LSTM)               (None, 22, 512)           1058816   
_________________________________________________________________
dropout_29 (Dropout)         (None, 22, 512)           0         
_________________________________________________________________
lstm_30 (LSTM)               (None, 512)               2099200   
_________________________________________________________________
dropout_30 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_29 (Dense)             (None, 64)                32832     
_________________________________________________________________
dense_30 (Dense)             (None, 1)                 65        
=================================================================
Total params: 3,190,913
Trainable params: 3,190,913
Non-trainable params: 0
_________________________________________________________________
Train on 13707 samples, validate on 1523 samples
Epoch 1/90
13707/13707 [==============================] - 7s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.0042 - val_acc: 0.0000e+00
Epoch 2/90
13707/13707 [==============================] - 5s - loss: 5.7127e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00
Epoch 3/90
13707/13707 [==============================] - 5s - loss: 1.5277e-04 - acc: 0.0000e+00 - val_loss: 5.1752e-04 - val_acc: 0.0000e+00
Epoch 4/90
13707/13707 [==============================] - 5s - loss: 1.1383e-04 - acc: 0.0000e+00 - val_loss: 3.1569e-04 - val_acc: 0.0000e+00
Epoch 5/90
13707/13707 [==============================] - 5s - loss: 8.3033e-05 - acc: 0.0000e+00 - val_loss: 2.9547e-04 - val_acc: 0.0000e+00
Epoch 6/90
13707/13707 [==============================] - 5s - loss: 8.4735e-05 - acc: 0.0000e+00 - val_loss: 3.0643e-04 - val_acc: 0.0000e+00
Epoch 7/90
13707/13707 [==============================] - 5s - loss: 7.5323e-05 - acc: 0.0000e+00 - val_loss: 2.4925e-04 - val_acc: 0.0000e+00
Epoch 8/90
13707/13707 [==============================] - 5s - loss: 8.1686e-05 - acc: 0.0000e+00 - val_loss: 2.1852e-04 - val_acc: 0.0000e+00
Epoch 9/90
13707/13707 [==============================] - 5s - loss: 7.3006e-05 - acc: 0.0000e+00 - val_loss: 2.3519e-04 - val_acc: 0.0000e+00
Epoch 10/90
13707/13707 [==============================] - 5s - loss: 6.8317e-05 - acc: 0.0000e+00 - val_loss: 2.2700e-04 - val_acc: 0.0000e+00
Epoch 11/90
13707/13707 [==============================] - 5s - loss: 6.5661e-05 - acc: 0.0000e+00 - val_loss: 2.1559e-04 - val_acc: 0.0000e+00
Epoch 12/90
13707/13707 [==============================] - 5s - loss: 6.8524e-05 - acc: 0.0000e+00 - val_loss: 2.3825e-04 - val_acc: 0.0000e+00
Epoch 13/90
13707/13707 [==============================] - 5s - loss: 6.5461e-05 - acc: 0.0000e+00 - val_loss: 3.3604e-04 - val_acc: 0.0000e+00
Epoch 14/90
13707/13707 [==============================] - 5s - loss: 6.9308e-05 - acc: 0.0000e+00 - val_loss: 2.8185e-04 - val_acc: 0.0000e+00
Epoch 15/90
13707/13707 [==============================] - 5s - loss: 7.7471e-05 - acc: 0.0000e+00 - val_loss: 2.4124e-04 - val_acc: 0.0000e+00
Epoch 16/90
13707/13707 [==============================] - 5s - loss: 6.6734e-05 - acc: 0.0000e+00 - val_loss: 1.9131e-04 - val_acc: 0.0000e+00
Epoch 17/90
13707/13707 [==============================] - 5s - loss: 6.0376e-05 - acc: 0.0000e+00 - val_loss: 1.9164e-04 - val_acc: 0.0000e+00
Epoch 18/90
13707/13707 [==============================] - 5s - loss: 5.7234e-05 - acc: 0.0000e+00 - val_loss: 1.8514e-04 - val_acc: 0.0000e+00
Epoch 19/90
13707/13707 [==============================] - 5s - loss: 6.0717e-05 - acc: 0.0000e+00 - val_loss: 3.0114e-04 - val_acc: 0.0000e+00
Epoch 20/90
13707/13707 [==============================] - 5s - loss: 6.2376e-05 - acc: 0.0000e+00 - val_loss: 2.2846e-04 - val_acc: 0.0000e+00
Epoch 21/90
13707/13707 [==============================] - 5s - loss: 5.7468e-05 - acc: 0.0000e+00 - val_loss: 1.7822e-04 - val_acc: 0.0000e+00
Epoch 22/90
13707/13707 [==============================] - 5s - loss: 7.3096e-05 - acc: 0.0000e+00 - val_loss: 2.3393e-04 - val_acc: 0.0000e+00
Epoch 23/90
13707/13707 [==============================] - 5s - loss: 5.8082e-05 - acc: 0.0000e+00 - val_loss: 3.0920e-04 - val_acc: 0.0000e+00
Epoch 24/90
13707/13707 [==============================] - 5s - loss: 8.2944e-05 - acc: 0.0000e+00 - val_loss: 2.0705e-04 - val_acc: 0.0000e+00
Epoch 25/90
13707/13707 [==============================] - 5s - loss: 8.1935e-05 - acc: 0.0000e+00 - val_loss: 1.7846e-04 - val_acc: 0.0000e+00
Epoch 26/90
13707/13707 [==============================] - 5s - loss: 5.6712e-05 - acc: 0.0000e+00 - val_loss: 2.2576e-04 - val_acc: 0.0000e+00
Epoch 27/90
13707/13707 [==============================] - 5s - loss: 5.5286e-05 - acc: 0.0000e+00 - val_loss: 2.2648e-04 - val_acc: 0.0000e+00
Epoch 28/90
13707/13707 [==============================] - 5s - loss: 5.5477e-05 - acc: 0.0000e+00 - val_loss: 2.1367e-04 - val_acc: 0.0000e+00
Epoch 29/90
13707/13707 [==============================] - 5s - loss: 5.6091e-05 - acc: 0.0000e+00 - val_loss: 1.9820e-04 - val_acc: 0.0000e+00
Epoch 30/90
13707/13707 [==============================] - 5s - loss: 6.1355e-05 - acc: 0.0000e+00 - val_loss: 3.4680e-04 - val_acc: 0.0000e+00
Epoch 31/90
13707/13707 [==============================] - 5s - loss: 5.4002e-05 - acc: 0.0000e+00 - val_loss: 1.8448e-04 - val_acc: 0.0000e+00
Epoch 32/90
13707/13707 [==============================] - 5s - loss: 5.4998e-05 - acc: 0.0000e+00 - val_loss: 2.5581e-04 - val_acc: 0.0000e+00
Epoch 33/90
13707/13707 [==============================] - 5s - loss: 5.3935e-05 - acc: 0.0000e+00 - val_loss: 1.5469e-04 - val_acc: 0.0000e+00
Epoch 34/90
13707/13707 [==============================] - 5s - loss: 6.4605e-05 - acc: 0.0000e+00 - val_loss: 4.2348e-04 - val_acc: 0.0000e+00
Epoch 35/90
13707/13707 [==============================] - 5s - loss: 5.3398e-05 - acc: 0.0000e+00 - val_loss: 2.7493e-04 - val_acc: 0.0000e+00
Epoch 36/90
13707/13707 [==============================] - 5s - loss: 4.8229e-05 - acc: 0.0000e+00 - val_loss: 1.8093e-04 - val_acc: 0.0000e+00
Epoch 37/90
13707/13707 [==============================] - 5s - loss: 5.2976e-05 - acc: 0.0000e+00 - val_loss: 2.8494e-04 - val_acc: 0.0000e+00
Epoch 38/90
13707/13707 [==============================] - 5s - loss: 5.0289e-05 - acc: 0.0000e+00 - val_loss: 2.7506e-04 - val_acc: 0.0000e+00
Epoch 39/90
13707/13707 [==============================] - 5s - loss: 4.9997e-05 - acc: 0.0000e+00 - val_loss: 1.4458e-04 - val_acc: 0.0000e+00
Epoch 40/90
13707/13707 [==============================] - 5s - loss: 4.7490e-05 - acc: 0.0000e+00 - val_loss: 1.6178e-04 - val_acc: 0.0000e+00
Epoch 41/90
13707/13707 [==============================] - 5s - loss: 4.5040e-05 - acc: 0.0000e+00 - val_loss: 1.7008e-04 - val_acc: 0.0000e+00
Epoch 42/90
13707/13707 [==============================] - 5s - loss: 5.9983e-05 - acc: 0.0000e+00 - val_loss: 2.1438e-04 - val_acc: 0.0000e+00
Epoch 43/90
13707/13707 [==============================] - 5s - loss: 4.7204e-05 - acc: 0.0000e+00 - val_loss: 1.9721e-04 - val_acc: 0.0000e+00
Epoch 44/90
13707/13707 [==============================] - 5s - loss: 5.6172e-05 - acc: 0.0000e+00 - val_loss: 1.4535e-04 - val_acc: 0.0000e+00
Epoch 45/90
13707/13707 [==============================] - 5s - loss: 4.8046e-05 - acc: 0.0000e+00 - val_loss: 2.2610e-04 - val_acc: 0.0000e+00
Epoch 46/90
13707/13707 [==============================] - 5s - loss: 5.0639e-05 - acc: 0.0000e+00 - val_loss: 1.6809e-04 - val_acc: 0.0000e+00
Epoch 47/90
13707/13707 [==============================] - 5s - loss: 4.6883e-05 - acc: 0.0000e+00 - val_loss: 1.3198e-04 - val_acc: 0.0000e+00
Epoch 48/90
13707/13707 [==============================] - 5s - loss: 5.2987e-05 - acc: 0.0000e+00 - val_loss: 4.9383e-04 - val_acc: 0.0000e+00
Epoch 49/90
13707/13707 [==============================] - 5s - loss: 1.0399e-04 - acc: 0.0000e+00 - val_loss: 4.8208e-04 - val_acc: 0.0000e+00
Epoch 50/90
13707/13707 [==============================] - 5s - loss: 6.2235e-05 - acc: 0.0000e+00 - val_loss: 1.3547e-04 - val_acc: 0.0000e+00
Epoch 51/90
13707/13707 [==============================] - 5s - loss: 4.5444e-05 - acc: 0.0000e+00 - val_loss: 1.3129e-04 - val_acc: 0.0000e+00
Epoch 52/90
13707/13707 [==============================] - 5s - loss: 4.1589e-05 - acc: 0.0000e+00 - val_loss: 2.2130e-04 - val_acc: 0.0000e+00
Epoch 53/90
13707/13707 [==============================] - 5s - loss: 4.9925e-05 - acc: 0.0000e+00 - val_loss: 1.5910e-04 - val_acc: 0.0000e+00
Epoch 54/90
13707/13707 [==============================] - 5s - loss: 4.6165e-05 - acc: 0.0000e+00 - val_loss: 3.0870e-04 - val_acc: 0.0000e+00
Epoch 55/90
13707/13707 [==============================] - 5s - loss: 5.1203e-05 - acc: 0.0000e+00 - val_loss: 1.2229e-04 - val_acc: 0.0000e+00
Epoch 56/90
13707/13707 [==============================] - 5s - loss: 4.3808e-05 - acc: 0.0000e+00 - val_loss: 1.6088e-04 - val_acc: 0.0000e+00
Epoch 57/90
13707/13707 [==============================] - 5s - loss: 5.2079e-05 - acc: 0.0000e+00 - val_loss: 1.4262e-04 - val_acc: 0.0000e+00
Epoch 58/90
13707/13707 [==============================] - 5s - loss: 4.1734e-05 - acc: 0.0000e+00 - val_loss: 2.2540e-04 - val_acc: 0.0000e+00
Epoch 59/90
13707/13707 [==============================] - 5s - loss: 4.9984e-05 - acc: 0.0000e+00 - val_loss: 2.5313e-04 - val_acc: 0.0000e+00
Epoch 60/90
13707/13707 [==============================] - 5s - loss: 6.4694e-05 - acc: 0.0000e+00 - val_loss: 1.3746e-04 - val_acc: 0.0000e+00
Epoch 61/90
13707/13707 [==============================] - 5s - loss: 4.2644e-05 - acc: 0.0000e+00 - val_loss: 1.1615e-04 - val_acc: 0.0000e+00
Epoch 62/90
13707/13707 [==============================] - 5s - loss: 4.8698e-05 - acc: 0.0000e+00 - val_loss: 1.7441e-04 - val_acc: 0.0000e+00
Epoch 63/90
13707/13707 [==============================] - 5s - loss: 4.2934e-05 - acc: 0.0000e+00 - val_loss: 1.8039e-04 - val_acc: 0.0000e+00
Epoch 64/90
13707/13707 [==============================] - 5s - loss: 5.5987e-05 - acc: 0.0000e+00 - val_loss: 1.4653e-04 - val_acc: 0.0000e+00
Epoch 65/90
13707/13707 [==============================] - 5s - loss: 4.4255e-05 - acc: 0.0000e+00 - val_loss: 1.3370e-04 - val_acc: 0.0000e+00
Epoch 66/90
13707/13707 [==============================] - 5s - loss: 4.0193e-05 - acc: 0.0000e+00 - val_loss: 1.5693e-04 - val_acc: 0.0000e+00
Epoch 67/90
13707/13707 [==============================] - 5s - loss: 4.2163e-05 - acc: 0.0000e+00 - val_loss: 1.9438e-04 - val_acc: 0.0000e+00
Epoch 68/90
13707/13707 [==============================] - 5s - loss: 4.8123e-05 - acc: 0.0000e+00 - val_loss: 3.8878e-04 - val_acc: 0.0000e+00
Epoch 69/90
13707/13707 [==============================] - 5s - loss: 4.3870e-05 - acc: 0.0000e+00 - val_loss: 1.2966e-04 - val_acc: 0.0000e+00
Epoch 70/90
13707/13707 [==============================] - 5s - loss: 4.7672e-05 - acc: 0.0000e+00 - val_loss: 1.1231e-04 - val_acc: 0.0000e+00
Epoch 71/90
13707/13707 [==============================] - 5s - loss: 4.1490e-05 - acc: 0.0000e+00 - val_loss: 1.1473e-04 - val_acc: 0.0000e+00
Epoch 72/90
13707/13707 [==============================] - 5s - loss: 4.6135e-05 - acc: 0.0000e+00 - val_loss: 1.8271e-04 - val_acc: 0.0000e+00
Epoch 73/90
13707/13707 [==============================] - 5s - loss: 4.2570e-05 - acc: 0.0000e+00 - val_loss: 1.4970e-04 - val_acc: 0.0000e+00
Epoch 74/90
13707/13707 [==============================] - 5s - loss: 3.6103e-05 - acc: 0.0000e+00 - val_loss: 1.0239e-04 - val_acc: 0.0000e+00
Epoch 75/90
13707/13707 [==============================] - 5s - loss: 3.6905e-05 - acc: 0.0000e+00 - val_loss: 1.0734e-04 - val_acc: 0.0000e+00
Epoch 76/90
13707/13707 [==============================] - 5s - loss: 3.5542e-05 - acc: 0.0000e+00 - val_loss: 1.1453e-04 - val_acc: 0.0000e+00
Epoch 77/90
13707/13707 [==============================] - 5s - loss: 4.0579e-05 - acc: 0.0000e+00 - val_loss: 2.1340e-04 - val_acc: 0.0000e+00
Epoch 78/90
13707/13707 [==============================] - 5s - loss: 4.4344e-05 - acc: 0.0000e+00 - val_loss: 2.5877e-04 - val_acc: 0.0000e+00
Epoch 79/90
13707/13707 [==============================] - 5s - loss: 3.8096e-05 - acc: 0.0000e+00 - val_loss: 1.3082e-04 - val_acc: 0.0000e+00
Epoch 80/90
13707/13707 [==============================] - 5s - loss: 4.1058e-05 - acc: 0.0000e+00 - val_loss: 1.3139e-04 - val_acc: 0.0000e+00
Epoch 81/90
13707/13707 [==============================] - 5s - loss: 3.9520e-05 - acc: 0.0000e+00 - val_loss: 1.5584e-04 - val_acc: 0.0000e+00
Epoch 82/90
13707/13707 [==============================] - 5s - loss: 4.0933e-05 - acc: 0.0000e+00 - val_loss: 1.1102e-04 - val_acc: 0.0000e+00
Epoch 83/90
13707/13707 [==============================] - 5s - loss: 3.4266e-05 - acc: 0.0000e+00 - val_loss: 1.2612e-04 - val_acc: 0.0000e+00
Epoch 84/90
13707/13707 [==============================] - 5s - loss: 3.6913e-05 - acc: 0.0000e+00 - val_loss: 1.1481e-04 - val_acc: 0.0000e+00
Epoch 85/90
13707/13707 [==============================] - 5s - loss: 4.1322e-05 - acc: 0.0000e+00 - val_loss: 3.3684e-04 - val_acc: 0.0000e+00
Epoch 86/90
13707/13707 [==============================] - 5s - loss: 5.6055e-05 - acc: 0.0000e+00 - val_loss: 3.4491e-04 - val_acc: 0.0000e+00
Epoch 87/90
13707/13707 [==============================] - 5s - loss: 4.5288e-05 - acc: 0.0000e+00 - val_loss: 1.2911e-04 - val_acc: 0.0000e+00
Epoch 88/90
13707/13707 [==============================] - 5s - loss: 3.6853e-05 - acc: 0.0000e+00 - val_loss: 1.0088e-04 - val_acc: 0.0000e+00
Epoch 89/90
13707/13707 [==============================] - 5s - loss: 3.5433e-05 - acc: 0.0000e+00 - val_loss: 1.4986e-04 - val_acc: 0.0000e+00
Epoch 90/90
13707/13707 [==============================] - 5s - loss: 4.2263e-05 - acc: 0.0000e+00 - val_loss: 4.1180e-04 - val_acc: 0.0000e+00
Train Score: 0.00009 MSE (0.01 RMSE)
Test Score: 0.00176 MSE (0.04 RMSE)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[112]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lists</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">neurons_result</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">lists</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Finding the best hyperparameter&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;neurons&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Square Error&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lists</span><span class="p">)),</span> <span class="n">y</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lists</span><span class="p">)),</span> <span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-112-6c062ca2d33d&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>lists <span class="ansi-blue-fg">=</span> sorted<span class="ansi-blue-fg">(</span>neurons_result<span class="ansi-blue-fg">.</span>items<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> x<span class="ansi-blue-fg">,</span>y <span class="ansi-blue-fg">=</span> zip<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>lists<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> 
<span class="ansi-green-intense-fg ansi-bold">      4</span> plt<span class="ansi-blue-fg">.</span>title<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;Finding the best hyperparameter&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> plt<span class="ansi-blue-fg">.</span>xlabel<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;neurons&#39;</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;neurons_result&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>12.4 Optimial Dropout value</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stock_name</span> <span class="o">=</span> <span class="s1">&#39;^GSPC&#39;</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># feature, window, output</span>
<span class="n">neurons</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">decaylist</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">build_model3</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">decay</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
        
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">neurons</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
        
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">neurons</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>        
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">neurons</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
    <span class="c1"># model = load_model(&#39;my_LSTM_stock_model1000.h5&#39;)</span>
    <span class="n">adam</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">decay</span><span class="o">=</span><span class="n">decay</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">quick_measure</span><span class="p">(</span><span class="n">stock_name</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">decay</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">get_stock_data</span><span class="p">(</span><span class="n">stock_name</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">build_model3</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">decay</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># model.save(&#39;LSTM_Stock_prediction-20170429.h5&#39;)</span>
    <span class="n">trainScore</span><span class="p">,</span> <span class="n">testScore</span> <span class="o">=</span> <span class="n">model_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">trainScore</span><span class="p">,</span> <span class="n">testScore</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">decay_result</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">decay</span> <span class="ow">in</span> <span class="n">decaylist</span><span class="p">:</span>    
    <span class="n">trainScore</span><span class="p">,</span> <span class="n">testScore</span> <span class="o">=</span> <span class="n">quick_measure</span><span class="p">(</span><span class="n">stock_name</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">decay</span><span class="p">)</span>
    <span class="n">decay_result</span><span class="p">[</span><span class="n">decay</span><span class="p">]</span> <span class="o">=</span> <span class="n">testScore</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lists</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">decay_result</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">lists</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Finding the best hyperparameter&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Decay&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Square Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stock_name</span> <span class="o">=</span> <span class="s1">&#39;^GSPC&#39;</span>
<span class="n">neurons</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">d</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1">#dropout</span>
<span class="n">decay</span> <span class="o">=</span> <span class="mf">0.4</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">seq_len_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">180</span><span class="p">]</span>

<span class="n">seq_len_result</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="n">seq_len_list</span><span class="p">:</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    
    <span class="n">trainScore</span><span class="p">,</span> <span class="n">testScore</span> <span class="o">=</span> <span class="n">quick_measure</span><span class="p">(</span><span class="n">stock_name</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">neurons</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">decay</span><span class="p">)</span>
    <span class="n">seq_len_result</span><span class="p">[</span><span class="n">seq_len</span><span class="p">]</span> <span class="o">=</span> <span class="n">testScore</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lists</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">seq_len_result</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">lists</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Finding the best hyperparameter&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Days&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Square Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

</div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
